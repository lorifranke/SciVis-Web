 A Scalable WebGL-based Approach for Visualizing Massive 3D
Point Clouds using Semantics-Dependent Rendering Techniques
                    Sören Discher                                                   Rico Richter                                 Jürgen Döllner
            Hasso Plattner Institute                                      Hasso Plattner Institute                          Hasso Plattner Institute
        University of Potsdam, Germany                                University of Potsdam, Germany                    University of Potsdam, Germany
            soeren.discher@hpi.de                                           rico.richter@hpi.de                            juergen.doellner@hpi.de

ABSTRACT                                                                                      ACM Reference Format:
3D point cloud technology facilitates the automated and highly de-                            Sören Discher, Rico Richter, and Jürgen Döllner. 2018. A Scalable WebGL-
                                                                                              based Approach for Visualizing Massive 3D Point Clouds using Semantics-
tailed digital acquisition of real-world environments such as assets,
                                                                                              Dependent Rendering Techniques. In Web3D ’18: Web3D ’18: The 23rd Inter-
sites, cities, and countries; the acquired 3D point clouds represent                          national Conference on Web3D Technology, June 20–22, 2018, Poznan, Poland.
an essential category of geodata used in a variety of geoinformation                          ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3208806.3208816
applications and systems. In this paper, we present a web-based sys-
tem for the interactive and collaborative exploration and inspection
of arbitrary large 3D point clouds. Our approach is based on stan-                            1    MOTIVATION
dard WebGL on the client side and is able to render 3D point clouds                           3D point clouds allow for a discrete representation of real-world
with billions of points. It uses spatial data structures and level-of-                        objects and environments. They can be time-efficiently and cost-
detail representations to manage the 3D point cloud data and to                               efficiently generated by a large number of acquisition techniques
deploy out-of-core and web-based rendering concepts. By provid-                               using active or passive sensing technology such as LiDAR, radar, or
ing functionality for both, thin-client and thick-client applications,                        aerial and digital cameras [Eitel et al. 2016; Ostrowski et al. 2014].
the system scales for client devices that are vastly different in com-                        Integrated into a variety of carrier platforms such as airplanes, he-
puting capabilities. Different 3D point-based rendering techniques                            licopters, UAVs, cars, trains, and robots, the sensing technology
and post-processing effects are provided to enable task-specific                              can capture data at different scales, ranging from small assets over
and data-specific filtering and highlighting, e.g., based on per-point                        buildings and infrastructure networks up to entire cities and coun-
surface categories or temporal information. A set of interaction                              tries [Kersten et al. 2016; Langner et al. 2016; Remondino et al.
techniques allows users to collaboratively work with the data, e.g.,                          2013]. The resulting data sets are essential for a growing number
by measuring distances and areas, by annotating, or by selecting and                          of applications in domains such as land surveying, urban planning,
extracting data subsets. Additional value is provided by the system’s                         landscape architecture, environmental monitoring, disaster man-
ability to display additional, context-providing geodata alongside                            agement, construction as well as spatial analysis and simulation
3D point clouds and to integrate task-specific processing and anal-                           [Eitel et al. 2016; Nebiker et al. 2010; Pătrăucean et al. 2015].
ysis operations. We have evaluated the presented techniques and                                  By their very nature, 3D point clouds are unstructured and do
the prototype system with different data sets from aerial, mobile,                            not contain or imply any order or connectivity between individ-
and terrestrial acquisition campaigns with up to 120 billion points                           ual points. As a consequence, traditional analysis algorithms for
to show their practicality and feasibility.                                                   geodata often struggle with 3D point clouds as they commonly
                                                                                              rely on explicitly defined connectivity information. Visualization
CCS CONCEPTS                                                                                  algorithms often apply a uniform pixel size and render style to
                                                                                              each point and, therefore, are prone to visual artifacts such as holes
• Human-centered computing → Geographic visualization; •
                                                                                              or visual clutter which severely limits perception, interaction, and
Computing methodologies → Computer graphics; Point-based
                                                                                              navigation [Richter et al. 2015]. As a remedy, GIS applications fre-
models;
                                                                                              quently use 3D point clouds only as input data to derive mesh
                                                                                              based 3D models (e.g., 3D city models, terrain models) [Berger et al.
KEYWORDS                                                                                      2014], leading to a loss of precision, density and data quality. In ad-
3D Point Clouds, web-based rendering, point-based rendering                                   dition, deriving 3D meshes constitutes a time consuming and only
                                                                                              semi-automatic process that does not scale for massive data sets.
                                                                                              Moreover, improved scanning hardware and novel carrier systems,
                                                                                              which get cheaper and easier-to-use, result in more dense 3D point
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed         clouds. Thus, there is a strong demand to store, manage, process
for profit or commercial advantage and that copies bear this notice and the full citation     and explore massive, arbitrarily dense 3D point clouds in order to
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
                                                                                              take advantage of their full potential and to provide an unfiltered,
to post on servers or to redistribute to lists, requires prior specific permission and/or a   detailed representation of captured sites.
fee. Request permissions from permissions@acm.org.                                               In this paper, we present a web-based visualization system for
Web3D ’18, June 20–22, 2018, Poznan, Poland
                                                                                              massive 3D point clouds (Fig. 1) based on spatial data structures
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5800-2/18/06. . . $15.00                                                  and level-of-detail representations that provide efficient access to
https://doi.org/10.1145/3208806.3208816                                                       arbitrary subsets of the 3D point cloud stored on a central server
Web3D ’18, June 20–22, 2018, Poznan, Poland                                                Sören Discher, Rico Richter, and Jürgen Döllner




Figure 1: Example of a massive 3D point cloud rendered with our web-based system. Context-providing geodata such as 2D
maps and 3D terrain models can be integrated into the visualization.


component. By combining out-of-core rendering concepts with web-           A general overview of point-based rendering techniques is pro-
based rendering concepts massive data sets can be simultaneously        vided by [Gross and Pfister 2011]. Photorealistic rendering tech-
distributed to and interactively visualized on an arbitrary number of   niques [Preiner et al. 2012; Schütz and Wimmer 2015a] focus on
client devices with different computation capabilities. To facilitate   minimizing artifacts such as visual clutter or visible holes between
a collaborative inspection and to highlight task-relevant aspects of    neighboring points by applying appropriate size, orientation and
the data, different point-based rendering and interaction techniques    color schemes to each point. Non-photorealistic rendering tech-
are implemented that can be combined and configured by the user.        niques [Simons et al. 2014; Zhang et al. 2014], on the other hand,
In addition, the system can take advantage of per-point attributes      deal with the fuzziness of a 3D point cloud and highlight edges and
generated by additional point-cloud analysis services. We evaluate      structures, commonly without requiring any additional attributes
our system with real-world data sets containing up to 120 billion       apart from a point’s spatial position [Boucheny 2009; Pintus et al.
points. Results show that the system is capable to provide a powerful   2011]. In our approach, we implement both rendering styles, which
component in production workflows to manage, distribute and             can be switched at runtime. State-of-the-art out-of-core render-
share 3D point clouds.                                                  ing concepts for massive 3D point clouds, initially introduced by
                                                                        [Rusinkiewicz and Levoy 2000], typically use spatial data structures
                                                                        such as quadtrees [Gao et al. 2014], octrees [Elseberg et al. 2013], or
                                                                        KD-trees [Goswami et al. 2013] to subdivide the data into smaller
2   RELATED WORK                                                        subsets that can be selected dynamically for rendering tasks.
3D point clouds represent a universal data category for a large            Recent approaches combine out-of-core and web-based render-
number of geospatial applications [Eitel et al. 2016; Rüther et al.     ing concepts to enable an ubiquitous visualization of 3D point
2012]; many approaches exist to enhance information implicitly          clouds [Rodriguez et al. 2012; Wand et al. 2008]. With Potree, [Schütz
contained in 3D point clouds by deriving information about sur-         and Wimmer 2015b] propose a thick-client approach for arbitrary
face categories for each point, typically based on local topological    large data sets, which is adapted by [Martinez-Rubi et al. 2015]
analysis [Chen et al. 2017] or by deep learning concepts [Boulch        to interactively present a massive data set of the Netherlands. An
et al. 2017; Huang and You 2016]. [Richter et al. 2013] show how        alternative thick-client renderer for 3D point clouds named Plasio
to efficiently identify changes in multi-temporal data sets, which      was introduced by [Butler et al. 2014]. Using open-source libraries
contain data acquired at different points in time. [Awrangjeb et al.    such as Entwine and Greyhound massive data sets can be streamed
2015] combine surface categories and change detection results to        interactively. While both approaches provide effective interaction
filter detected changes based on semantics. Our approach provides       and inspection techniques for 3D point clouds, they offer only mini-
efficient means to integrate such analyses as separate web process-     mal support to integrate additional, context-providing geodata (e.g.,
ing services [Müller and Pross 2015]. Furthermore, analysis results     shapes, 2D maps). While thin-client approaches that delegate the
can be shared, explored and inspected.                                  rendering to the server side have been successfully implemented in
                                                                        the past [Christen and Nebiker 2015; Döllner et al. 2012; Gutbell et al.
A Scalable WebGL-based Approach for Visualizing Massive 3D Point Clouds                                                                             Web3D ’18, June 20–22, 2018, Poznan, Poland


                                              Processing Engine                                              Rendering Engine                                              Thin
     Active                                                                                                                                                                Client
    Sensing
                                                                                        Level-of-Detail                            Panorama
    Devices                       Filtering             Change Detection
                                                                                        Selection
                                                                                                               Rendering
                                                                                                                                   Provision                                Thick
                                                                                                                                                                            Client
                                  Level-of-Detail       Surface Category                Memory
                                                                                                               Image Compositing   Data Provision
                                  Generation            Identification                  Management
                                                                                                                                                                    Interaction
    Passive             3D                                                                                                                                            Handler
    Sensing        Point Clouds
    Devices


                                                    Point Cloud Manager                                     Geodata Manager                                       Workspace Manager



                                                           Point           LoD Data
                                      3D Points                                                           2D Maps          3D Shapes                Annotations   Views      Measurements   Selections
                                                         Attributes        Structures




Figure 2: System architecture showing data flow between integration, processing, visualization, and interaction components.


2016], those contributions typically focus on mesh-based geometry                                               R10 Provision of interaction techniques to inspect (e.g., mea-
instead of 3D point clouds. To generate stereoscopic panoramas                                                      suring of distances, areas, volumes) and annotate 3D point
we implemented the theoretical concepts described by [Peleg et al.                                                  clouds.
2001] by means of modern 3D computer graphics.                                                                  R11 Basic user management to customize data access.
   Systems for the efficient management of massive 3D point clouds                                              R12 Capabilities to share specific rendering configurations, an-
have been recently presented and evaluated by [Cura et al. 2017],                                                   notations and measurements with others (e.g., via link).
[van Oosterom et al. 2017], and [Poux et al. 2016]. However, those
contributions focus on the efficient storage, retrieval and processing
                                                                                                              4      CONCEPTS
of the stored data sets. Less emphasis is put on the collaborative
exploration, inspection and manipulation of the stored data sets.                                             We have addressed the aforementioned requirements in the design
                                                                                                              and implementation of our web-based system that seamlessly com-
                                                                                                              bines functionality to integrate, process, and collaboratively explore
3     REQUIREMENTS                                                                                            massive, heterogeneous 3D point clouds as well as supplementary,
We have identified the following requirements that need to be                                                 context-providing geodata. The proposed system (Fig. 2) consists
addressed by a system for the web-based visualization and collabo-                                            of the following major components:
rative exploration of massive 3D point clouds:
                                                                                                              4.1          Point Cloud Manager
    R1 Use of 3D point clouds as a fundamental geometry type
                                                                                                              In our approach, 3D point clouds are organized in a single, ho-
       instead of generalized mesh-based representations to enable
                                                                                                              mogeneous spatial data model. Access to that model is handled
       a direct and unfiltered provision of the data.
                                                                                                              by the point cloud manager storing spatial information and addi-
    R2 No limitations regarding used acquisition methods as well
                                                                                                              tionally provided or computed per-point attributes (e.g., temporal
       as density, resolution, and scale of the data (e.g., hundreds
                                                                                                              information or surface categories) (R1). Level-of-detail represen-
       of billions of points, complete countries).
                                                                                                              tations [Elseberg et al. 2013; Goswami et al. 2013] are required to
    R3 Support for varying hardware platforms and computation
                                                                                                              efficiently access arbitrary data subsets of any size based on spatial,
       capabilities, ranging from high-end desktop computers to
                                                                                                              temporal or any other attributes. These representations as well as
       low-end mobile devices.
                                                                                                              additional per-point attributes can be generated by the processing
    R4 Distributed data storage to enable load balancing and to
                                                                                                              engine (Section 4.4) (R2). While the point cloud manager logically
       adjust for data specific requirements (e.g., certain 3D point
                                                                                                              acts as a singular component, the data itself may be stored in a
       clouds might have to be stored on a specific server).
                                                                                                              distributed infrastructure, e.g., to maximize data throughput and
    R5 Capabilities to prepare and clean up 3D point clouds for the
                                                                                                              network transfer rates or to account for data specific requirements
       visualization (e.g., noise and outlier removal).
                                                                                                              regarding server location and data security (R4).
    R6 Capabilities to conduct task and data specific analyses on
       3D point clouds (e.g., surface category extraction) to provide
       adaptive and task specific content.                                                                    4.2          Workspace Manager
    R7 Visualization of analysis results (e.g., surface categories) to                                        The workspace manager handles information specific to a workspace,
       enable task specific highlighting and filtering.                                                       i.e., each user‘s private view of a specific data subset containing
    R8 Capabilities to compare and show differences between 3D                                                custom selections, measurements, annotations, view positions, and
       point clouds from different points in time of the same site                                            angles. Per default, each user operates in its own private workspace
       (i.e., change detection).                                                                              rather than sharing one globally with everyone else to avoid con-
    R9 Integration of supplementary, context-providing geodata                                                flicting modifications (R11). However, a given workspace may be
       such as 2D maps.                                                                                       shared via links (R12). Each user may also own multiple workspaces.
Web3D ’18, June 20–22, 2018, Poznan, Poland                                               Sören Discher, Rico Richter, and Jürgen Döllner




              (a) Measuring of distances between points.                     (b) Measuring of areas defined by multiple points.




              (c) Annotation of selected points or areas.                               (d) Selecting areas of interest.



                         Figure 3: Overview of implemented interaction techniques for 3D point clouds.


4.3   Geodata Manager                                                  easily integrated into the system. The results of each operation
By application-specific geodata, we refer to additional geodata that   are automatically stored by the point cloud manager and can be
should be used and rendered in combination with a 3D point cloud       seamlessly integrated by the rendering engine (Section 4.5) into
to provide application-specific information layers (R9). Examples      depictions of the corresponding site (R7).
are digital terrain models, aerial images, BIM models, or 3D city
models. Similar to 3D point clouds, these data types also require      4.5    Rendering Engine
supplemental level-of-detail representations to allow for an inter-
                                                                       Providing the core functionality of our system, the rendering en-
active visualization. Application-specific geodata can be stored and
                                                                       gine is responsible for interactively visualizing three types of data:
provided by independent geospatial databases or geodata services,
                                                                       (a) 3D point clouds featuring a varying number of per-point at-
access to which is handled by the geodata manager.
                                                                       tributes, (b) task-specific geodata providing context (e.g., maps
                                                                       (R9)), and (c) workspace elements resulting from user interactions
4.4   Processing Engine                                                (e.g., annotations or selection and measurement indicators (R10)).
The processing engine conducts task and data specific operations       For each of those data types the corresponding manager is queried,
on a given data subset. These operations range from (a) essential      retrieving only data subsets that are relevant for the current view
preprocessing steps (e.g., converting input data sets into a homo-     and task. To highlight certain aspects of the data (e.g., temporal
geneous georeference system or generating level-of-detail repre-       changes or surface categories in an area), different point-based
sentations), over (b) simple point cloud filtering (e.g., noise and    rendering techniques and post processing effects can be combined
outlier removal (R5)) to (c) more complex analyses (e.g., surface      (R8). Changes to the currently applied render configuration can be
category extraction and change detection (R6)) deriving additional     made dynamically via the interaction handler) (Fig. 4). In general,
per-point attributes. The operations can be accessed via web pro-      retrieved data subsets will be transferred to and rendered on client
cessing services implemented as separate web services that are         side, which minimizes the workload on the server (i.e., thick clients).
individually combined and scheduled by the processing engine.          As an alternative, server-side rendering can be applied to reduce
Thus, existing web processing services for 3D point clouds can be      the performance impact for clients (i.e., thin clients). Thus, the
A Scalable WebGL-based Approach for Visualizing Massive 3D Point Clouds                           Web3D ’18, June 20–22, 2018, Poznan, Poland




                (a) Colors extracted from aerial imagery.                            (b) Height-based gradient from black to white.




 (c) Colorization based on surface categories, i.e., green for vegetation, (d) Colors from aerial imagery combined with change detection re-
 red for buildings, and brown for ground.                                  sults, i.e., gradient from yellow to red indicates the degree of change.


                    Figure 4: Different point-based rendering styles can be selected and configured at runtime.


system scales for a broad range of devices, ranging from high-end            5.1    Level-of-Detail and Data Subset Selection
workstations to mobile devices (R3).                                         While 3D point clouds may easily contain billions of points, only
                                                                             a fraction of that data is required to render a frame. Subsets of
4.6     Interaction Handler                                                  the 3D point cloud that are manageable by available CPU and
The interaction handler is responsible for handling user interactions        GPU capabilities can be queried dynamically from the point cloud
as well as for updating the rendered data and workspace elements             manager by specifying the current view frustum as well as device
accordingly (R10). Users may                                                 specific metrics (e.g., an assigned memory budget) and task specific
                                                                             qualifiers (e.g., value ranges for selected per-point attributes) that
      • define or load workspaces,
                                                                             further filter the corresponding data sets. To enable an efficient
      • select which data subsets to render,
                                                                             subset retrieval, the data is hierarchically subdivided using multiple
      • configure the presentation of the data (with regards to ap-
                                                                             layers, i.e., for each data set, a separate spatial data structure is
        plied rendering techniques),
                                                                             generated that best compliments the spatial distribution of the
      • select, query and highlight individual points or groups of
                                                                             corresponding points (e.g., quadtrees for airborne data sets, octrees
        points,
                                                                             or kd-trees for terrestrial data sets). In turn, those spatial data
      • measure distances and areas between selected points,
                                                                             structures are integrated into an overarching quadtree, allowing
      • annotate selected points or areas,
                                                                             to efficiently answer queries stretching across multiple data sets.
      • modify annotations,
                                                                             Compared to uniform, single-layer spatial data structures, e.g., as
      • saving and loading view positions and angles.
                                                                             they are used by [Schütz and Wimmer 2015b], this avoids a time
                                                                             consuming rebalancing when new 3D point clouds are added while
5     RENDERING ENGINE IMPLEMENTATION                                        simultaneously ensuring balanced tree structures and minimal data
To seamlessly combine 3D point clouds, context providing geodata             access times. Context providing geodata and workspace elements
and interactive workspace elements into a homogeneous visualiza-             are handled in similar fashion by their corresponding manager and
tion, a multi-pass rendering pipeline is used that consists of three         are queried simultaneously when required.
distinct stages (Fig. 5):
Web3D ’18, June 20–22, 2018, Poznan, Poland                                                                                      Sören Discher, Rico Richter, and Jürgen Döllner

                        Server Component                Client/Server Component         Client Component


                     Rendering Technique
   Rendering             Repository                 Renderer                 Image        Interaction
                                                   Renderer
                                                   Renderer
   Engine            Level-of-Detail &                                     Compositer       Handler
                     Memory Manager



                                                                   G-
                                                                  G-
                                                                  Buffer
                                                                Buffer
                                                               G-B uffer
   3D Point Clouds



                                                                   G-
                                                                  G-
      Additional                                                  Buffer
                                                                Buffer
                                                               G-B uffer

      Geodata                                                                                Screen


                                                                   G-
                                                                  G-
      Workspace                                                   Buffer
                                                                Buffer
                                                               G-B uffer
      Elements
                                      GPU Memory           GPU Memory
                                        (VB O)               (FBO)




Figure 5: Overview of the rendering pipeline. Each data type
                                                                                                           Figure 6: Post-processing effects such as Eye Dome Lighting
is managed and rendered separately.
                                                                                                           facilitate visual filtering and highlighting.

5.2        Rendering
                                                                                                                                    Server Component       Client Component
After being queried from the respective managers, 3D point clouds,
context providing geodata and interactive workspace elements                                                 Thin
are rendered into separate g-buffers [Saito and Takahashi 1990],                                             Client   Database       Selection          Rendering        Rendered
                                                                                                                                                                          Images
                                                                                                                                                                                    Display


i.e., specialized frame buffer objects (FBO) combining multiple 2D
textures for, e.g., color, depth, normal, or id values. The use of id
values is important to separate point clouds from context data. Each                                         Thick
                                                                                                                                     Selection                          Rendering   Display
                                                                                                                      Database
rendered point has a unique identifier, stored in an id texture, to                                          Client
                                                                                                                                                       Relevant

allow for an efficient point selection, e.g., to implement interaction
                                                                                                                                                        Data


features (Section 5.3). In addition, different rendering styles can
be configured and applied at runtime. As an example, size and                                              Figure 7: Comparison of web-based rendering concepts:
color of each point can be modified based on selected per-point                                            Thin clients vs thick clients
attributes (e.g., surface categories, topological metrics) to enable
task specific visual filtering and highlighting (Fig. 4). Similarly,
several options exist to dynamically adjust the appearance of mesh-                                        massive numbers of clients simultaneously. Transferred data sub-
based geometry, ranging from transparency settings to changeable                                           sets are cached on client side up to a device specific limit, thus,
texture mappings.                                                                                          minimizing the frequency of data requests for subsequent frames.
                                                                                                           In fact, additional data subsets are only required if the view frus-
5.3        Image Compositing                                                                               tum changes significantly, whereas inspecting the transferred data
A final image compositing stage is used to merge the separate g-                                           or changing the applied rendering style triggers no such requests.
buffers, i.e., to combine several independently generated views of                                         Alternatively, in the sense of a thin-client approach, the data can
3D point sub-clouds into a final image. For example, image-based                                           be rendered directly on the server, supplying only the resulting
post processing effects emphasizing edges and depth differences                                            images. While this comes with the drawback of increased work-
(e.g., Screen Space Ambient Occlusion [Mittring 2007] or Eye Dome                                          load on server side as any user interactions trigger a new data
Lighting [Boucheny 2009]) can be applied at that stage to improve                                          request, hardware requirements for clients are notably reduced. A
the visual identification of structures within 3D point cloud de-                                          common optimization for such thin-client applications is to render
pictions (Fig. 6). The id textures stored by the g-buffers provide                                         and transfer cube-maps or virtual panoramas instead of individual
efficient means to identify which point was rendered at a specific                                         images [Döllner et al. 2012; Hagedorn et al. 2017]. This provides
pixel. Thus, individual points can be selected in real-time, which is                                      clients with efficient means to locally reconstruct the 3D scene for
an essential requirement to support annotating points or measuring                                         a specific view position. Thus, the data only has to be rendered
distances and areas.                                                                                       anew whenever the view center or the rendering style are modified,
                                                                                                           which significantly reduces the frequency of data requests for sub-
5.4        Web-based Rendering                                                                             sequent frames. Our system provides a thin-client application that
To accommodate for client devices with varying computation capa-                                           expands that concept, distributing not only traditional 2D panora-
bilities, different web-based rendering concepts are combined with                                         mas but also stereoscopic panoramas. Thus, emerging virtual reality
the presented rendering pipeline (Fig. 7). We provide a thick-client                                       technologies allowing for an immersive exploration of 3D point
application that uses a central server infrastructure to organize,                                         clouds even on mobile devices can be easily integrated. We generate
process, select and distribute the data, but delegates the actual                                          those stereoscopic panoramas by rendering several equally-sized
rendering of selected data subsets to the clients. This approach                                           image strips along a viewing circle that are stitched together in a
significantly reduces workload on server side, allowing to serve                                           post-processing step [Peleg et al. 2001]. The visual quality of the
A Scalable WebGL-based Approach for Visualizing Massive 3D Point Clouds                                Web3D ’18, June 20–22, 2018, Poznan, Poland

Table 1: Devices used to evaluate the rendering engine. All web browsers were updated to the latest version as of 04/20/2018.

       Client Device                       CPU          Main Memory             GPU                             Evaluated Web Browsers
      Lenovo M710t              Intel Core i7-6700           32GB       GeForce GTX 1050Ti                Chrome, Firefox, Opera, Edge
     Macbook Pro 13”           Intel Core i5-4278U           16GB          Intel Iris 5100                    Safari, Chrome, Firefox
        iPhone SE             Apple A9 @ 1.84 GHz             2GB        PowerVR GT7600                   Safari Mobile, Chrome Mobile
        Galaxy s7             Samsung Exynos 8890             4GB       ARM Mali-T880 MP12           Samsung Internet Browser, Chrome Mobile


                                                                             applications, server-side rendering is based on OpenGL, glbinding 4
                                                                             and GLFW 5 . On client-side, Three.js 6 , WebGL, and WebVR Poly-
                                                                             fill 7 are combined to display 2D as well as stereoscopic panoramas.
                                                                             For data compression we use gzip 8 and lodePNG 9 , respectively.
                                                                             Evaluated 3D point clouds are represented by separate kd-trees,
                                                                             that in turn are integrated into an overarching quadtree. We opted
     (a) Terrestrial indoor scan.            (b) Mobile mapping scan.        to use kd-trees to optimize the balancedness of the tree structures,
                                                                             speeding up the subset retrieval, albeit at the cost of a prolonged
                                                                             preprocessing. Those spatial data structures and corresponding
                                                                             data subsets are serialized into files acting as a point cloud data-
                                                                             base. Similar, file-based approaches are applied to store and access
                                                                             context-providing geodata and workspace elements.
                                                                                 All server side operations were performed on a server featuring
                                                                             an AMD Ryzen 7 1700 CPU, 32 GB main memory and an NVIDIA
 (c) Airborne scan of a city (d) Airborne scan of a city                     GeForce GTX 1070 with 8 GB device memory. The test data sets
 (zoomed out)                (zoomed in)                                     include a terrestrial, indoor scan of an individual site (1.33 billion
                                                                             points), a mobile mapping scan (2.57 billion points) and a mas-
                                                                             sive, multi-temporal data set of an urban region (120 billion points)
 Figure 8: Scenes used during the performance evaluation.                    captured by airborne devices. For all data sets essential prepro-
                                                                             cessing steps (i.e., spatial data structure generation) and filtering
Table 2: Average data throughput of the processing engine.                   (i.e., noise and outlier removal) were performed by the processing
                                                                             engine. In addition, surface categories (i.e., ground, building, vege-
                                                                             tation) and changes in comparison to earlier scans were extracted
          Processing Operation              Average Data Throughput          for the airborne data set, allowing to evaluate the system’s ability
      Noise & Outlier Filtering                  1.26B pts/hour              to dynamically combine different rendering styles. The average
     Surface Category Extraction                 0.10B pts/hour              data throughput for the applied processing operations is listed in
          Change Detection                       1.42B pts/hour              Table 2.
         Kd-Tree Generation                      4.85B pts/hour                  The rendering engine was evaluated based on four different
                                                                             scenes (Fig. 8) with client applications running on a number of
                                                                             different devices and web browsers (Table 1). Similar to [Schütz and
panoramas depends on the requested resolution as well as the num-            Wimmer 2015b], our thick client implementation allows to render
ber of image strips; both settings can be specified upon requesting          several millions of points simultaneously at interactive frame rates
a new panorama. To further reduce overall network load, both ap-             (i.e., >30 fps) on standard desktop computers and notebooks (Ta-
plications dynamically compress and decompress the transferred               ble 4). On mobile devices, frame rates are significantly lower due to
data, using common standards such as gzip (for thick clients) and            the more limited computing capabilities. However, arbitrary large
png (for thin clients), respectively. We decided against using any           data sets can be visualized on all evaluated devices by assigning
lossy compression standards (e.g., jpeg compression) to maximize             device-specific memory budgets, thus, limiting the density of the
visual quality.                                                              point cloud depiction. As an alternative, our thin client implementa-
                                                                             tion provides a uniform rendering quality on all client devices since
6     PERFORMANCE EVALUATION                                                 the panoramas are generated on server side, minimizing workload
We have implemented the presented concepts on the basis of several           on client side. On all evaluated devices we measured frame rates
C++ and Javascript libraries. The processing engine uses CUDA 1              close to the corresponding display’s refresh rate (e.g., 60 fps on
and the Point Cloud Library 2 . Regarding the rendering engine, we           4 https://github.com/cginternals/glbinding
use WebGL and Cesium 3 for thick client applications. For thin client        5 http://www.glfw.org
                                                                             6 https://threejs.org
1 https://developer.nvidia.com/cuda-zone                                     7 https://github.com/immersive-web/webvr-polyfill
2 http://pointclouds.org                                                     8 http://www.gzip.org
3 https://cesiumjs.org                                                       9 http://lodev.org/lodepng/
Web3D ’18, June 20–22, 2018, Poznan, Poland                                                         Sören Discher, Rico Richter, and Jürgen Döllner

Table 3: Average data throughput of the rendering engine based on the scenes defined in Fig. 8. For thin clients, a stereoscopic
panorama was created per request. While the same, device-dependent resolution was requested for each scene, different en-
tropies affected the compressed image size.

                                            Thick Client                                              Thin Client
               Scene
                                  Transferred Data Transfer Time            Transferred Data    Panorama Generation Time           Transfer Time
            Terrestrial               156.2 MB              16.18s              4.68 MB                        5.27s                   1.36s
         Mobile Mapping               140.7 MB              14.15s              4.16 MB                        5.05s                   1.27s
      Airborne (zoomed out)            16.1 MB               3.43s              4.15 MB                        4.96s                   1.22s
      Airborne (zoomed in)             82.4 MB               8.09s              4.54 MB                        5.13s                   1.32s

   Table 4: Average performance rate of the thick client for different point budgets based on the airborne data set (Fig. 8d).

                                Transferred Data      Transferred Data
          Number of Points                                                   Lenovo M710t      Macbook Pro 13”         iPhone SE   Galaxy s7
                                (uncompressed)          (compressed)
                2M pts                29.5 MB               26.2 MB             122.63fps           53.85fps           41.83fps     39.96fps
                4M pts                57.4 MB               50.9 MB              84.48fps           45.63fps           36.44fps     35.29fps
                6M pts                85.6 MB               76.1 MB              63.23fps           39.08fps           26.36fps     24.83fps
                8M pts               113.6 MB              100.7 MB              56.87fps           35.83fps           19.65fps     18.43fps

            Table 5: Panorama generation time for different configurations based on the terrestrial data set (Fig. 8a).

                                                                               Panorama Generation Time
                            Resolution      Transferred Data
                                                                  90 image strips 120 image strips 160 image strips
                           2360x1600 px          2.33 MB                1.88s               2.26s                 2.29s
                           2360x3200 px          4.68 MB                4.20s               4.68s                 5.27s
                           2360x6400 px          9.35 MB                6.17s               7.14s                 7.88s


the Galaxy S7), making our approach applicable to state-of-the-art              can be conducted on the already transferred data and does not
VR devices such as GearVR or Oculus Rift. The performance of                    trigger any new data requests.
the panorama generation is primarily influenced by the requested
resolution and to a lesser degree on the number of image strips
used (Table 5).                                                                 7   CONCLUSION AND FUTURE WORK
    For all evaluated scenes, thick client applications require to trans-       Web-based visualization and exploration of massive 3D point clouds
fer significantly more data for an individual scene than thin clients           from aerial, mobile, or terrestrial data acquisitions represent a key
as long as no reusable data subsets have been cached from previous              feature for today’s and future systems and applications dealing
requests, even if gzip compression is applied (Table 3). However,               with digital twins of our physical environment. In our web-based
they do not require all those data subsets at once, allowing to up-             approach, we show a system architecture that scalably visualizes
date the scene progressively. Furthermore, while exploring a 3D                 massive 3D point clouds to web-based client devices. To cope with
point cloud, the view will usually change only gradually across                 extremely large number of points, the implementation relies on
subsequent frames, allowing for thick clients to reuse many of the              spatial data structures and level-of-detail representations, combined
previously transferred data subsets, thus, resulting in smaller and             with different out-of-core rendering and web-based rendering con-
faster scene updates over prolonged explorations. Changes to the                cepts. Since the rendering process can be shifted from client side to
rendering style as well interaction techniques such as picking, se-             server side, the system can be easily adapted to varying network
lecting or measuring don’t trigger any additional data requests at              conditions and to clients with different computing and graphics
all and can be applied even under unstable network conditions. For              capabilities. Tests on data sets with up to 120 billion points show
thin client applications on the other hand, no parts of the previously          the usability of the system and the feasibility of the approach. Vari-
transferred data can be reused if the currently used panorama be-               ous rendering techniques allow us to filter and highlight subsets of
comes invalid: Navigating -apart from merely looking around from                the data based on any available per-point attributes (e.g., surface
a fixed position- as well as rendering style adjustments require the            categories or temporal information), which is required to build
server to generate and transfer a new panorama as a replacement.                task-specific or application-specific tools. Various interaction meth-
Similar to thick clients however, picking, selecting or measuring               ods (e.g., for collaborative measurements and annotations), built-in
                                                                                support to display context-providing, mesh-based geodata, and
A Scalable WebGL-based Approach for Visualizing Massive 3D Point Clouds                                              Web3D ’18, June 20–22, 2018, Poznan, Poland


the possibility to conduct different processing and analysis oper-                            Computers, Environment and Urban Systems 36, 1 (2012), 54–64.
ations provide additional features. Our system could be further                            Thomas P Kersten, Heinz-Jürgen Przybilla, Maren Lindstaedt, Felix Tschirschwitz, and
                                                                                              Martin Misgaiski-Hass. 2016. Comparative geometrical investigations of hand-held
extended by integrating additional analyses (e.g., for asset detec-                           scanning systems. ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial
tion, or surface segmentation) [Jochem et al. 2012; Teo and Chiu                              Information Sciences (2016).
                                                                                           Tobias Langner, Daniel Seifert, Bennet Fischer, Daniel Goehring, Tinosch Ganjineh,
2015] as well as by specialized interaction techniques. For example,                          and Raúl Rojas. 2016. Traffic awareness driver assistance based on stereovision,
[Scheiblauer and Wimmer 2011] and [Wand et al. 2008] propose                                  eye-tracking, and head-up display. In Proceedings of ICRA 2016. 3167–3173.
spatial data structures that allow for an interactive editing of 3D                        Oscar Martinez-Rubi, Stefan Verhoeven, Maarten Van Meersbergen, M Schûtz, Peter
                                                                                              Van Oosterom, Romulo Gonçalves, and Theo Tijssen. 2015. Taming the beast:
point clouds. In addition, sophisticated visualization techniques                             Free and open-source massive point cloud web visualization. In Proceedings of the
for multi-temporal 3D point clouds are becoming more and more                                 Capturing Reality Forum 2015.
important to understand captured environments.                                             Martin Mittring. 2007. Finding next gen: Cryengine 2. In ACM SIGGRAPH 2007 courses.
                                                                                              ACM, 97–121.
                                                                                           Matthias Müller and Benjamin Pross. 2015. OGC WPS 2.0 interface standard. Open
ACKNOWLEDGMENTS                                                                               Geospatial Consortium Inc. (2015).
                                                                                           Stephan Nebiker, Susanne Bleisch, and Martin Christen. 2010. Rich point clouds in
We thank Pawel Böning and Pascal Führlich for their contributions                             virtual globes–A new paradigm in city modeling? Computers, Environment and
to the thin client implementation. Data sets have been provided by                            Urban Systems 34, 6 (2010), 508–517.
                                                                                           Steve Ostrowski, Grzegorz Jóźków, Charles Toth, and Benjamin Vander Jagt. 2014.
Illustrated Architecture, SHH sp. z o.o. and virtualcitySYSTEMS.                              Analysis of point cloud generation from UAS images. ISPRS Annals of the Pho-
                                                                                              togrammetry, Remote Sensing and Spatial Information Sciences 2, 1 (2014), 45–51.
                                                                                           Viorica Pătrăucean, Iro Armeni, Mohammad Nahangi, Jamie Yeung, Ioannis Brilakis,
REFERENCES                                                                                    and Carl Haas. 2015. State of research in automatic as-built modelling. Advanced
Mohammad Awrangjeb, Clive S Fraser, and Guojun Lu. 2015. Building change detection            Engineering Informatics 29, 2 (2015), 162–171.
   from LiDAR point cloud data based on connected component analysis. ISPRS Annals         Shmuel Peleg, Moshe Ben-Ezra, and Yael Pritch. 2001. Omnistereo: Panoramic stereo
   of the Photogrammetry, Remote Sensing and Spatial Information Sciences 2 (2015),           imaging. IEEE Transactions on Pattern Analysis and Machine Intelligence 23, 3 (2001),
   393–400.                                                                                   279–290.
Matthew Berger, Andrea Tagliasacchi, Lee Seversky, Pierre Alliez, Joshua Levine,           Ruggero Pintus, Enrico Gobbetti, and Marco Agus. 2011. Real-time Rendering of Mas-
   Andrei Sharf, and Claudio Silva. 2014. State of the art in surface reconstruction          sive Unstructured Raw Point Clouds Using Screen-space Operators. In Proceedings
   from point clouds. In EUROGRAPHICS star reports, Vol. 1. 161–185.                          of VAST 2011. 105–112.
Christian Boucheny. 2009. Interactive Scientific Visualization of Large Datasets: To-      Florent Poux, Pierre Hallot, Romain Neuville, and Roland Billen. 2016. Smart point
   wards a Perceptive-Based Approach. Ph.D. Dissertation. Université Joseph Fourier,          cloud: Definition and remaining challenges. ISPRS Annals of the Photogrammetry,
   Grenoble.                                                                                  Remote Sensing and Spatial Information Sciences 4 (2016), 119–127.
Alexandre Boulch, Bertrand Le Saux, and Nicolas Audebert. 2017. Unstructured               Reinhold Preiner, Stefan Jeschke, and Michael Wimmer. 2012. Auto Splats: Dynamic
   point cloud semantic labeling using deep segmentation networks. In Eurographics            Point Cloud Visualization on the GPU.. In Proceedings of the EGPGV. 139–148.
   Workshop on 3D Object Retrieval, Vol. 2. 1.                                             Fabio Remondino, Maria Grazia Spera, Erica Nocerino, Fabio Menna, Francesco Nex,
Howard Butler, David C Finnegan, Peter J Gadomski, and Uday K Verma. 2014. plas.              and Sara Gonizzi-Barsanti. 2013. Dense image matching: comparisons and analyses.
   io: Open Source, Browser-based WebGL Point Cloud Visualization. In AGU Fall                In Proceedings of DigitalHeritage 2013, Vol. 1. 47–54.
   Meeting Abstracts.                                                                      Rico Richter, Sören Discher, and Jürgen Döllner. 2015. Out-of-core visualization of
Dong Chen, Ruisheng Wang, and Jiju Peethambaran. 2017. Topologically aware                    classified 3d point clouds. In 3D Geoinformation Science. Springer, 227–242.
   building rooftop reconstruction from airborne laser scanning point clouds. IEEE         Rico Richter, Jan E Kyprianidis, and Jürgen Döllner. 2013. Out-of-Core GPU-based
   Transactions on Geoscience and Remote Sensing 55, 12 (2017), 7032–7052.                    Change Detection in Massive 3D Point Clouds. Transactions in GIS 17, 5 (2013),
Martin Christen and Stephan Nebiker. 2015. Visualisation of complex 3D city models            724–741.
   on mobile webbrowsers using cloud-based image provisioning. ISPRS Annals of the         Marcos B Rodriguez, Enrico Gobbetti, Fabio Marton, Ruggero Pintus, Giovanni Pin-
   Photogrammetry, Remote Sensing and Spatial Information Sciences 2 (2015), 517–522.         tore, and Alex Tinti. 2012. Interactive Exploration of Gigantic Point Clouds on
Rémi Cura, Julien Perret, and Nicolas Paparoditis. 2017. A scalable and multi-purpose         Mobile Devices.. In 13th International Conference on Virtual Reality, Archaeology
   point cloud server (PCS) for easier and faster point cloud data management and             and Cultural Heritage. 57–64.
   processing. ISPRS Journal of Photogrammetry and Remote Sensing 127 (2017), 39–56.       Szymon Rusinkiewicz and Marc Levoy. 2000. QSplat: A multiresolution point rendering
Jürgen Döllner, Benjamin Hagedorn, and Jan Klimke. 2012. Server-based rendering of            system for large meshes. In Proceedings of the 27th annual conference on Computer
   large 3D scenes for mobile devices using G-buffer cube maps. In Proceedings of the         graphics and interactive techniques. 343–352.
   17th International Conference on 3D Web Technology. 97–100.                             Heinz Rüther, Christoph Held, Roshan Bhurtha, Ralph Schroeder, and Stephen Wessels.
Jan UH Eitel, Bernhard Höfle, Lee A Vierling, Antonio Abellán, Gregory P Asner,               2012. From point cloud to textured model, the zamani laser scanning pipeline in
   Jeffrey S Deems, Craig L Glennie, Philip C Joerg, Adam L LeWinter, Troy S Magney,          heritage documentation. South African Journal of Geomatics 1, 1 (2012), 44–59.
   et al. 2016. Beyond 3-D: The new spectrum of lidar applications for earth and           Takafumi Saito and Tokiichiro Takahashi. 1990. Comprehensible rendering of 3-D
   ecological sciences. Remote Sensing of Environment 186 (2016), 372–392.                    shapes. In ACM SIGGRAPH Computer Graphics, Vol. 24. ACM, 197–206.
Jan Elseberg, Dorit Borrmann, and Andreas Nüchter. 2013. One billion points in             Claus Scheiblauer and Michael Wimmer. 2011. Out-of-core selection and editing of
   the cloud–an octree for efficient processing of 3D laser scans. ISPRS Journal of           huge point clouds. Computers & Graphics 35, 2 (2011), 342–351.
   Photogrammetry and Remote Sensing 76 (2013), 76–88.                                     Markus Schütz and Michael Wimmer. 2015a. High-quality point-based rendering using
Zhenzhen Gao, Luciano Nocera, Miao Wang, and Ulrich Neumann. 2014. Visualizing                fast single-pass interpolation. In Proceedings of Digital Heritage 2015. 369–372.
   aerial LiDAR cities with hierarchical hybrid point-polygon structures. In Proceedings   Markus Schütz and Michael Wimmer. 2015b. Rendering large point clouds in web
   of Graphics Interface 2014. 137–144.                                                       browsers. Proceedings of CESCG (2015), 83–90.
Prashant Goswami, Fatih Erol, Rahul Mukhi, Renato Pajarola, and Enrico Gobbetti.           Lance Simons, Stewart He, Peter Tittman, and Nina Amenta. 2014. Point-based ren-
   2013. An efficient multi-resolution framework for high quality interactive rendering       dering of forest LiDAR. In Workshop on Visualisation in Environmental Sciences
   of massive point clouds using multi-way kd-trees. The Visual Computer 29, 1 (2013),        (EnvirVis), The Eurographics Association. 19–23.
   69–83.                                                                                  Tee-Ann Teo and Chi-Min Chiu. 2015. Pole-like road object detection from mobile
Markus Gross and Hanspeter Pfister. 2011. Point-based graphics. Morgan Kaufmann.              lidar system using a coarse-to-fine approach. IEEE Journal of Selected Topics in
Ralf Gutbell, Lars Pandikow, Volker Coors, and Yasmina Kammeyer. 2016. A framework            Applied Earth Observations and Remote Sensing 8, 10 (2015), 4805–4818.
   for server side rendering using OGC’s 3D portrayal service. In Proceedings of the       Peter van Oosterom, Oscar Martinez-Rubi, Theo Tijssen, and Romulo Gonçalves. 2017.
   21st International Conference on Web3D Technology. 137–146.                                Realistic benchmarks for point cloud data management systems. In Advances in 3D
Benjamin Hagedorn, Simon Thum, Thorsten Reitz, Volker Coors, and Ralf Gutbell.                Geoinformation. Springer, 1–30.
   2017. OGC 3D Portrayal Service 1.0. OGC Implementation Standard 1.0. Open               Michael Wand, Alexander Berner, Martin Bokeloh, Philipp Jenke, Arno Fleck, Mark
   Geospatial Consortium.                                                                     Hoffmann, Benjamin Maier, Dirk Staneker, Andreas Schilling, and Hans-Peter Seidel.
Jing Huang and Suya You. 2016. Point cloud labeling using 3d convolutional neural             2008. Processing and interactive editing of huge point clouds from 3D scanners.
   network. In Proceedings of the 23rd International Conference on Pattern Recognition.       Computers & Graphics 32, 2 (2008), 204–220.
   2670–2675.                                                                              Long Zhang, Qian Sun, and Ying He. 2014. Splatting lines: an efficient method for
Andreas Jochem, Bernhard Höfle, Volker Wichmann, Martin Rutzinger, and Alexander              illustrating 3D surfaces and volumes. In Proceedings of the 18th meeting of the ACM
   Zipf. 2012. Area-wide roof plane segmentation in airborne LiDAR point clouds.              SIGGRAPH Symposium on Interactive 3D Graphics and Games. 135–142.
