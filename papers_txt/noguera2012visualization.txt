   The 20th International Conference in Central Europe on Computer
              Graphics, Visualization and Computer Vision

                         in co-operation with

                         EUROGRAPHICS




         W S C G ' 2012
                   Conference Proceedings

                               Part I

                                Plzen

                           Czech Republic

                          June 26 - 28, 2012




                             Co-Chairs

Enhua Wu, University of Macau & Chinese Academy of Sciences, China
     Vaclav Skala, University of West Bohemia, Czech Republic



                             Edited by
                            Vaclav Skala



                                                Vaclav Skala – Union Agency
WSCG’2012 Conference Proceedings

Editor-in-Chief:    Vaclav Skala
                    c/o University of West Bohemia, Univerzitni 8
                    CZ 306 14 Plzen
                    Czech Republic
                    http://www.VaclavSkala.eu

Managing Editor:    Vaclav Skala


Published and printed by:
                     Vaclav Skala – Union Agency
                     Na Mazinách 9
                     CZ 322 00 Plzen
                     Czech Republic

Hardcopy:           ISBN 978-80-86943-79-4
                               WSCG 2012
             International Program Committee

Adzhiev,V. (United Kingdom)          Murtagh,F. (Ireland)
Benes,B. (United States)             Myszkowski,K. (Germany)
Bengtsson,E. (Sweden)                Pan,R. (China)
Benoit,C. (France)                   Pasko,A. (United Kingdom)
Bilbao,J. (Spain)                    Pedrini,H. (Brazil)
Biri,V. (France)                     Platis,N. (Greece)
Bittner,J. (Czech Republic)          Rojas-Sola,J. (Spain)
Bouatouch,K. (France)                Rokita,P. (Poland)
Bourke,P. (Australia)                Rudomin,I. (Mexico)
Coquillart,S. (France)               Sakas,G. (Germany)
Daniel,M. (France)                   Santos,L. (Portugal)
de Geus,K. (Brazil)                  Skala,V. (Czech Republic)
Debelov,V. (Russia)                  Slavik,P. (Czech Republic)
Feito,F. (Spain)                     Sochor,J. (Czech Republic)
Ferguson,S. (United Kingdom)         Sramek,M. (Austria)
Flaquer,J. (Spain)                   Staadt,O. (Germany)
Gavrilova,M. (Canada)                Stroud,I. (Switzerland)
Gudukbay,U. (Turkey)                 Teschner,M. (Germany)
Havran,V. (Czech Republic)           Tokuta,A. (United States)
Hege,H. (Germany)                    Triantafyllidis,G. (Greece)
Chmielewski,L. (Poland)              Vergeest,J. (Netherlands)
Chover,M. (Spain)                    Vitulano,D. (Italy)
Chrysanthou,Y. (Cyprus)              Weiss,G. (Germany)
Jansen,F. (Netherlands)              Wu,E. (China)
Klosowski,J. (United States)         Wuethrich,C. (Germany)
Magnor,M. (Germany)                  Zara,J. (Czech Republic)
Max,N. (United States)               Zemcik,P. (Czech Republic)
Molla Vaya,R. (Spain)                Zitova,B. (Czech Republic)
Muller,H. (Germany)
                                 WSCG 2012
                              Board of Reviewers

Abad,F. (Spain)                  Durikovic,R. (Slovakia)        Chrysanthou,Y. (Cyprus)
Adzhiev,V. (United Kingdom)      Eisemann,M. (Germany)          Ihrke,I. (Germany)
Ariu,D. (Italy)                  Erbacher,R. (United States)    Jansen,F. (Netherlands)
Assarsson,U. (Sweden)            Erleben,K. (Denmark)           Jeschke,S. (Austria)
Aveneau,L. (France)              Essert,C. (France)             Jones,M. (United Kingdom)
Barthe,L. (France)               Faudot,D. (France)             Juettler,B. (Austria)
Battiato,S. (Italy)              Feito,F. (Spain)               Kanai,T. (Japan)
Benes,B. (United States)         Ferguson,S. (United Kingdom)   Kim,H. (Korea)
Benger,W. (United States)        Fernandes,A. (Portugal)        Klosowski,J. (United States)
Bengtsson,E. (Sweden)            Flaquer,J. (Spain)             Kohout,J. (Czech Republic)
Benoit,C. (France)               Flerackers,E. (Belgium)        Krivanek,J. (Czech Republic)
Beyer,J. (Saudi Arabia)          Fuenfzig,C. (Germany)          Kurillo,G. (United States)
Biasotti,S. (Italy)              Galo,M. (Brazil)               Kurt,M. (Turkey)
Bilbao,J. (Spain)                Garcia Hernandez,R. (Spain)    Lay Herrera,T. (Germany)
Biri,V. (France)                 Garcia-Alonso,A. (Spain)       Lien,J. (United States)
Bittner,J. (Czech Republic)      Gavrilova,M. (Canada)          Liu,S. (China)
Bosch,C. (Spain)                 Giannini,F. (Italy)            Liu,D. (Taiwan)
Bouatouch,K. (France)            Gobron,S. (Switzerland)        Loscos,C. (France)
Bourdin,J. (France)              Gonzalez,P. (Spain)            Lucas,L. (France)
Bourke,P. (Australia)            Gudukbay,U. (Turkey)           Lutteroth,C. (New Zealand)
Bruckner,S. (Austria)            Guérin,E. (France)             Maciel,A. (Brazil)
Bruder,G. (Germany)              Hall,P. (United Kingdom)       Madeiras Pereira,J. (Portugal)
Bruni,V. (Italy)                 Hansford,D. (United States)    Magnor,M. (Germany)
Buriol,T. (Brazil)               Haro,A. (United States)        Manak,M. (Czech Republic)
Cakmak,H. (Germany)              Hasler,N. (Germany)            Manzke,M. (Ireland)
Capek,M. (Czech Republic)        Hast,A. (Sweden)               Mas,A. (Spain)
Cline,D. (United States)         Havran,V. (Czech Republic)     Masia,B. (Spain)
Coquillart,S. (France)           Hege,H. (Germany)              Masood,S. (United States)
Corcoran,A. (Ireland)            Hernandez,B. (Mexico)          Matey,L. (Spain)
Cosker,D. (United Kingdom)       Herout,A. (Czech Republic)     Matkovic,K. (Austria)
Daniel,M. (France)               Hicks,Y. (United Kingdom)      Max,N. (United States)
Daniels,K. (United States)       Horain,P. (France)             McDonnell,R. (Ireland)
de Geus,K. (Brazil)              House,D. (United States)       McKisic,K. (United States)
De Paolis,L. (Italy)             Chaine,R. (France)             Mestre,D. (France)
Debelov,V. (Russia)              Chaudhuri,D. (India)           Molina Masso,J. (Spain)
Dingliana,J. (Ireland)           Chmielewski,L. (Poland)        Molla Vaya,R. (Spain)
Dokken,T. (Norway)               Choi,S. (Korea)                Montrucchio,B. (Italy)
Drechsler,K. (Germany)           Chover,M. (Spain)              Muller,H. (Germany)
Murtagh,F. (Ireland)            Sadlo,F. (Germany)             Tian,F. (United Kingdom)
Myszkowski,K. (Germany)         Sakas,G. (Germany)             Tokuta,A. (United States)
Niemann,H. (Germany)            Salvetti,O. (Italy)            Torrens,F. (Spain)
Okabe,M. (Japan)                Sanna,A. (Italy)               Triantafyllidis,G. (Greece)
Oliveira Junior,P. (Brazil)     Santos,L. (Portugal)           TYTKOWSKI,K. (Poland)
Oyarzun Laura,C. (Germany)      Sapidis,N. (Greece)            Umlauf,G. (Germany)
Pala,P. (Italy)                 Savchenko,V. (Japan)           Vavilin,A. (Korea)
Pan,R. (China)                  Sellent,A. (Germany)           Vazquez,P. (Spain)
Papaioannou,G. (Greece)         Sheng,B. (China)               Vergeest,J. (Netherlands)
Paquette,E. (Canada)            Sherstyuk,A. (United States)   Vitulano,D. (Italy)
Pasko,A. (United Kingdom)       Shesh,A. (United States)       Vosinakis,S. (Greece)
Pasko,G. (United Kingdom)       Schultz,T. (Germany)           Walczak,K. (Poland)
Pastor,L. (Spain)               Sirakov,N. (United States)     WAN,L. (China)
Patane,G. (Italy)               Skala,V. (Czech Republic)      Wang,C. (Hong Kong SAR)
Patow,G. (Spain)                Slavik,P. (Czech Republic)     Weber,A. (Germany)
Pedrini,H. (Brazil)             Sochor,J. (Czech Republic)     Weiss,G. (Germany)
Peters,J. (United States)       Solis,A. (Mexico)              Wu,E. (China)
Peytavie,A. (France)            Sourin,A. (Singapore)          Wuensche,B. (New Zealand)
Pina,J. (Spain)                 Sousa,A. (Portugal)            Wuethrich,C. (Germany)
Platis,N. (Greece)              Sramek,M. (Austria)            Xin,S. (Singapore)
Plemenos,D. (France)            Staadt,O. ()                   Xu,D. (United States)
Poulin,P. (Canada)              Stroud,I. (Switzerland)        Yang,X. (China)
Puig,A. (Spain)                 Subsol,G. (France)             Yoshizawa,S. (Japan)
Reisner-Kollmann,I. (Austria)   Sunar,M. (Malaysia)            YU,Q. (United Kingdom)
Renaud,c. (France)              Sundstedt,V. (Sweden)          Yue,Y. (Japan)
Reshetov,A. (United States)     Svoboda,T. (Czech Republic)    Zara,J. (Czech Republic)
Richardson,J. (United States)   Szecsi,L. (Hungary)            Zemcik,P. (Czech Republic)
Rojas-Sola,J. (Spain)           Takala,T. (Finland)            Zhang,X. (Korea)
Rokita,P. (Poland)              Tang,M. (China)                Zhang,X. (China)
Rudomin,I. (Mexico)             Tavares,J. (Portugal)          Zillich,M. (Austria)
Runde,C. (Germany)              Teschner,M. (Germany)          Zitova,B. (Czech Republic)
Sacco,M. (Italy)                Theussl,T. (Saudi Arabia)      Zwettler,G. (Austria)
                                  WSCG 2012
                Communications Proceedings
                                     Contents



Kenwright,B.: A Beginners Guide to Dual-Quaternions : What They Are, How They           1
  Work, and How to Use Them for 3D Character Hierarchies
Hast,A., Marchetti,A.: An Efficient Preconditioner and a Modified RANSAC for Fast      11
  and Robust Feature Matching
Vassilev,T.I., Spanlang,B.: Fast GPU Garment Simulation and Collision Detection        19
Saito,P.T.M., de Rezende,P.J., Falcao,A.X., Suzuki,C.T.N., Gomes,J.F.: Improving       27
  Active Learning with Sharp Data Reduction
Morik,M., Masik,S., Müller,R., Köppen,V.: Exposing Proprietary Virtual Reality         35
  Software to Nontraditional Displays
Lee,G.R. , Lee,H.C. , Lee,T.M. , Yoon,G.H.: Image Abstraction with Cartoonlike         45
  Shade Representation
Klein,A., Nischwitz,A., Obermeier,P.: Contact Hardening Soft Shadows using             53
   Erosion
Schmidt,M., Guthe,M., Blanz,V.: Diffusion-based parametrization of surfaces on         59
  3D-meshes
Minoi,J.-L., Gillies,D.F., Robert,A.J.: Realistic Facial Expression Synthesis of 3D    69
  Human Face based on Real Data using Multivariate Tensor Methods
Kurowski,M.: Procedural Generation of Meandering Rivers Inspired by Erosion            79
Schiffner,D., Krömker,D.: Parallel Treecut-Manipulation for Interactive Level of       87
  Detail Selection
Nakata,N., Kakimoto,M., Nishita,T.: Animation of Water Droplets on a                   95
  Hydrophobic Windshield
Noguera,J.M., Jimenez,J.R.: Visualization of Very Large 3D Volumes on Mobile          105
  Devices and WebGL
Crumley,Z., Marais,P., Gain,J.: Voxel-Space Shape Grammars                            113
Tang,Y., Wu,Z., Zhou,M.: Interactively Simulating Fluid based on SPH and CUDA         123
Lazunin,V., Savchenko,V.: Artificial jellyfish: evolutionary optimization of          131
  swimming
Pimenta,W., Santos,L.P.: A Comprehensive Taxonomy for Three-dimensional               139
   Displays
Metzgar,J., Semwal,S.K.: Approximating the Fire Flicker effect using Local            147
  Dynamic Radiance Maps
Schumann,M., Hoppenheit,J., Müller,S.: A Matching Shader Technique for Model-         155
  Based Tracking
Tomori,Z., Gargalik,R., Hrmo,I.: Active Segmentation in 3D using Kinect Sensor        163
Marks,S., Windsor,J., Wuensche,B.: Using Game Engine Technology for Virtual           169
  Environment Teamwork Training
Leonardi,V., Mari,J.L., Vidal,V., Daniel,M.: A Morphing Approach for Kidney           179
  Dynamic Modeling : from 3D Reconstruction to Motion Simulation
Debelov,V., Kozlov,D.: Rendering of Translucent Objects, Verification and             189
  Validation of Algorithms
Klein,A., Tappert,B., Nischwitz,A., Obermeier,P.: Volumetric Percentage Closer        197
   Soft Shadows
Klicnar,L., Beran,V.: Robust Motion Segmentation for On-line Application              205
Oshita,M.: Multi-Touch Interface for Character Motion Control Using Example-          213
  Based Posture Synthesis
Hulík,R., Kršek,P.: Local Projections Method and Curvature Approximation on 3D        223
  Polygonal Models
Bahnsen,C., Dewilde,A., Pedersen,C., Tranchet,G., Madsen,C.B.: Realtime global        231
  illumination using compressed pre-computed indirect illumination textures
Reuter,A., Seidel,H.-P., Ihrke,I.: BlurTags: spatially varying PSF estimation with    239
  out-of-focus patterns
Nguyen,M.H., Wuensche,B., Delmas,P., Lutteroth,C.: 3D Models from the Black           249
  Box: Investigating the Current State of Image-Based Modeling
Krivokuca,M., Wuensche,B., Abdulla,W., Lavoué,G.: Investigating the Rate-             259
   Distortion Performance of a Wavelet-Based Mesh Compression Algorithm by
   Perceptual and Geometric Distortion Metrics
Bittorf,B., Wüthrich,C.: EmotiCon - Interactive emotion control for virtual           269
   characters
Mahiddine,A., Seinturier,J., Jean-Marc Boi,J.-M., Drap,P., Merad,D.: Performances     275
  Analysis of Underwater Image Preprocessing Techniques on the Repeatability
  of SIFT and SURF Descriptors
Bruni,V., Rossi,E., Vitulano,D.: Unsupervised Perception-based Image Restoration      283
  of Semi-transparent Degradation using Lie Group Transformations
Bugaj,M., Cyganek,B.: GPU Based Computation of the Structural Tensor for Real-        291
  Time Figure Detection
Safdar,K.: Detecting and Removing Islands in Graphics-Rendering-Based                 299
  Computations of Lower Envelopes of Plane Slabs
Minich,C.: Search for small monostatic polyhedra                                      309
Warburton,M., Maddock,S.: Creating Animatable Non-Conforming Hexahedral               317
  Finite Element Facial Soft-Tissue Models for GPU Simulation
François,A., Raffin,R., Aryal,J.: 3D modelling and analysis: ISO standard tools for   327
   air traffic
Jawad,M., Yasin,M., Sarfraz,M.S.: License Plate Detection using NMF with              335
  Sparseness constraints through still Images
Bian,X., Krim,H.: Video-based Human Motion Analysis: An Operator-based                341
   Approach
Arora,N., Kumar,A., Kalra,P.: Digital Restoration of Old Paintings                    347
Khurana,S., Brener,N, Benger,W., Karki,B., Roy,S., Acharya,S., Ritter,M.,             357
  Iyengar,S.: Multi Scale Color Coding of Fluid Flow Mixing Indicators along
  Integration Lines
Qureshi,H., Malik,M., Ahmad,M.A., Heinzl,C.: Benchmarking of De-noising               367
  Techniques for Streaking Artifacts in Industrial 3DXCT Scan Data
Graca,S., Oliveira,J.F., Realinho,V.: WorldPlus: An Augmented Reality Application     377
  with Georeferenced content for smartphones - the Android example
                        A Bigger Mathematical Picture
                           for Computer Graphics
                                                  Eric Lengyel
                                            Terathon Software LLC
                                       http://www.terathon.com/lengyel/
                                                          USA




ABSTRACT                                                        BRIEF BIOGRAPHY
Some of the most brilliant mathematical discoveries             Eric Lengyel is the founder of Terathon Software and
of the 1800s were pushed aside for over a century in            the creator of the C4 Engine, a comprehensive tech-
favor of the vector analysis and linear algebra that we         nology platform for games and virtual simulations.
are all familiar with. However, these old ideas have            He holds a Ph.D. in Computer Science from the Uni-
recently been rediscovered in the field of computer             versity of California at Davis and a Masters Degree
graphics by researchers who understand how they                 in Mathematics from Virginia Tech.
can unify many of the geometric operations that are             Eric is the best-selling author of the book Mathemat-
used every day.                                                 ics for 3D Game Programming & Computer
This talk introduces the basic concepts of the exterior         Graphics, and he is the series editor for the new
algebra and presents a bigger mathematical picture              Game Engine Gems series. Eric is also a member of
that enables a deeper understanding of the homoge-              the editorial board for the Journal of Graphics Tools,
neous representation of points, lines, and planes, as           and he is a major contributor to the successful Game
well as the operations that can be performed among              Programming Gems series.
them using the progressive and regressive products.             Eric previously worked in the advanced technology
Some emphasis is placed on the history of related               group at Naughty Dog where he developed the driver
mathematics and the past development of incomplete              architecture for the Playstation 3. Prior to that, was
pieces of the bigger picture, such as Plücker coordi-           the lead programmer for Sierra Studio’s popular ad-
nates. The goal is to help the audience unlearn some            venture game Quest for Glory V, and he worked on
longstanding misnomers in 3D geometry and to pro-               OpenGL in Apple’s graphics and imaging depart-
vide the knowledge of a larger, unified world into              ment.
which many familiar mathematical concepts fit to-
gether.
Overcoming Physical Limitations of Display Devices
                 in Rendering
                                               Karol Myszkowski
                                       Max-Planck-Institut für Informatik
                                          http://www.mpi-inf.mpg.de/
                                                    Germany




ABSTRACT                                                  BRIEF BIOGRAPHY
The knowledge of human visual system (HVS)                Karol Myszkowski is a tenured senior researcher at
enables more efficient image rendering by                 the MPI Informatik, Saarbruecken, Germany. In the
overcoming physical constraints of display devices.       period from 1993 till 2000 he served as an associate
This talk presents a number of successful examples        professor in the Department of Computer Software at
of embedding HVS models into real-time rendering          the University of Aizu, Japan. In the period from
pipelines. In particular, I discuss the problem of        1986 till 1992 he worked for Integra, Inc. a Japan-
improving the appearance of highlights and light          based, company specialized in developing rendering
sources by boosting their apparent brightness using       and global illumination software. He received his
the temporal glare technique. Also, I discuss how to      PhD (1991) and habilitation (2001) degrees in
overcome physical contrast limitations of display         computer science from Warsaw University of
devices by using the 3D unsharp masking technique         Technology (Poland). In 2011 he was awarded with a
to boost the apparent contrast. Also, I present           lifetime professor title by the President of Poland.
techniques for apparent resolution enhancement,           His research interests include perception issues in
which enable showing image details beyond the             graphics, high dynamic range imaging, global
physical pixel resolution of the display device.          illumination and rendering. Karol published and
Finally, I discuss the role of perception in context of   lectured on these topics widely including ACM
stereovision and accommodation/vergence conflict          Siggraph/Siggraph Asia Courses in 2001, 2002,
reduction                                                 2004, 2006, and 2012. He also co-chaired Rendering
                                                          Symposium in 2001, ACM Symposium on Applied
                                                          Perception in Graphics and Visualization in 2008,
                                                          Spring Conference on Computer Graphics 2008, and
                                                          Graphicon 2012
  Physically Based Weathering Simulation of Natural
        Objects Based on Biological Analysis
                                                   Enhua Wu
              State Key Lab. of Computer Science, Chinese Academy of Sciences, Beijing
                                                   &
                                  University of Macau, Macao，China




ABSTRACT                                                  BRIEF BIOGRAPHY
The Weathering effect of nature objects or natural        Dr. Enhua Wu completed his BSc in Tsinghua
scenes is a common phenomenon in our daily life.          University, Beijing in 1970 and received his Ph.D
However, little investigation has been made to the        degree from Dept. of Computer Science, University
phenomenon so far in computer graphics field. The         of Manchester, England in 1984. Since 1985 he has
weathering procedure on the nature objects such as        been working at the Institute of Software, Chinese
plants, trees, grasses etc. is a slowly changing          Academy of Sciences, as a director of the Research
process, and in fact it is involved with              a   Dept. of Fundamental Theory and Advanced
comprehensive drying procedure made towards the           Technology until 1998. Since September of 1997, he
biological structure of the nature objects, in terms of   has been also invited as a full professor of University
the shape change of the objects & the color change of     of Macau (UM).
their appearance. With regard to the shape change or          Dr. Wu’s main interests are Realistic Image
deformation, a physically based mechanical                Synthesis,     Virtual    Reality     and     Scientific
calculation is applied to the biological components       Visualization. Now he is an Associate Editor-in-
incurred by the drying effect in our solution. On the     Chief of the Journal of Computer Science and
other hand, the change of color appearance could be       Technology (Science Press and Springer) and the
simulated based on the synthesis to the color             editorial board member of TVC, CAVW, IJIG, IJVR,
spectrum of the samples collected in the weathering       IJSI. He has been also in recent years invited as a
process of the objects. The simulation based the          keynote speaker or chairing works in a number of
scheme will be demonstrated by the simulation result      international conferences such as ACM VRST2010,
to the trees, grassland, fruits etc.                      CASA2011, ACM VRCAI2008-2012, IEEE
                                                          VR2011-12 etc.
               A Beginners Guide to Dual-Quaternions
What They Are, How They Work, and How to Use Them for 3D Character Hierarchies
                                                  Ben Kenwright
                                 School of Computing Science, Newcastle University
                                      Newcastle Upon Tyne, United Kingdom
                                              b.kenwright@ncl.ac.uk


                                                      ABSTRACT
In this paper, we give a beginners guide to the practicality of using dual-quaternions to represent the rotations
and translations in character-based hierarchies. Quaternions have proven themselves in many fields of science
and computing as providing an unambiguous, un-cumbersome, computationally efficient method of representing
rotational information. We hope after reading this paper the reader will take a similar view on dual-quaternions.
We explain how dual number theory can extend quaternions to dual-quaternions and how we can use them to
represent rigid transforms (i.e., translations and rotations). Through a set of examples, we demonstrate exactly
how dual-quaternions relate rotations and translations and compare them with traditional Euler’s angles in
combination with Matrix concatenation. We give a clear-cut, step-by-step introduction to dual-quaternions,
which is followed by a no-nonsense how-to approach on employing them in code. The reader, I believe, after
reading this paper should be able to see how dual-quaternions can offer a straightforward solution of
representing rigid transforms (e.g., in complex character hierarchies). We show how dual-quaternions propose a
novel alternative to pure Euler-Matrix methods and how a hybrid system in combination with matrices results in
a faster more reliable solution. We focus on demonstrating the enormous rewards of using dual-quaternions for
rigid transforms and in particular their application in complex 3D character hierarchies.
Keywords
Dual-Quaternion, 3D, Real-Time, Character Hierarchies, Rigid Transformation
1. INTRODUCTION                                                    Dual-quaternions are interesting and important
Real-time dynamic 3D character systems combine                     because they cut down the volume of algebra. They
key framed animations, inverse kinematics (IK) and                 make the solution more straightforward and robust.
physics-based models to produce controllable,                      They allow us to unify the translation and rotation
responsive, realistic motions.        The majority of              into a single state; instead of having to define
character-based systems use a skeleton hierarchical                separate vectors. While matrices offer a comparable
composition of rigid transforms.            Each rigid             alternative to dual-quaternions, we argue that they
transform has six degrees of freedom (DOF) that                    can be inefficient and cumbersome in comparison. In
consists of three translational and three rotational               fact, dual-quaternions give us a compact, un-
components. Matrices are the most popular method                   ambiguous, singularity-free, and computational
of storing and combining these transforms. While                   minimalistic rigid transform. In addition, dual-
matrices are adequate, we ask the question, is there a             quaternions have been shown to be the most efficient
better method? In this paper, we address the                       and most compact form of representing rotation and
advantages and disadvantages of matrices while                     translation. Dual-quaternions can easily take the
proposing a novel alternative based on quaternions                 place of matrices in hierarchies at no additional cost.
called dual-quaternions. The purpose of this paper is              For rigid transform hierarchies that combine and
to present a beginner’s guide to dual-quaternions in               compare rigid transforms on a frame-by-frame bases
sufficient detail that the reader can begin to use them            (e.g., character inverse kinematics (IK) and joint
as a practical problem-solving tool for rigid character            constraints), alternative methods such as matrices
transforms. This paper covers the basics of dual-                  need to be converted to quaternions to generate
quaternions and their application to complex                       reliable contrast data; this can be done without any
hierarchical systems with many DOF.                                conversion using dual-quaternions.
                                                                   Many students have a great deal of trouble
 Permission to make digital or hard copies of all or part of       understanding essentially what quaternions are and
 this work for personal or classroom use is granted without        how they can represent rotation. So when the subject
 fee provided that copies are not made or distributed for
                                                                   of dual-quaternions is presented, it is usually not
 profit or commercial advantage and that copies bear this
 notice and the full citation on the first page. To copy           welcomed with open arms. Dual-quaternions are a
 otherwise, or republish, to post on servers or to                 break from the norm (i.e., matrices) which we hope
 redistribute to lists, requires prior specific permission         to entice the reader into embracing to represent their
 and/or a fee.



WSCG 2012 Communication Proceedings                            1                                       http://www.wscg.eu
rigid transforms. The reader should walk away from           method for representing rigid transforms instead of
this paper with a clear understanding of what dual-          matrices, and gives evidence that the results can be
quaternions are and how they can be used.                    faster with accumulated transformations of joints if
The majority of computer scientists are familiar with        the inferences per vertex are large enough.
vectors, matrices, and quaternions. They provide a           Selig [SELI11] address the key problem in computer
set of tools to help solve problems. This paper              games. Examining the problem of solving the
presents the case for adding dual-quaternions to this        equations of motion in real-time and puts forward
set of tools.                                                how dual-quaternion give a very neat and succinct
The contribution of this paper is the explanation and        way of represent rigid-body transformations.
demonstration of dual-quaternions in a sufficiently          Vasilakis [VAFU09] discussed skeleton-based rigid-
detailed way that the reader can begin to appreciate         skinning for character animation.
their practical problem-solving advantages. We use           Kuang [KMLX11] presented a strategy for creating
character-based hierarchies as a base method to              real-time animation of clothed body movement.
illustrate the realistic reward of dual-quaternions in
time critical systems (e.g., games).                         2.2. Robotics
This paper presents dual-quaternions as a method for         Pham [PPAF10] solved linked chain inverse
representing rigid transforms in complex character           kinematic (IK) problems using Jacobian matrix in the
hierarchies with a large number of DOF. We explain           dual-quaternion space.
how to implement a basic dual-quaternion class and           Malte [SCHI11] used a mean of multiple
combine dual-quaternions through straightforward             computational (MMC) model with dual-quaternions
multiplication to work in place of matrices.                 to model bodies.
The roadmap for the rest of the paper is as follows:         Ge [GVMC98] demonstrated dual-quaternions to be
we begin with a review of recent and related work            an efficient and practical method for interpolating
that emphasises the power of dual-quaternions. We            three-dimensional motions.
review familiar rigid transform methods and their            Yang-Hsing [LIWC10] calculated          the   relative
advantages and disadvantages. We then outline the            orientation using dual-quaternions.
primary reasons for using dual-quaternions and why
you would want to use them for rigid transforms over         Perez [PEMC04] formulated dynamic constraints for
other methods. We then give the background                   articulated robotic systems using dual-quaternions.
mathematical information for dual numbers,
                                                             3. FAMILIAR PHYSICAL CONCEPTS
quaternions and dual-quaternions. The following
                                                             We review the most common methods of
sections then focus on the practical aspects of dual-
                                                             representing rigid body orientations and translations
quaternions. We discuss a variety of experiments
                                                             in our physical world (three spatial dimensions).
with computer simulations and character hierarchies
                                                             While orientation and rotation are familiar concepts,
in relation to dual-quaternion. Finally, the end
                                                             there are many ways to represent them both
section presents the conclusion and proposed future
                                                             mathematically and computationally, each with their
work.
                                                             own strengths and weaknesses. We briefly describe
2. RELATED WORK                                              four of the most popular methods of representing
The dual-quaternion has been around since 1882               rigid transforms in character systems. This helps
[CLIF82] but has gained less attention compared to           illustrate the mathematical and computational issues
quaternions alone. Comparable to quaternions the             that occur. The four alternate methods we compare
dual-quaternions have had a taboo associated with            mathematically and computationally to dual-
them, whereby students avoid quaternion and hence            quaternions are:
dual-quaternions. While the robotics community has                   Matrices
started to adopt dual-quaternions in recent years, the               Axis-Angles
computer graphics community has not embraced                         Euler-Angles     + Translation
them as whole-heartedly. We review some recent                       Quaternions
work which has taken hold and has demonstrated the
                                                             Each alternative method needs to represent both the
practicality of dual-quaternions, both in robotics and
                                                             orientation and translation. In some cases this is
computer graphics.
                                                             achieved by using two separate state variables and
2.1. Computer Graphics                                       combining them separately, while matrices and dual-
Kavan [KCŽO08] demonstrated the advantages of                quaternions give us a unified state variable.
dual-quaternions in character skinning and blending.         For each case we focus on issues of interpolation,
Ivo [IVIV11] extended Kavans [KCŽO08] work with              computational speed, mathematical robustness and
dual-quaternions and qtangents as an alternative             distance metrics.



WSCG 2012 Communication Proceedings                      2                                      http://www.wscg.eu
The properties we look for to represent the rigid body                          1        0          0 
transform are:                                                                  
                                                                            X  0 cos  x        sin  x 
Robustness – be continuous and not contain any                                  0 sin  x      cos  x 
discontinuities (such as gimbal lock with Euler’s
angles which we discuss later). Contain a unique                                  cos  y 0       sin  y 
representation, where some methods contain                                                                 
                                                                            Y 0            1        0 
redundant information, such that several or an
                                                                                   sin  y 0     cos  y 
infinite number of elements can represent the same                               
transform.                                                                       cos  z     sin  z    0
Efficiency – should consume the smallest necessary                          Z   sin  z   cos  z      0 
amount of space and be computationally fast. We                                   0            0        1 
would like a minimum number of calculations to
combine      and   convert  between     alternative           Combining the translation is just a matter of rotating
representations (e.g., cost to convert between                the translational components (x, y and z) by the
matrices and Euler angles).                                   rotation.
Ease of Use – can be used without too many                    To combine and calculate interpolating differences
complications.                                                requires us to find the equivalent axis-angle of the
                                                              two orientations and extrapolate the Euler angles.
3.1. Orientation and Translation
It might seem intuitive how objects are rotated and                   Create a matrix for each Euler angle.
translated. For example, we can pick up any object                    Multiply the three matrices together.
around us and spin (rotate) and translate (move) it                   Extract axis-angle from resulting matrix.
without thinking. However, how do we model this               Converting, combining, and extracting Euler angles
computationally and mathematically? The following             is computationally expensive.      Moreover, Euler
sub-sections are devoted to the explanation and               angles can have discontinuities around 0 and 2,
understanding of these basic principles.                      since the components live on separate circles and not
For methods which are formed from separate                    a single vector space.
orientation and translational information, we can
analyse their workings by considering orientation and         3.3.1. Advantages
translation separately and combining them at the end          People prefer Euler angles as they can comprehend
of each transform.                                            them effortlessly and can create orientations with
                                                              them without difficulty. They are also very intuitive
3.2. Translation                                              and have a long history in physics and graphics and
The translation coordinates are relatively simple to          can make certain integrals over rotational space
work with. They compose of the scalar values along            easier.
each of the principle axes (x, y and z). The computed         Euler angles are minimalistic and require only three
orientations are combined with the translations by            parameters; however, we show later how four
rotating the principle axis.                                  parameters are better than three. Furthermore, since
                                                              the angles are used directly, there is no drifting or the
3.3. Euler-Angles
                                                              need for normalization.
A familiar way of representing the orientation and
translation in character systems is to factor it into         3.3.2. Disadvantages
three sequential angles around the principle                  Euler angles suffer from singularities - angles will
orthogonal axes (x, y and z).                                 instantaneously change by up to 2 radians as other
Euler’s angles in 3D do not (in-general) commute              angles go through the singularity; Euler angles are
under composition.                                            virtually impossible to use for sequential rotations.
                                                              There are twelve different possible Euler angle
In practice, the angles are used by inserting them into
                                                              rotation sequences - XYZ, XYX, XZY, and so on.
matrices. The product of the three angle-matrices
produces the Euler angle set. There are twelve                There is no one "simplest" or "right" set of Euler
possible products: XYZ, XYX, YZX, YZY, ZXY,                   angles. To derive a set of Euler angles you must
                                                              know which rotational sequence you are using and
ZXZ, XZY, XZX, YXZ, YXY, ZYX, and ZYZ.
These are the order the rotations are applied in. For         stick to it.
example, the factorization XYZ, would mean rotate             In practice when Euler angles are needed; the
round X then Y then Z.                                        underlying rotation operations are done using
                                                              quaternions and are converted to Euler angles for the
To work with Euler angles we convert them to
                                                              task at hand.
matrices:




WSCG 2012 Communication Proceedings                       3                                              http://www.wscg.eu
3.3.3. Gimbals Lock                                            3.4.2. Disadvantages
The coordination singularity in Euler’s angles is              We can renormalize the axis since it is a unit vector,
commonly referred to as gimbals lock. A gimbal is a            but numerical errors can still creep into the angle
physical device consisting of spherical concentric             portion.
hoops with pivots connecting adjacent hoops,                   Infinite number of angle choices (multiples of 2), so
allowing them to rotate within each other (see Figure          two axis-angle pairs can still refer to the same
1).                                                            rotation but be different.
                                                               Axis-angle interpolation cannot be done using linear
                                                               interpolation of the four elements. Interpolating
                                                               between the four elements naively in this way does
                                                               not give the shortest path.
                                                               Interpolating the angle alone can introduce
                                                               discontinuities as the angle crosses from 0 to 2.
                                                               These ‘jumps’ are highly undesirable and can cause
                                                               anarchy with the interpolation and numerical
                                                               integration schemes.

Figure 1. Gimbal with points of rotation indicated.            3.5. Matrices
                                                               Representing a rigid transform using a matrix we
A gimbal is constructed by aligning three rings and
                                                               extend a 3x3 rotation matrix to include translation
attaching them orthogonally. Gimbals are often seen
                                                               information which makes it a 4x3 matrix. While a
in gyroscopes used by the aeronautical industry.
                                                               4x3 matrix is the most efficient, on most occasions a
As objects are rotated, they approach gimbal lock the          4x4 matrix is used because of availability.
singularity will cause numerical ill-conditioning,
                                                               The 3x3 part of the matrix consists of three
often evidented physically by the gimbal wiggling
                                                               orthogonal column vectors which are of unit
madly around as it operates near the singularity. This
                                                               magnitude.
is one reason why the aerospace industry, early on,
switched to quaternions to represent orientation –             A transform matrix can transform a vector coordinate
satellites, rockets and airplanes would have their             by simply matrix multiplication:
navigation gyro lock up and could cause them to                                       y  Tx
crash.
                                                               where T is a transform matrix, x a vector coordinate
3.3.4. Interpolation                                           and y the transformed result.
The major problem with Euler interpolation is that
                                                               If the position and basis vectors are known, the
they have problems while interpolating near gimbals
                                                               transform matrix can trivially be produced, because
lock regions.      When close to a gimbal lock
                                                               each of the columns in the 3x3 part of the matrix
singularity the interpolation become jittery and noise
                                                               represent the base vectors and the bottom row the
ridden; eventually becoming random and unstable as
                                                               translation.
it converges on the singularity.
                                                               The combination of matrix elements is achieved
If Euler angles are interpolated linearly the resulting
                                                               through simple multiplication. Matrices are not
path will not take the shortest path between the
                                                               commutative     and    therefore  their   matrix
endpoints as it does in vector space [ALMA92].
                                                               representation of rigid body transforms is non-
3.4. Axis-Angle                                                commutative as well.
The axis-angle is represented by a unit axis and angle         3.5.1. Advantages
( nˆ, ) pair. This axis-angle representation can easily       Matrices are taught in linear algebra early on in
be converted to and from a matrix.                             colleges so this makes them more familiar and
It is difficult to combine the axis-angle elements in          favourable. In addition, a great many algorithms
their native form; usually being converted to an               have been formulated and tested with matrices and so
alternate representation for concatenation (e.g.,              people choose them instinctively first.
matrices, quaternions).                                        3.5.2. Disadvantages
3.4.1. Advantages                                              While matrices might seem to be the utopia, they in-
The greatest single advantage of the axis-angle                fact can be found to have several problems.
representation is that it directly represents the action       Firstly, they take a minimum of 12 parameters to
of rotation, while being straightforward and intuitive         represent a structure with only six DOF; if memory is
to work with.                                                  at a premium this can be undesirable.



WSCG 2012 Communication Proceedings                        4                                      http://www.wscg.eu
Secondly, the rotational part of the matrix is                 components, the dual operator  is used in the same
composed of orthogonal columns which can drift and             way.
introduce unwanted scaling and sheering. We can re-            The dual number theory can be extended to other
normalize the matrix using Gram-Schmidt method                 concepts, such as vectors and real numbers, but we
[GILB86] but this can be computationally expensive.            focus on their applicability in conjunction with
Thirdly, interpolating between matrices is difficult.          quaternions to represent rotation and translation
The three columns forming the orthogonal axis                  transforms.
directions in the rotation part of the matrix do not
represent the vector space and cannot be interpolated.         5.1. Dual Number Arithmetic Operations
                                                               Dual numbers can perform                                the       fundamental
Finally, it is difficult to visualize a matrix and the
                                                               arithmetic operations below:
axis-angle component about which it will rotate and
translate.                                                     Addition
                                                                    (rA  d A )  (rB  d B )  (rA  rB )  (d A  d B )
3.6. Method Summary
We have outlined and examined current methods for              Multiplication
representing a robust, practical and viable
hierarchical rigid body solution. We now follow on              (rA  d A )(rB  d B )  rA rB  rA d B  rB d A  d A d B 2
from this by introducing and explaining how and                                                 rA rB  (rA d B  rB d A )
why dual-quaternions stand-out above these methods.            Division
4. WHY DUAL-QUATERNIONS?                                                    (rA  d A ) (rA  d A ) (rB  d B  )
We use dual-quaternions as a tool for expressing and                                     
                                                                            (rB  d B  ) (rB  d B  ) (rB  d B  )
analyzing the physical properties of rigid bodies.
Dual-quaternions can formulate a problem more                                                     rA rB  (rB d A  rA d B )
                                                                                              
concisely, solve it more rapidly and in fewer steps,                                                        (rB ) 2
present the result more plainly to others, be put into                                            rA rB rB d A  rA d B
practice with fewer lines of code and debugged                                                                        
                                                                                                   rB2        rB2
effortlessly.   Furthermore, there is no loss of
efficiency; dual-quaternions can be just as efficient if       Further reading on the subject of dual numbers is
not more efficient than using matrix methods. In all,          presented by Gino [BERG09].
there are several reasons for using dual-quaternions,
which we summarize:                                            5.2. Dual Number Differentiation
                                                               Dual numbers differentiate in the same way as any
   Singularity-free
                                                               other vector using elementary calculus principles,
   Un-ambiguous                                               e.g.:
   Shortest path interpolation
   Most efficient and compact form for                                          d                s ( x   x )  s( x )
                                                                                   s( x)  lim
    representing rigid transforms [SCHI11] - (3x4                               dx          x 0          x
    matrix 12 floats compared to a dual-quaternion 8           The derivative of a dual number is another dual
    floats)                                                    number. Remarkably, the dual operator’s condition
   Unified representation of translation and rotation
                                                                 2  0 enables us to take advantage of Taylor series
   Can be integrated into a current system with
                                                               to find the differentiable. Where we can see below,
    little coding effort
                                                               if we substituting a dual number into Taylor series,
   The individual translation and rotational
                                                               we get:
    information is combined to produce a single
    invariant coordinate frame [GVMC98]                                                  f '(rA )         f ''(rA )             f '''(rA )
                                                               f (rA  d A )  f (rA )          d A             (d A ) 2             (d A )3  ...
                                                                                            1!               2!                    3!
5. DUAL NUMBERS                                                               f (rA ) 
                                                                                         f '(rA )
                                                                                                  d A  0  0  ...            (as,  2  0)
Clifford [CLIF82] introduced dual numbers; similar                                          1!
to complex numbers that consists of two parts known                           f (rA )  f '(rA )d A
as the real and complex component. Dual numbers
break the problem into two components and are                  Remarkably, the Taylor series result gives us an
defined as:                                                    exceptionally tidy answer; from this we use dual
          z  r  d  with  2  0 but   0                   number arithmetic and substitution to find the
                                                               solution to any differential.
where  is the dual operator, r is the real part and d
                                                               The derivative also enables us to find the tangent of
the dual part. Similar to complex number theory,
                                                               an arbitrary point p on a given parametric curve that
where i is added to distinguish the real and complex
                                                               is equal to the normalized dual part of the point p.



WSCG 2012 Communication Proceedings                        5                                                           http://www.wscg.eu
6. QUATERNIONS                                                   6.2. Quaternion Interpolation
Quaternions were introduced by Hamilton in 1866                  An extremely important quality of quaternions that
[HAMI86] and have had a rollercoaster of a time with             make them indispensable in animation systems is
acceptance.   Quaternions are an extension of                    their ability to interpolate two or more quaternions
complex number-theory to formulate a four                        smoothly and continuously. Shoemake [SHOE85],
dimensional manifold. A quaternion is defined as:                presents an outstanding paper on using quaternion
                    q  w  ( xi  yj  zk )                     curves for animating rotations. Furthermore, it
                                                                 should be noted, the spherical linear interpolation
where w, x, y and z are the numerical values, while i,           (SLERP) properties of quaternions are inherited by
j and k are the imaginary components.                            dual-quaternions.
The imaginary components properties:                             7. DUAL-QUATERNIONS
                      i 2  j 2  k 2  1                       When quaternions are combined with dual number
                                                                 theory, we get dual-quaternions which was presented
and                                                              by Clifford in 1882 [CLIF82]. While the unit
                       ij  k ,   ji  k                        quaternion only has the ability to represent rotation,
                      jk  i,     kj  i                        the unit dual-quaternion can represent both
                                                                 translation and rotation.     Each dual-quaternion
                      ki  j ,    ik   j                       consists of eight elements or two quaternions. The
It is more common to represent the quaternion as two             two quaternion elements are called the real part and
components, the vector component (x, y and z) and                the dual part.
the scalar component (w).                                                              q  qr  qd 
                           q  (w, v)                            where q r and q d are quaternions. Combining the
For further reading on the workings of quaternions               algebra operations associated with quaternions with
and their advantages I highly recommend reading                  the additional dual number  , we can form the dual-
McDonalds [MCDO10] introductory paper for                        quaternion arithmetic.
students.
                                                                 7.1. Dual-Quaternion Arithmetic
6.1. Quaternion Arithmetic Operations                                 Operations
Since we are combining quaternions with dual                     The elementary arithmetic operations necessary for
number theory, we give the elementary quaternion                 us to use dual-quaternions are:
arithmetic operations below:
                                                                 Scalar Multiplication
Scalar Multiplication                                                               sq  sqr  sqd 
                         sq  (sw, sv)
                                                                 Addition
   where s is a scalar value.                                               q1  q2  qr1  qr 2  (qd1  qd 2 )
Addition                                                         Multiplication
                q1  q2  (w1  w2 , v1  v 2 )                           q1q2  qr1qr 2  (qr1qd 2  qd1qr 2 )
Multiplication                                                   Conjugate
      q1q2  ( w1w2  v1v2 , w1 v2  w2 v1  ( v1  v2 ) )                            q*  q*r  q*d 

Conjugate                                                        Magnitude
                                                                                          || q || qq*
                         q*  (w,  v)
                                                                 Unit Condition
Magnitude                                                                                   || q ||  1
                          || q || qq*
                                                                                     q qd  q*d qr  0
                                                                                      *
                                                                                      r

                                                                 The unit dual-quaternion is our key concern as it can
For a unit quaternion, || q || 1 . The unit quaternion          represent any rigid rotational and translational
is used to represent a rotation of an angle  , radians          transformations.
about a unit axis n , in three-dimensional space:                The rigid rotational and translational information for
                                                               the unit dual-quaternion is:
                  q  ( cos( ), n sin( ) )
                            2         2




WSCG 2012 Communication Proceedings                          6                                            http://www.wscg.eu
                        qr  r                                      4.   If necessary, for long chains, the dual-quaternion
                                                                         should be re-normalized (to mend drift and
                               1
                        qd      tr                                      maintain a unit dual-quaternion).
                               2                                    5.   To get the homogeneous transformation matrix,
where r is a unit quaternion representing the rotation                   convert the dual-quaternion by extracting the
and t is the quaternion describing the translation                       translational and rotational components.
represented by the vector t  (0, t ) .                             6.   The extracted rotation quaternion r and vector
                                                                         translation information is extracted using:
The dual-quaternion can represent a pure rotation the
same as a quaternion by setting the dual part to zero.                                     r  Qr

                                                                                       t  2 Qd Qr *
qr  [cos( ), n x sin( ), n y sin( ), n z sin( ) ][ 0,0,0,0]
          2           2           2           2                     Dual-quaternion multiplication is more efficient than
                                                                    matrix multiplication and can effortlessly be
To represent a pure translation with no rotation, the               converted back to a matrix when needed. Dual-
real part can be set to an identity and the dual part               quaternions, unlike Euler angles, do not present
represents the translation.                                         issues like "gimbal lock" and hence, are ideal for
                                                                    complex articulated hierarchies.
                                     tx t y tz
              qt  [1, 0, 0, 0][0,     , , ]                        9. COMPLEX CHARACTER
                                     2 2 2
Combining the rotational and translational                             HIERARCHY FORWARD
transforms into a single unit quaternion to represent                  KINEMATICS
a rotation followed by a translation we get:                        The focus of our attention is with rigid hierarchies
                                                                    having a large number of DOF. Humans have a
                        q  qt  q r                                tremendous amount of flexibility which we emulate
This arithmetic operation defines how we transform a                and analyze using numerical and mathematical
point p, using a unit dual-quaternion:                              models. Forward kinematics is the method of
                                                                    concatenating local positions and rotations together
                        p '  qpq*                                  to give their global ones. The forward kinematic
where      q and q* represent             a   dual-quaternion       method for concatenating transforms is the same for
                                                                    dual-quaternions and matrices; which use simple
transform and its conjugate;         while p and p '                multiplication to propagate transforms between the
represent our point inserted into a quaternion and its              connected links.
resulting transform.
                                                                    For example, the concatenation of transforms with
8. PORTING EXISTING CODE TO                                         matrices and dual-quaternions:
   DUAL-QUATERNIONS                                                 Matrix
A dual-quaternion consists of two quaternions, but is                                 M03  M0 M1M2M3
represented by a single variable Q. Systems that
                                                                    Dual-Quaternion
have been constructed using separate translation and
rotation (vector for translation and quaternion for                                     q03  q0q1q2q3
rotation) in combination with matrices schemes are                  where the subscript represents the transform, matrix
easily modified to use dual-quaternions for spatial                 transform M 0 corresponding to dual-quaternion
information.
                                                                    transform q 0 .
1.   For each link, construct a dual-quaternion Q
     from the rotation and translation information.                 10. EXPERIMENTAL RESULTS
2.   Real part of the quaternion is the rotation                    We used traditional matrix methods during initial
     quaternion r. The dual part is calculated by                   character transformation experiments; e.g., inverse
     multiplying the quaternion r and translation                   kinematic (IK) and animation blending to
     component t, e.g.:                                             demonstrate their numerous problems.         Matrix
                                                                    methods are a popular choice and solutions to these
                       Qr  r                                       problems have been developed; we used some of
                       Qd  0.5 (0, t ) r                           these engineering solutions.      Of course, these
                                                                    workarounds to these problems introduced an
3.   Combine transformations as you would matrices                  additional computational cost. Furthermore, certain
     using multiplication.                                          circumventions to overcome a problem often
                                                                    introduced errors in other areas.        One such
                                                                    engineering solution for reducing the impact of drift



WSCG 2012 Communication Proceedings                             7                                          http://www.wscg.eu
and concatenation error was to renormalize the
matrices at each level (and at each update frame).
The error reduced skewing and scaling but
manifested itself in the ideal global end-link
orientations and positions being inaccurate.
To demonstrate the problems, we constructed
numerous test cases to emphasis them. We also
demonstrate and explore how dual-quaternions can              Figure 4.        Ortho-normalizing matrices          in
represent rigid body character based systems.                 hierarchies in an attempt to reduce errors.

10.1. Rigid Body Transform Chains                             Ortho-normalizing the rotational part of the
We constructed a straightforward IK solver that               transform matrix between updates removed scaling
would follow a target end-effector. To mimic how a            and skewing problems.          The joints presented
character would move his arm or leg. The end-                 discontinuity errors in the frames hierarchy (see
effector had six DOF, which the IK solver had to              Figure 4). The ideal end-effectors position and
work with to meet its target goal.                            rotation were also different from the calculated one
                                                              using the refurbish matrices.
                                                              10.2. Biped Model
                                                              For our test character, we used a 16 link biped model,
                                                              shown in Figure 5. The character has 36 degrees of
                                                              freedom (DOF).          Character rigs can produce
                                                              extremely non-linear motions due to their joint limits,
                                                              flexibility and elaborate arrangement of joints.


Figure 2. Rigid body links attached in a single
hieararchy frame. (Draw ideal(red) and calculated
end-effector (green).
The hierarchy is composed of rigid links. Each link
held a rotation and translation in the form of a matrix
or dual-quaternion. For calculations, the axis-angle
and translation could be extracted and used when
needed. Local transforms were combined from the
root to the end-effectors. Concatenation of the
transforms throughout the levels was achieved by
                                                                Figure 5. 16 link biped model used for testing.
multiplying parent transforms with current
transforms.                                                   Figure 5, shows the biped model in its starting stance
Certain orientation and translation configurations            pose.
produced errors in the output, shown in Figure 3.             Buildup of computational inaccuracies will cause a
These errors presented themselves as skewing and              dual-quaternion to become of non-unit length; we fix
scaling manifestations.                                       these errors by renormalization.        In contrast,
                                                              repairing a non-orthogonal matrix is much more
                                                              complicated (see [SALA79]).
                                                              11. RESULTS
                                                              The dual-quaternion unifies the translation and
                                                              rotation into a single state variable. This single state
                                                              variable     offers     a     robust,    unambiguous,
                                                              computationally efficient way of representing rigid
Figure 3. Artifact error when matrices representing           transform.
translation and orientation in linked hierarchies.
                                                              The computational cost of combining matrices and
Early workarounds to amend the problem were to                dual-quaternions:
repair the matrix at each level in the hierarchy by                    Matrix4x4      : 64mult + 48adds
ortho-normalizing the rotational component. While                      Matrix4x3      : 48mult + 32adds
ortho-normalizing the matrix reduced scaling and                       DualQuaternion : 42mult + 38adds
skewing artifacts, alternative errors manifest
themselves in alternative forms.



WSCG 2012 Communication Proceedings                       8                                        http://www.wscg.eu
In our tests, we found the dual-quaternion                   public DualQuaternion_c( Quaternion r, Vector3 t )
                                                             {
multiplication method of transforms on average ten             m_real    = Quaternion.Normalize( r );
percent faster compared matrix multiplication. We            }
                                                               m_dual = ( new Quaternion( t, 0 ) * m_real ) * 0.5f;

did not take advantage of CPU architecture using             public    static       float    Dot(    DualQuaternion_c    a,
parallel methods such as SIMD which can further              DualQuaternion_c b )
                                                             {
improve speeds as demonstrated by Pallavi                      return Quaternion.Dot( a.m_real, b.m_real );
[MEHU10] (both for matrices and quaternion                   }
                                                             public static DualQuaternion_c operator* (DualQuaternion_c
multiplication).                                             q, float scale)
                                                             {
One major advantage we found when working with                 DualQuaternion_c ret = q;
                                                               ret.m_real *= scale;
dual-quaternions was the added advantage of                    ret.m_dual *= scale;
calculating angular and linear differences. When               return ret;
                                                             }
working with pure matrix methods we needed to                public static DualQuaternion_c Normalize( DualQuaternion_c q
convert the matrix to a quaternion to calculate              )
                                                             {
angular variations.                                            float mag = Quaternion.Dot( q.m_real, q.m_real );
                                                               Debug_c.Assert( mag > 0.000001f );
12. CONCLUSION AND FURTHER                                     DualQuaternion_c ret = q;
                                                               ret.m_real *= 1.0f / mag;
    WORK                                                       ret.m_dual *= 1.0f / mag;
The dual-quaternion model is an accurate,                    }
                                                               return ret;

computationally efficient, robust, and flexible              public static DualQuaternion_c operator + (DualQuaternion_c
method of representing rigid transforms and should           lhs, DualQuaternion_c rhs)
                                                             {
not be overlooked. Implementing pre-programmed                 return new DualQuaternion_c(lhs.m_real + rhs.m_real,
                                                                 lhs.m_dual + rhs.m_dual);
dual-quaternion modules (e.g., multiplication and            }
normalization) enables the creation of more elegant          // Multiplication order - left to right
                                                             public static DualQuaternion_c operator * (DualQuaternion_c
and clearer computer programs that are easier to             lhs, DualQuaternion_c rhs)
work with and control.                                       {
                                                               return new DualQuaternion_c(rhs.m_real*lhs.m_real,
While matrices are the de-facto method used for the               rhs.m_dual*lhs.m_real + rhs.m_real*lhs.m_dual);
                                                             }
majority of hierarchy based simulations, we have             public static DualQuaternion_c Conjugate( DualQuaternion_c q
shown that they can present certain problems which           )
                                                             {
are costly to avoid (e.g., renormalizing a matrix).            return     new     DualQuaternion_c(   Quaternion.Conjugate(
The problem and cost of drifting and normalizing is          q.m_real ), Quaternion.Conjugate( q.m_dual ) );
                                                             }
less with dual-quaternions compared to matrix                public static Quaternion GetRotation( DualQuaternion_c q )
methods. When dealing with rigid transforms the              {
                                                               return q.m_real;
dual-quaternion method shines through due to its             }
numerous advantages.                                         public static Vector3 GetTranslation( DualQuaternion_c q )
                                                             {
This paper has only provided a taste of the potential          Quaternion t = ( q.m_dual * 2.0f ) * Quaternion.Conjugate(
                                                             q.m_real );
advantages of dual-quaternions, and one can only               return new Vector3( t.X, t.Y, t.Z );
imagine the further future possibilities that they can       }
                                                             public        static       Matrix      DualQuaternionToMatrix(
offer. For example, there is a deeper investigation of       DualQuaternion_c q )
the mathematical properties of dual-quaternions (e.g.,       {
                                                               q = DualQuaternion_c.Normalize( q );
zero divisions). There is also the concept of dual-
dual-quaternions (i.e., dual numbers within dual               Matrix M = Matrix.Identity;
                                                               float w = q.m_real.W;
numbers) and calculus for multi-parametric objects             float x = q.m_real.X;
for the reader to pursue if he desires.                        float y = q.m_real.Y;
                                                               float z = q.m_real.Z;

13. APPENDIX                                                   // Extract rotational information
                                                               M.M11 = w*w + x*x - y*y - z*z;
13.1. Dual-Quaternion Implementation                           M.M12 = 2*x*y + 2*w*z;
                                                               M.M13 = 2*x*z - 2*w*y;
    Class.                                                     M.M21 = 2*x*y - 2*w*z;
public class DualQuaternion_c
                                                               M.M22 = w*w + y*y - x*x - z*z;
{
                                                               M.M23 = 2*y*z + 2*w*x;
public Quaternion   m_real;
public Quaternion   m_dual;
                                                               M.M31 = 2*x*z + 2*w*y;
public DualQuaternion_c()
                                                               M.M32 = 2*y*z - 2*w*x;
{
                                                               M.M33 = w*w + z*z - x*x - y*y;
  m_real = new Quaternion(0,0,0,1);
  m_dual = new Quaternion(0,0,0,0);
                                                               // Extract translation information
}
                                                               Quaternion t = (q.m_dual * 2.0f) * Quaternion.Conjugate(
public DualQuaternion_c( Quaternion r, Quaternion d )
                                                             q.m_real);
{
                                                               M.M41 = t.X;
  m_real = Quaternion.Normalize( r );
                                                               M.M42 = t.Y;
  m_dual = d;
                                                               M.M43 = t.Z;
}
                                                               return M;




WSCG 2012 Communication Proceedings                      9                                            http://www.wscg.eu
}                                                                      of robot manipulators using dual quaternion
#if false                                                              feedback,” in Intelligent Robots and Systems
public static void SimpleTest()
{
                                                                       (IROS), 2010 IEEE/RSJ International Conference
  DualQuaternion_c     dq0    =     new    DualQuaternion_c(           on, 2010, pp. 658–663.
Quaternion.CreateFromYawPitchRoll(1,2,3),                new
Vector3(10,30,90) );                                                [SCHI11] M. Schilling, “Universally manipulable
  DualQuaternion_c     dq1    =     new
Quaternion.CreateFromYawPitchRoll(-1,3,2),
                                           DualQuaternion_c(
                                                         new
                                                                       body models — dual quaternion representations
Vector3(30,40, 190 ) );                                                in layered and dynamic MMCs,” Autonomous
  DualQuaternion_c     dq2    =     new    DualQuaternion_c(
Quaternion.CreateFromYawPitchRoll(2,3,1.5f),             new
                                                                       Robots, 2011.
Vector3(5,20, 66 ) );
  DualQuaternion_c dq = dq0 * dq1 * dq2;
                                                                    [GVMC98] Q. Ge, A. Varshney, J. P. Menon, and C.
                                                                       F. Chang, “Double quaternions for motion
  Matrix                     dqToMatrix                    =
                                                                       interpolation,” in Proceedings of the ASME
DualQuaternion_c.DualQuaternionToMatrix( dq );                         Design Engineering Technical Conference, 1998.
  Matrix   m0    =  Matrix.CreateFromYawPitchRoll(1,2,3)   *        [LIWC10] Y. Lin, H. Wang, and Y. Chiang,
Matrix.CreateTranslation( 10, 30, 90 );
  Matrix   m1   =  Matrix.CreateFromYawPitchRoll(-1,3,2)   *           “Estimation of relative orientation using dual
Matrix.CreateTranslation( 30, 40, 190 );                               quaternion,” System Science and, no. 2, pp. 413-
  Matrix m2 = Matrix.CreateFromYawPitchRoll(2,3,1.5f)      *
Matrix.CreateTranslation( 5, 20, 66 );                                 416, 2010.
  Matrix m = m0 * m1 * m2;
}                                                                   [PEMC04] A. Perez and J. M. McCarthy, “Dual
#endif                                                                 quaternion synthesis of constrained robotic
} // End DualQuaternion_c
                                                                       systems,” Journal of Mechanical Design, vol.
                                                                       126, p. 425, 2004.
13.2. Novice Errors                                                 [ALMA92] W. Alan and W. Mart, Advanced
There are a few things to look out for when
                                                                       Animation and Rendering Techniques: Theory
implementing a dual-quaternion class.          Firstly,
                                                                       and Practice. Adison-Wesley, 1992.
ensure the multiplication order is correct and
remains consistent with matrices (i.e., left to right).             [GILB86] S. Gilbert, Introduction to Applied
Secondly, always ensure that the dual-quaternions                      Mathematics. Wellesley-Cambridge Press, 1986.
remain normalized (i.e., unit-length).                              [BERG09] G. van den Bergen, “Dual Numbers:
                                                                       Simple Math, Easy C++ Coding, and Lots of
14. REFERENCES                                                         Tricks,” GDC Europe, 2009. [Online]. Available:
[CLIF82] W. Clifford, Mathematical                 Papers.             www.gdcvault.com/play/10103/Dual-Numbers-
   London: Macmillan, 1882.                                            Simple-Math-Easy.
[KCŽO08] L. Kavan, S. Collins, J. Žára, and C.                      [HAMI86] W. R. Hamilton,              Elements     of
   O’Sullivan,    “Geometric    skinning     with                      Quaternions. London: , 1886.
   approximate dual quaternion blending,” ACM
   Transactions on Graphics (TOG), vol. 27, no. 4,                  [MCDO10] J. McDonald, “Teaching Quaternions is
   p. 105, 2008.                                                      not Complex,” Computer Graphics Forum, vol.
                                                                      29, no. 8, pp. 2447-2455, Dec. 2010.
[IVIV11] F. Z. Ivo and H. Ivo, “Spherical skinning
   with dual quaternions and QTangents,” ACM                        [SHOE85] K. Shoemake, “Animating rotation with
   SIGGRAPH 2011 Talks, vol. 27, p. 4503, 2011.                        quaternion curves,” ACM SIGGRAPH computer
                                                                       graphics, 1985.
[SELI11] J. Selig, “Rational interpolation of rigid-
   body motions,” Advances in the Theory of                         [SALA79] E. Salamin, “Application of quaternions
   Control, Signals and Systems with Physical                          to computation with rotations,” Internal Report,
   Modeling, pp. 213–224, 2011.                                        Stanford University, Stanford, CA, vol. 1, 1979.
[VAFU09] A. Vasilakis and I. Fudos, “Skeleton-                      [MEHU10] P. Mehrotra and R. Hubbard, “Benefits
   based rigid skinning for character animation,” in                  of Intel® Advanced Vector Extensions For
   Proc. of the Fourth International Conference on                    Quaternion Spherical Linear Interpolation
   Computer Graphics Theory and Applications,                         (Slerp),”      2010.      [Online].      Available:
   2009, no. February, pp. 302–308.                                   http://software.intel.com/en-us/articles/benefits-
                                                                      of-intel-advanced-vector-extensions-for-
[KMLX11] Y. Kuang, A. Mao, G. Li, and Y. Xiong,
                                                                      quaternion-spherical-liner-interpolation-slerp/.
  “A strategy of real-time animation of clothed
  body movement,” in Multimedia Technology
  (ICMT), 2011 International Conference on, 2011,
  pp. 4793–4797.
[PPAF10] H. L. Pham, V. Perdereau, B. V. Adorno,
   and P. Fraisse, “Position and orientation control



WSCG 2012 Communication Proceedings                            10                                      http://www.wscg.eu
An Efficient Preconditioner and a Modified RANSAC for Fast
                and Robust Feature Matching
                                 Anders Hast                               Andrea Marchetti
                                Uppsala University,                              IIT, CNR
                                Uppsala, Sweden                                 Pisa, Italy
                               anders.hast@it.uu.se                      andrea.marchetti@iit.cnr.it


                                                      ABSTRACT
Standard RANSAC does not perform very well for contaminated sets, when there is a majority of outliers. We present a method
that overcomes this problem by transforming the problem into a 2D position vector space, where an ordinary cluster algorithm
can be used to find a set of putative inliers. This set can then easily be handled by a modified version of RANSAC that draws
samples from this set only and scores using the entire set. This approach works well for moderate differences in scale and
rotation. For contaminated sets the increase in performance is in several orders of magnitude. We present results from testing
the algorithm using the Direct Linear Transformation on aerial images and photographs used for panographs.

Keywords
RANSAC, Preconditioner, Homography, Clustering, Feature Matching, Image Stitching.

1   INTRODUCTION                                                   set of putative inliers is often contaminated with out-
RANSAC was introduced by Fischler and Bolles more                  liers. Another consequence for highly contaminated
than 30 years ago [FB81] and is one of the far most                sets is that the stopping criterion might indicate that
used algorithms for finding corresponding pairs of fea-            there is not yet a consensus, while most of the inliers
ture points in images. Distinguishing these so called              are already found.
true matches or inliers from the outliers or non match-
ing pairs is essential for many applications of com-               Contributions and Delimitations
puter vision, such as image stitching [Sze10], 3D re-              We propose a naive preconditioner that eliminates the
construction [Pol00] and point-cloud shape detection               majority of outliers before running a modified version
[SWK07], just to mention a few. Many variants have                 of LO-RANSAC [CMK03] on the set. The precondi-
been proposed since then, trying to enhance perfor-                tioner transforms the problem of finding the consen-
mance of the algorithm in different ways, as will be               sus set to a position vector space, where an ordinary
shown in the end of this section.                                  clustering algorithm can be used to find the cluster
   One disadvantage with standard RANSAC is that it                that contains the putative inliers. It will be show in
handles contaminated sets poorly. In fact, many imple-             examples that the approach works well if the differ-
mentations of RANSAC do not perform well when the                  ences in rotation and scale are moderate, which they
number of inliers is less than 50% [Low04]. RANSAC                 usually are for matching of images with mainly side-
is based on random sampling, as the name itself sug-               way camera translations. The modified RANSAC will
gests: RANdom Sample Consensus and the proba-                      draw samples from this set only and whenever a larger
bility of finding an initial sample containing inliers             set is found the local optimization step samples this
only, decreases when the amount of outliers increases.             set at least 4 times while using the homography to
Furthermore, RANSAC usually terminates when the                    score the whole set. This approach will be many times
probability of finding more inliers is low or rather               faster for contaminated sets than ordinary RANSAC,
when an outlier free set has been picked with some                 as the transformation is simple and clustering is rel-
predefined probability. Nonetheless, for heavily con-              atively fast. Moreover, the modified RANSAC will
taminated sets, the output is not useful as it usually             find consensus in very few iterations as it works on
contains too few inliers if any. Moreover, the output              a set with a large majority of inliers. The precondi-
                                                                   tioner will therefore reduce the number of iterations
Permission to make digital or hard copies of all or part of
                                                                   in the modified RANSAC by orders of magnitude for
this work for personal or classroom use is granted without         contaminated sets. Since clustering can be done with
fee provided that copies are not made or distributed for           O(n) complexity it could also be used for sets with low
profit or commercial advantage and that copies bear this           contamination as it will be fast. However, In this paper
notice and the full citation on the first page. To copy            an O(n2 ) algorithm was used.
otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission             The proposed approach will be compared to stan-
and/or a fee.                                                      dard RANSAC only, as many of the already proposed
                                                                   extensions of RANSAC could be used to enhance



WSCG 2012 Communication Proceedings                           11                                         http://www.wscg.eu
the modified RANSAC. Especially, MultiRANSAC                    be planned in order to avoid scoring useless pairs.
[ZKM05] could be used when there are multiple                   KALMANSAC [VJFS05] was designed for estima-
planes in the images. Nonetheless, the proposed                 tion of structure from motion (SFM). It is derived
approach is able to handle such cases too and it will           from pseudo-Bayesian filtering algorithms in a sam-
be discussed how.                                               pling framework and can handle sequences contain-
   Furthermore, we have chosen to delimit ourselves in          ing large number of outliers. Other examples from
this paper to use a perspective transformation based on         robotics are Preemptive RANSAC [Nis03] and Itera-
at least four points, the so called Direct Linear Trans-        tive RANSAC [KK06].
formation (DLT)[HZ03], which throughout the text we                Other important contributions to RANSAC use dif-
will be referred to as the homography. This transfor-           ferent strategies. MultiRANSAC [ZKM05] is a par-
mation can be used for a number of applications such            allel version that allows to deal with multiple models
as image stitching of aerial images and panographs.             and have the advantage of being able to cope with a
                                                                high percentage of outliers. GASAC [RH06] is an-
RANSAC and some of its Variants                                 other parallel approach using a genetic algorithm ap-
Standard RANSAC proceeds in the following way:                  proach. Moreover, RANSAC has a low probability
first a minimal number of points is selected, which is          to find the correct solution when the data is quasi de-
required to determine the homography [BL07] [HZ03]              generate and QDEGSAC [FP05] was proposed for use
[VL01], which is the projective transformation be-              in such cases. NAPSAC[MTN∗ 02] was proposed for
tween the images. Then the set is scored so that the            problems with high noise and takes advantage of the
inliers that falls below a certain predefined tolerance         fact that if an inlier is found then any point close to
ε are counted. After transforming using the homogra-            that point will have a high probability to be an inlier.
phy, these points are close enough to its corresponding            There are many more versions proposed in literature
match and are therefore regarded as true inliers. The           and a performance evaluation of some of the more im-
algorithm terminates when the probability of finding a          portant variants of RANSAC is done by Choi et al.
better model falls under a predefined threshold, other-         [CKY09] and a comparative analysis of RANSAC is
wise it starts all over.                                        given by Raguram et al. [RFP08]. Lowe [Low04] pro-
   RANSAC generally treats all correspondences                  posed to use the Hough transform [DH72] for cluster-
equally and draws random samples uniformly from                 ing data instead of RANSAC, and there are even hy-
the full set. However there are some approaches that            brids [HH07]. Nevertheless, RANSAC is after more
tries to exclude probable outliers early on or alterna-         than 30 years still used and improved for computer vi-
tively determining which ones are probable inliers.             sion applications.
Just to mention a few: MLESAC [TZ00] performs                   2   THE PRECONDITIONER
non-uniform, i.e. guided sampling of correspondences            The idea is to use a preconditioner that transforms the
and PROSAC [CM05] draw samples from a pro-                      problem of finding the consensus set to finding a clus-
gressively larger set of top-ranked correspondences.            ter in a position vector space. Generally, a vector can
GODSAC [MvHK∗ 06] use an assessment driven                      be constructed from two points and each matching pair
selection of good samples, instead of random sam-               consists of exactly two points. Hence, it can be re-
pling. Fuzzy RANSAC [LK07] divides the input data               garded as a vector from image a to image b, just like
into categories depending on the residual error and             how the final homography transforms each point in im-
sampling is done in the good set. Another approach              age a to its corresponding point in image b, within a
[ZK06] transforms the whole problem into classi-                certain threshold ε. The main advantage is that a po-
fication of the residual distribution. SCRAMSAC                 sition vector can be treated as a 2D point rather than
[SLK09] tries to reduce the number of outliers using a          a 2D vector. The position vector will be scaled in the
spatial consistency check. R-RANSAC [CM08], was                 range [0..1] so that the cluster algorithm can be given
proposed for the situation when the contamination of            a tolerance εc similar to the tolerance ε used for the
outliers is known, using a randomized model verifi-             modified RANSAC. This is done by dividing the vec-
cation strategy. Cov-RANSAC [RFP09] incorporates                tor by the length of the sum of the sides in each di-
the inherent uncertainty of the estimation proce-               rection, where image b is translated in each direction
dure in order to achieve a more efficient algorithm.            using the lengths of image a, so that there is no spatial
GroupSAC[NJD09] take advantage of additional                    overlap between the images.
grouping information between features provided by                  Let the position vector between the feature point at
optical flow based clustering.                                  (x0 , y0 ) in image a and the corresponding point (x1 , y1 )
   Other approaches, designed for real-time tracking            in image b be:
take into account that there are similarities between
                                                                                                                 
a series of images captured by a camera. Hence,                                   (ax + x1 ) − x0 (ay + y1 ) − y0
the order of the scoring of the pairs of matches can                        v=                   ,                  ,   (1)
                                                                                      ax + bx         ay + by


WSCG 2012 Communication Proceedings                        12                                          http://www.wscg.eu
where ax , ay and bx , by are the sizes in the x and y di-        ter center. First the vectors are computed and then the
rection for image a and b, respectively.                          clustering algorithm needs to find the cluster. The cost
   The nice result of such an approach is that true               of computing the distance between all points in the
matches will yield points in the 2D space that are                space for a brute force algorithm is n(n − 1)/2, hence
forming a cluster, while outliers will be spread out in a         the complexity is O(n2 ). Then all points sufficiently
more random fashion. The search for true matches can              near the point with most neighbors need to be found.
therefore be done using any appropriate 2D cluster-               Nevertheless, this cost could be reduced by dividing
ing algorithm, since the vectors are regarded as points           the space using for instance quad trees [FB74] or kd-
rather than vectors, i.e. they are position vectors. This         trees [TBK08]. Moreover, binning would reduce the
is true also for cases when the images are taken from             complexity to O(n) as each point is classified to be-
a sequence of a forward camera motion. Some pre-                  long to a bin depending on its spatial location, in a lin-
fer to visualize the matching by showing only one of              ear search. The bin with most points will be chosen as
the images using lines that start in the feature points in        the cluster. Nonetheless, the borders of the bins may
that image. The line ends in the point corresponding              divide the cluster and this can easily be handled by
to the feature points of the second image, which is not           overlapping bins. Once again the bin with most points
shown but is supposed to overlap the first image. If the          are the putative inliers. The size of the bins would be
images are taken from a sequence of a forward camera              proportional to the tolerance εc .
motion this approach will yield lines that can point in
independent directions. However, if the proposed ap-              A Modified RANSAC
proach is used, where the images are put so that they             A modified version of the LO-RANSAC [CMK03]
do not overlap and they do not share an edge, then they           [CMO04] algorithm is here proposed, which utilizes a
will all have a similar direction compared to the out-            local optimization step. Both the cluster and the whole
liers. The direction and length of these lines or vectors,        set are input parameters to the algorithm, which sam-
will not be exactly the same and can vary. Nonethe-               ples from the cluster only. As the cluster contains the
less, they will usually vary a lot less than compared to          set, which is close to the final solution and therefore
the outliers even if the cluster will be less dense.              pretty free from outliers, it was chosen to sample us-
                                                                  ing up to half of the matches in the cluster but obvi-
Clustering                                                        ously never less than four. This usually lead to consen-
There are many clustering algorithms [CRW91]                      sus faster than sampling just four samples every time,
[JMF99] that could be used and some of the more                   which give less support compared to using up to half
popular are k-means clustering [Mac67] and the mean               of them. This set is used to estimate the homography
shift algorithm [CM02]. In our tests it was chosen to             and scoring the number of inliers.
use a simple approach that for each point (position                  Every time scoring gives a maximum set of inliers
vector) in the set computes the distance to all other             the local optimization step samples iteratively from
points. The point that have most neighbors closer                 this set and estimates the homography from it. How-
than the threshold εc will be chosen as the cluster               ever, scoring is done using the whole set. Once again
center and all points in the cluster are considered               it is more efficient to use up to half the size of the set,
putative inliers. Obviously a better algorithm could              when doing re-estimation and re-scoring. Whenever a
be used, especially for situations where the points               larger set is found it uses this set to sample from and
lie in different planes giving different homographies.            restarts the local optimization loop. We have found
However, focus in this paper does not lie on the                  that about 4 iterations is usually enough for the pro-
clustering algorithm as it is a well studied area.                posed approach, while Chum et al used 10 iterations.
Hence, we will instead focus on the preconditioner                This is of course a value that can be increased if nec-
that transforms the matches to the new position vector            essary. The algorithm terminates when the probability
space and on how to treat the clustered points using a            is 99% that we have picked an outlier free set and the
modified version of LO-RANSAC.                                    parameters for this test are constructed using the set
   The cluster will contain a majority of the inliers             belonging to the cluster. Generally, N iterations are
and also some outliers depending both on the toler-               need in order to find an outlier free set with the proba-
ance εc and how well the cluster algorithm performs.              bility p as:
Nonetheless, it is not of vital importance that the clus-
ter will contain inliers only, as the modified RANSAC                                       log (1 − p)
will clean it up. In all our tests we used the same value                            N=                   ,              (2)
                                                                                           log (1 − γ s )
of εc for the clustering as the tolerance ε for the mod-
ified RANSAC.                                                     where γ is the inlier ratio, i.e. number of inliers di-
   The computational cost for the preconditioner is               vided by number of points in the cluster and s is the
rather low. We used a simple approach to find the clus-           number of samples drawn each time.



WSCG 2012 Communication Proceedings                          13                                           http://www.wscg.eu
    0.9                                                        0.9

    0.8                                                        0.8

    0.7                                                        0.7

    0.6                                                        0.6

    0.5                                                        0.5

    0.4                                                        0.4

    0.3                                                        0.3

    0.2                                                        0.2

    0.1                                                        0.1

     0                                                          0
     0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   1    0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   1




Figure 1: The result of using the preconditioner of
a quasi degenerate set, where all points in blue are
outliers. Left: the cluster found by the clustering
algorithm (red). Right: the points in the space that
corresponds to the true inliers found by the modi-                                                                             Figure 2:         c MiBAC-ICCD, Aerofototeca
fied RANSAC (green).                                                                                                           Nazionale, fondo RAF. Two historical photos
                                                                                                                               taken over Pisa during WWII, with the true inliers
  If N is larger than the number of iterations of the                                                                          connected with yellow lines (16.5%) and the false
main loop the algorithm starts all over and samples the                                                                        matches with red lines (83.5%)
cluster set from the preconditioner once again.
                                                                                                                                      Iterations                     Inliers
3         RESULTS                                                                                                                             N        µ       σ          µ       σ
Several tests using different images were conducted in
                                                                                                                                  1      6196 6.144 0.452 36.000 0.000
order to prove the efficiency of the proposed precondi-
                                                                                                                                  2   1123000 17.904 6.967 81.005 0.261
tioner and the modified RANSAC. The Harris corner
                                                                                                                                  3        87 6.000 6.000 211.000 0.000
detector [HS88] was chosen to detect features in most
                                                                                                                                  4      8271 11.103 5.747 46.999 0.150
of the tests instead of the more accurate SIFT detector
                                                                                                                                  5        26 7.000 0.000 225.000 0.000
[Low04]. As the proposed method will be of inter-
                                                                                                                                  6       731 8.817 2.878 76.723 0.828
est especially for sets with rather high contamination,
                                                                                                                                  7         9 6.000 0.000 137.000 0.000
at least 50%, Harris is preferable as it is less accurate
                                                                                                                                  8        90 7.934 3.573 11.954 2.646
than SIFT.
                                                                                                                                  9        18 6.005 0.071 90.000 0.000
A Quasi Degenerate Set                                                                                                         Table 1: The number of iterations (theoretical) and
A quasi degenerate set with an inlier ratio of 0.1651                                                                          the mean and standard deviation for number of it-
was chosen and according to equation 2 it would need                                                                           erations and inliers for different matchings and im-
6196 iterations to find an outlier free set. The perspec-                                                                      ages.
tive distortion is small in these aerial images. How-
ever, they are rotated in a way that it becomes very
hard for standard RANSAC to find the consensus set.                                                                            A Heavily Contaminated Set
A test was performed 10 000 times measuring how                                                                                A heavily contaminated set with just an inlier ratio of
many iterations were needed to find all inliers and the                                                                        0.045 was obtained by increasing the number of fea-
result was on average 32 887 iterations. Moreover, a                                                                           ture points and the ratio for the matching. Figure 3
test was done 10 000 times counting both number of it-                                                                         shows how the preconditioner finds the cluster (red) in
erations and number of inliers using the preconditioner                                                                        the image at the left. In the right is a close-up of the
and the modified RANSAC. The result is shown in Ta-                                                                            inliers (green). Standard RANSAC would, according
ble 1 on the first row. The preconditioner finds the set                                                                       to equation 2, need about 1 123 000 iterations to find
(red) in Fig. 1 at the left. In the right is the same points                                                                   the majority of inliers. After the preconditioner, the
with the inliers (green) that are found by the modified                                                                        probability is increased to 0.9759, which corresponds
RANSAC.                                                                                                                        to 1.93 iterations on average. The modified RANSAC
   The images used for the result in Fig. 1 are shown in                                                                       could easily find almost all inliers in every run in just
Fig. 2. The true inliers are connected by yellow lines                                                                         about 17 iterations as shown on the second row in Ta-
and the outliers with red ones. The set is quasi de-                                                                           ble 1 which is an enormous increase in performance
generate because the true matches covers a small area,                                                                         compared to the theoretical 1 123 000 iterations.
which is rather elongated. Furthermore, the images are
not perfectly aligned with each other as there is about                                                                        An Almost All Inlier Set
18 degrees of rotation between them. This causes stan-                                                                         A set that is almost outlier free with an inlier ratio is
dard RANSAC to find just a portion of the inliers in                                                                           98.40% was tested and Figure 4 shows how the pre-
most runs. This problem is overcome by the precon-                                                                             conditioner finds the whole set (red). The theoreti-
ditioner as it finds the major part of the inliers and the                                                                     cal number of iterations are just 1.66 and the modi-
modified RANSAC draws sample from this set.                                                                                    fied RANSAC needs 6 iterations to find the set, which



WSCG 2012 Communication Proceedings                                                                                       14                                        http://www.wscg.eu
    1

   0.9
                                                                       0.35
   0.8

   0.7
                                                                        0.3
   0.6

   0.5
                                                                       0.25
   0.4

   0.3
                                                                        0.2
   0.2

   0.1
                                                                       0.15
    0
         0   0.2   0.4   0.6       0.8                1                              0.1           0.15   0.2   0.25   0.3




Figure 3: The result of using the preconditioner of
a highly contaminated set, where all points in blue                                                                               Figure 5: c Anders Hast. Two images of the "Ponte
are outliers. Left: the cluster found by the clus-                                                                                Vecchio" in Florence, Italy. The inliers are con-
tering algorithm (red). Right: a close up of the                                                                                  nected with yellow lines (47.63%) and the false
points in the space that corresponds to the true in-                                                                              matches are depicted with red crosses (52.37%)
liers found by the modified RANSAC (green).                                                                                                                                                                       0.49

                                                                                                                                                                                                                 0.488
                                                                                                                                     0.8
                                                                                                                                                                                                                 0.486
                         0.75
                                                                                                                                     0.7                                                                         0.484

                                                                                                                                                                                                                 0.482
                          0.7                                                                                                        0.6
                                                                                                                                                                                                                  0.48

                                                                                                                                                                                                                 0.478
                                                                                                                                     0.5
                         0.65                                                                                                                                                                                    0.476
                                                                                                                                     0.4                                                                         0.474

                          0.6                                                                                                                                                                                    0.472
                                                                                                                                     0.3
                                                                                                                                                                                                                  0.47

                                                                                                                                     0.2
                         0.55                                                                                                              0   0.1   0.2    0.3     0.4         0.5     0.6    0.7   0.8   0.9      0.27          0.275         0.28       0.285        0.29          0.295



                          0.5
                                0.35     0.4   0.45       0.5   0.55   0.6    0.65    0.7   0.75    0.8
                                                                                                                                  Figure 6: A moderately contaminated set with an
Figure 4: A set with an inlier ratio of 98.40% The                                                                                inlier ratio of 47.63% Left: The preconditioner
preconditioner finds the cluster (red), which is the                                                                              finds the cluster (red). Right: A close up of the in-
same set as the modified RANSAC will find.                                                                                        liers found by the modified RANSAC (green).
                                                                                                                                     0.9                                                                          0.53

                                                                                                                                                                                                                  0.52

could be reduced by diminishing the number of itera-                                                                                 0.8

                                                                                                                                                                                                                  0.51
                                                                                                                                     0.7

tions in the local optimization.                                                                                                     0.6
                                                                                                                                                                                                                   0.5

                                                                                                                                                                                                                  0.49

                                                                                                                                     0.5                                                                          0.48



Multiple Planes                                                                                                                      0.4
                                                                                                                                                                                                                  0.47

                                                                                                                                                                                                                  0.46
                                                                                                                                     0.3

A set of photos taken on ground were used to test the                                                                                0.2
                                                                                                                                                                                                                  0.45

                                                                                                                                       0.1     0.2    0.3     0.4         0.5         0.6     0.7    0.8   0.9             0.18    0.19   0.2     0.21   0.22   0.23   0.24    0.25   0.26    0.27

algorithm for stitching of panographs. The precondi-
tioner is also able to find the correct cluster for images                                                                        Figure 7: A contaminated set with an inlier ratio of
where the perspective distortion is greater and as in                                                                             15.36% Left: The preconditioner finds the cluster
this case, where there are several planes. The precon-                                                                            (red). Right: A close up of the inliers found by the
ditioner and modified RANSAC was used for the set                                                                                 modified RANSAC (green).
of images shown in Figure 5 and Figure 6 shows how
the preconditioner finds the cluster (red) in the image                                                                           inliers (green). The modified RANSAC finds almost
to the left and on the right is a close-up of the inliers                                                                         all inliers in every run in just 11 iterations as shown on
(green). The cluster becomes elongated and curved                                                                                 the fourth row in Table 1 compared to the theoretical
because of perspective distortions and the three planes                                                                           value of 8271 iterations.
in the image. Nevertheless, the clustering algorithm is                                                                              Yet another set of images where used and Figure 8
able to find the cluster containing all three planes. A                                                                           shows how the preconditioner finds the cluster (red) at
more sophisticated clustering algorithm might be able                                                                             the left. In the right is a close-up of the inliers (green).
to separate it into three clusters. However, this task                                                                            The modified RANSAC finds almost all inliers in ev-
could also be handled by a version of RANSAC that                                                                                 ery run in just 7 iterations as shown on the fifth row
finds multiple planes.                                                                                                            in Table 1 compared to the theoretical value of 26 it-
   The modified RANSAC finds all inliers in every run                                                                             erations. Standard RANSAC found all inliers in an
in just 6 iterations as shown on the third row in Table 1                                                                         average of 157.6 iterations with a standard deviation
compared to the theoretical value of 87 iterations for                                                                            of 156.4. Hence, the proposed method have the ad-
finding 99% of the inliers. This is not a huge increase                                                                           vantage of being less variable when it comes to the
in performance. However, keep in mind that the modi-                                                                              number of iterations, also for medium contaminated
fied RANSAC finds all inliers in every run for this set,                                                                          sets.
which is not the case for standard RANSAC. Increas-                                                                                  Finally we made a test using SIFT on a pair of im-
ing the rate to 99.99% would double the theoretical                                                                               ages where there are two separate planes as shown in
number of iterations needed.                                                                                                      Figure 9. This time SIFT was used since Harris was
   Another set of images were used in the next test and                                                                           not able to detect the points we were interested in. One
Figure 7 shows how the preconditioner finds the clus-                                                                             set of inliers is connected with yellow lines and the
ter (red) at the left. In the right is a close-up of the                                                                          other with blue ones. The cluster found for the yellow



WSCG 2012 Communication Proceedings                                                                                          15                                                                                                           http://www.wscg.eu
    0.9                                                                       0.6                                                                                          1                                                                               0.6

    0.8                                                                                                                                                                   0.9

                                                                             0.58                                                                                         0.8
    0.7                                                                                                                                                                                                                                                   0.55
                                                                                                                                                                          0.7
    0.6
                                                                             0.56
                                                                                                                                                                          0.6
    0.5                                                                                                                                                                                                                                                    0.5
                                                                                                                                                                          0.5
    0.4                                                                      0.54
                                                                                                                                                                          0.4
    0.3                                                                                                                                                                                                                                                   0.45
                                                                                                                                                                          0.3
                                                                             0.52
    0.2
                                                                                                                                                                          0.2

    0.1                                                                       0.5                                                                                         0.1                                                                              0.4

      0                                                                                                                                                                    0
      0.2   0.3   0.4     0.5         0.6         0.7     0.8    0.9    1             0.28    0.29   0.3   0.31     0.32   0.33   0.34     0.35   0.36   0.37              0.1    0.2     0.3         0.4    0.5   0.6    0.7         0.8     0.9   1            0.28   0.3    0.32     0.34   0.36   0.38         0.4   0.42   0.44     0.46




Figure 8: A set with an inlier ratio of 63.38% Left:                                                                                                                  Figure 11: The Cluster (left) becomes larger in size
The preconditioner finds the cluster (red). Right:                                                                                                                    when the scale in the input images are different. To
A close up of the inliers found by the modified                                                                                                                       the right is the inliers (green).
RANSAC (green).




Figure 9: c Anders Hast Two images with two
clearly separable planes. The major set of inliers
are connected with yellow lines and the minor set                                                                                                                     Figure 12: c Anders Hast. Two images with taken
of inliers with blue ones.                                                                                                                                            on different distances from the main object ( the
    0.8                                                                       0.8
                                                                                                                                                                      chuch towers). The set of inliers are connected with
   0.75                                                                      0.75                                                                                     yellow lines and the outliers are depicted with red
    0.7                                                                       0.7

   0.65                                                                      0.65                                                                                     crosses.
    0.6                                                                       0.6
                                                                                                                                                                          0.5                                                                              0.5
   0.55                                                                      0.55
                                                                                                                                                                         0.48                                                                             0.48
    0.5                                                                       0.5
                                                                                                                                                                         0.46                                                                             0.46
   0.45                                                                      0.45
                                                                                                                                                                         0.44                                                                             0.44
    0.4                                                                       0.4
                                                                                                                                                                         0.42                                                                             0.42
   0.35                                                                      0.35
                                                                                                                                                                          0.4                                                                              0.4
      0.1   0.2     0.3         0.4         0.5         0.6     0.7    0.8      0.1          0.2     0.3          0.4      0.5       0.6          0.7      0.8
                                                                                                                                                                         0.38                                                                             0.38

                                                                                                                                                                         0.36                                                                             0.36


Figure 10: The preconditioner finds the first plane                                                                                                                      0.34                                                                             0.34

                                                                                                                                                                         0.32                                                                             0.32

corresponding to the cluster (left). RANSAC finds                                                                                                                           0.2     0.3         0.4         0.5     0.6         0.7         0.8     0.9      0.2         0.3          0.4       0.5          0.6         0.7       0.8          0.9



at the inliers corresponding to that cluster and it
                                                                                                                                                                      Figure 13: A set with an inlier ratio of 63.38%
is removed from the set. The preconditioner finds
                                                                                                                                                                      Left: The preconditioner finds the cluster (red).
the second plane (right) corresponding to the clus-
                                                                                                                                                                      Right: A close up of the inliers found by the modi-
ter (red)
                                                                                                                                                                      fied RANSAC (green).

ones is shown in the left of Figure 10 and the other                                                                                                                  better matches of the images that were taken on differ-
cluster in the right. Since the clusters were easily sep-                                                                                                             ent distances to the object. Hence, the same problem
arated the preconditioner and modified RANSAC was                                                                                                                     of scale will occur. Figure 13 shows how the precon-
run to find the first set of inliers. Then these were re-                                                                                                             ditioner finds just a part of the set (depending on the
moved from the whole set and the procedure was re-                                                                                                                    tolerance εc ), which once again becomes more spread
peated. The next set of inliers was easily found by the                                                                                                               over a larger area. Anyhow, the modified RANSAC
proposed approach. The result is on row six and seven                                                                                                                 will find the whole set and the result is on row nine in
in the Table 1 for each set.                                                                                                                                          Table 1.
Scale Differences                                                                                                                                                     Efficiency
One aerial image was scaled down to 75% of its size to                                                                                                                Some further testing were done to test the efficiency
examine the impact on the cluster. As can be seen from                                                                                                                of the method and the results are shown in Table 2.
Figure 11 the shape of the cluster is different from the                                                                                                              Four sets of aerial images (four first rows) and six sets
one in Figure 8 even if they are exactly the same im-                                                                                                                 of photos taken on the ground (rows five to ten) were
ages. Depending on the cluster approach used and the                                                                                                                  used. The tests were once again performed 10 000
tolerance εc , the preconditioner will find such sets as                                                                                                              times. In the first column is the size of the clusters
well and the modified RANSAC has no problems of                                                                                                                       obtained by the preconditioner. Next is the theoreti-
handling them. (See row eight in Table 1.)                                                                                                                            cal number of iterations (N). Then follows the mean µ
  In the second test a pair of images, shown in Fig-                                                                                                                  and standard deviation σ of the number of iterations
ure 12 were matched using SIFT in order to obtain                                                                                                                     needed by the modified RANSAC to find the inliers.



WSCG 2012 Communication Proceedings                                                                                                                              16                                                                                                                   http://www.wscg.eu
                                                                   A similar case is when the images are taken during a
         Cluster      Iter.                  Inl.                  forward camera movement, which yields images with
           Size          N     µ      σ        µ      σ            different scales. It has been shown that the precon-
    1       559        19 7.5 1.5         551.0     0.09           ditioner is able to handle moderate changes in scale,
    2        82      569 28.9 11.9         73.6      4.9           even if only a part of the cluster is found because the
    3        37    9144 7.9 2.3            36.7     0.48           cluster becomes proportionally larger, i.e it is spread
    4        50 2.3 · 104 14.5 6.2         49.0      0.7           out.
    5       221        61 10.9 1.9        196.9      0.5              When there are multiple planes in the image, the
    6       141      214 19.6 5.1         119.8     0.97           cluster will be a bit different and sometimes it is even
    7       105      891 25.7 10.5        101.0     0.25           separable in space, but not always. Here some more
    8        61    6109 20.6 7.7           53.1      1.3           sophisticated clustering algorithm could be used in or-
    9        44 6.0 · 104 6.0 0.03         56.0      0.0           der to separate the clusters in a more accurate way.
    10       56 2.4 · 106 6.2 0.5          67.0     0.09           Nonetheless, the preconditioner was able to find the
                                                                   main cluster in all our tests and the modified RANSAC
Table 2: Four test runs for aerial images and six for              extracted all inliers from the set. Hence, it is possible
images with multiple planes, with the cluster size,                to modify and use some other version of RANSAC
the mean and standard deviation for number of it-                  that is able to yield separate planes such as Multi-
erations and inliers.                                              RANSAC [ZKM05]. Otherwise, one could also in
                                                                   many cases extract one cluster at a time and run the
The next values in the end of the row is the µ and σ of            modified RANSAC on each of them.
number of inliers.                                                    The size of the cluster will also affect the result and
   The proposed approach is able to find most of the               different εc could be tested. Moreover, it is possible to
inliers with low deviation, except for the case on row             change the performance by changing how many sam-
2 and 8, which have a σ greater than 1.0. Remember                 ples are drawn. Usually four samples are drawn in
that only 4 iterations are done in the local optimization          standard RANSAC. However, by increasing this num-
step and the σ could be decreased by increasing this               ber to half of the current consensus set, but obviously
number, which of course would increase the number                  never less than four, performance was increased for
of iterations in total.                                            the modified RANSAC. One could experiment further
   Obviously, the proposed approach is very efficient              with what is actually the optimal number to use.
as it reduces the number of iterations while still main-              We delimited ourselves to use the four point DLT.
taining a high accuracy in terms of number of inliers              Nonetheless, there is nothing that prevent using other
found. Most remarkably is that the preconditioner                  types of homographies. In any case, the output of the
makes it, not only possible, but even easy and fast to             preconditioner is independent of the homography. It
find the consensus set when the theoretical number of              is just the result of the modified RANSAC that might
iterations exceeds tenth’s of thousands and even mil-              change depending on what homography is being used.
lions. The result on row ten is from a set with an inlier          Moreover, we used a clustering algorithm that was
ratio of 0.0372 and the theoretical number of iterations           easy to implement but is not the fastest one. Never-
exceeds 2.4 million iterations. By using the precon-               theless, what clustering algorithm to use is not so im-
ditioner the number of iterations were 6.2 on average              portant. The important thing is that it finds the cluster
with only 0.5 in deviation. All 67 inliers were found in           and preferably does that fast.
almost every run with a deviation of only 0.09. When
the proposed approach does not find all inliers in ev-             5   CONCLUSION AND FUTURE
ery run one could increase the number of iterations in                 WORK
the local optimization step to increase the probability            Standard RANSAC handles highly contaminated sets
of finding more inliers.                                           poorly as the probability of drawing samples giving
                                                                   an outlier free set after scoring becomes very small.
4    DISCUSSION                                                    This problem can easily be overcome by the proposed
It is important to set an appropriate tolerance ε for              preconditioner that transforms the problem to a posi-
RANSAC and likewise it is important that the toler-                tion vector space where each vector is a scaled vector
ance εc is set properly for the preconditioner. By scal-           representing the matches. An ordinary clustering al-
ing the position vector into the range [1..0] it is pos-           gorithm can be used to find the cluster of putative in-
sible to use the same tolerance for both. Nonetheless,             liers. This set is then processed by a modified version
care must be taken so that the tolerance is proportional           of RANSAC that draws from this set exclusively but
to the size of the image. Moreover, one must take into             scores using the whole set. This approach will increase
account the scale differences as it will affect the size of        performance substantially for contaminated sets. The
the cluster and the tolerance must be set accordingly.             preconditioner can be used also for sets with low con-



WSCG 2012 Communication Proceedings                           17                                         http://www.wscg.eu
tamination as the clustering algorithm is relatively fast                       tions - Volume Part I (Berlin, Heidelberg, 2007), ICCSA’07,
compared to estimation and scoring in the RANSAC                                Springer-Verlag, pp. 992–1002.
procedure.                                                                  [Low04] Lowe D. G.: Distinctive image features from scale-
                                                                               invariant keypoints. International Journal of Computer Vision
  The preconditioner can be modified in such way that                          60, 2 (2004), 91–110.
more powerful clustering methods are used in order                          [Mac67] MacQueen J. B.: Some methods for classification and
to find more than one projection plane. Moreover, it                           analysis of multivariate observations. In 5-th Berkeley Sympo-
should be determined how large differences in scale                            sium on Mathematical Statistics and Probability (1967), vol. 1,
                                                                               Berkeley, University of California Press, pp. 281–297.
and rotation the preconditioner can handle and also
                                                                            [MTN∗ 02] Myatt D., Torr P., Nasuto S., Bishop J., Craddock R.:
what could be done to handle the more extreme cases.
                                                                               Napsac: High noise, high dimensional robust estimation - its in
                                                                               the bag. In British Machine Vision Conference (2002), vol. 2,
6    REFERENCES                                                                pp. 458–467.
[BL07] Brown M., Lowe D. G.: Automatic panoramic image                      [MvHK∗ 06] Michaelsen E., von Hansen W., Kirchhof M., Meidow
   stitching using invariant features. International Journal of Com-           J., Stilla U.: Estimating the essential matrix: Goodsac versus
   puter Vision 74, 1 (2007), 59–73.                                           ransac. In Photogrammetric Computer Vision (2006), pp. 1–6.
[CKY09] Choi S., Kim T., Yu W.: Performance evaluation of                   [Nis03] Nister D.: Preemptive ransac for live structure and motion
   ransac family. In British Machine Vision Conference (2009),                  estimation. In International Conference on Computer Vision
   pp. 1–12.                                                                    (ICCV) (2003), pp. 109–206.
[CM02] Comaniciu D., Meer P.: Mean shift: A robust approach                 [NJD09] Ni K., Jin H., Dellaert F.: Groupsac: Efficient consensus
   toward feature space analysis. IEEE Transactions on Pattern                 in the presence of groupings. In ICCV (2009), IEEE, pp. 2193–
   Analysis Machine Intelligence (PAMI) 24, 5 (2002), .603–619.                2200.
[CM05] Chum O., Matas J.: Matching with prosac - progressive                [Pol00] Pollefeys M.: Automated reconstruction of 3d scenes from
   sample consensus. In IEEE Conference on Computer Vision and                  sequences of images. ISPRS Journal of Photogrammetry and
   Pattern Recognition (CVPR) (2005), pp. 220–226.                              Remote Sensing 55, 4 (2000), 251–267.
[CM08] Chum O., Matas J.: Optimal randomized ransac. IEEE                   [RFP08] Raguram R., Frahm J.-M., Pollefeys M.: A comparative
   Transactions on Pattern Analysis and Machine Intelligence 30,               analysis of ransac techniques leading to adaptive real-time ran-
   8 (2008), 1472–1482.                                                        dom sample consensus. In European Conference on Computer
                                                                               Vision (ECCV) (2008), pp. 500–513.
[CMK03] Chum O., Matas J., Kittler J.: Locally optimized ransac.
   In the Annual Pattern Recognition Symposium of the German                [RFP09] Raguram R., Frahm J.-M., Pollefeys M.: Exploiting un-
   Association for Pattern Recognition (DAGM) (2003), pp. 236–                 certainty in random sample consensus. In International Confer-
   243.                                                                        ence on Computer Vision (ICCV) (2009), pp. 2074–2081.
[CMO04] Chum O., Matas J., Obdrzalek S.: Enhancing ransac by                [RH06] Rodehorst V., Hellwich O.: Genetic algorithm sample con-
   generalized model optimization. In Asian Conference on Com-                 sensus (gasac) - a parallel strategy for robust parameter estima-
   puter Vision (ACCV) (2004).                                                 tion. In IEEE Computer Society Conference on Computer Vision
                                                                               and Pattern Recognition Workshop (CVPRW) (2006), pp. 1–8.
[CRW91] Capoyleas V., Rote G., Woeginger G.: Geometric clus-
   terings. Journal of Algorithms 12 (1991), 341–356.                       [SLK09] Sattler T., Leibe B., Kobbelt L.: Scramsac: Improv-
                                                                               ing ransac’s efficiency with a spatial consistency filter. In
[DH72] Duda R. O., Hart P. E.: Use of the hough transformation to
                                                                               International Conference on Computer Vision (ICCV) (2009),
   detect lines and curves in pictures. Communications of the ACM
                                                                               pp. 2090–2097.
   15 (1972), 11–15.
                                                                            [SWK07] Schnabel R., Wahl R., Klein R.: Efficient ransac for
[FB74] Finkel R. A., Bentley J. L.: Quad trees a data structure for
                                                                               point-cloud shape detection. Computer Graphics Forum 26, 2
   retrieval on composite keys. Acta Informatica 4, 1 (1974), 1–9.
                                                                               (2007), 214–226.
[FB81] Fischler M. A., Bolles R. C.: Random sample consensus: A
                                                                            [Sze10] Szeliski R.: Computer vision : Algorithms and applica-
   paradigm for model fitting with applications to image analysis
                                                                                tions. Computer 5, 3 (2010), 832.
   and automated cartography. Communications of the ACM 24
   (1981), 381–395.                                                         [TBK08] Tsakok J. A., Bishop W., Kennings A.: kd-tree traversal
                                                                               techniques. 2008 IEEE Symposium on Interactive Ray Tracing
[FP05] Frahm J. M., Pollefeys M.: Ransac for (quasi-) de-generate
                                                                               44, 1 (2008), 190–190.
   data (qdegsac). In IEEE Conference on Computer Vision and
   Pattern Recognition (CVPR) (2005), pp. 220–226.                          [TZ00] Torr P. H. S., Zisserman A.: Mlesac: A new robust estima-
                                                                               tor with application to estimating image geometry. Computer
[HH07] Hollander R. J. M. D., Hanjalic A.: A combined ransac-
                                                                               Vision and Image Understanding 78 (2000), 138–156.
   hough transform algorithm for fundamental matrix estimation.
   In British Machine Vision Conference (2007).                             [VJFS05] Vedaldi A., Jin H., Favaro P., Soatto S.: Kalmansac:
                                                                               Robust filtering by consensus. In International Conference on
[HS88] Harris C., Stephens M.: A combined corner and edge de-
                                                                               Computer Vision (ICCV) (2005), pp. 633–640.
   tection. In Alvey Vision Conference (1988), pp. 147–151.
                                                                            [VL01] Vincent E., Laganiere R.: Detecting planar homographies
[HZ03] Hartley R. I., Zisserman A.: Multiple View Geometry â 2nd
                                                                               in an image pair. Image and Signal Processing and Analysis
   edition. Cambridge University Press, 2003.
                                                                               (2001), 182–187.
[JMF99] Jain A., Murty M., Flynn P.: Data clustering - a review.
                                                                            [ZK06] Zhang W., Kosecka J.: A new inlier identification scheme
   ACM Computing Surveys 31, 3 (1999), 264–323.
                                                                               for robust estimation problems. In Proceedings of Robotics:
[KK06] K K. T., Kondo E.: Incremental ransac for online reloca-                Science and Systems (Philadelphia, USA, August 2006).
   tion in large dynamic environments. In IEEE International Con-
                                                                            [ZKM05] Zuliani M., Kenney C., Manjunath B.: The multiransac
   ference on Robotics and Automation (ICRA) (2006), pp. 1025–
                                                                               algorithm and its application to detect planar homographies.
   1030.
                                                                               In The International Conference on Image Processing (ICIP)
[LK07] Lee J. J., Kim G.: Robust estimation of camera homog-                   (2005), vol. 3, pp. 153–156.
   raphy using fuzzy ransac. In Proceedings of the 2007 inter-
   national conference on Computational science and its applica-




WSCG 2012 Communication Proceedings                                    18                                                http://www.wscg.eu
                            Fast GPU Garment Simulation
                               and Collision Detection
                  Tzvetomir I. Vassilev                                              Bernhard Spanlang
            Dept. of IIT, University of Ruse                                EventLab, Universitat de Barcelona
                   8 Studentska St                                          Campus de Mundet - Edifici Teatre
                 Bulgaria 7017, Ruse                                         Passeig de la Vall d'Hebron 171,
                  tvassilev@uni-ruse.bg                                          Spain 08035, Barcelona
                                                                                     bspanlang@ub.edu


                                                        ABSTRACT
This paper describes a technique for garment simulation and collision detection implemented on modern
Graphics Processors (GPU). It exploits a mass-spring cloth model with velocity modification approach to over-
come the super-elasticity. Our novel algorithms for cloth-body and cloth-cloth collision detection and response
are based on image-space interference tests. For collision detection a 3D texture is generated, in which each slice
represents depth and normal information for collision detection and response. These algorithms were implement-
ed to build a fast web-based virtual try-on system. Our simulation starts from flat garment pattern meshes and
performs the entire seaming and cloth draping simulation on the GPU. By mapping cloth properties of real fabric
measurements we are able to approximate the real drape behaviour of different types of fabric, taking into ac-
count different warp and weft fabric drape properties. As the results section shows the average time of dressing a
virtual body with a garment on state of the art graphics hardware is 0.2 seconds.

Keywords
Cloth Simulation, GPU programming, Collision detection.
                                                                      of general-purpose computation beyond the graphics
1. INTRODUCTION                                                       applications for which they were designed and many
Physical simulation and elastic deformable objects                    researchers have utilized them in cloth modelling
have been widely used by researchers in computer                      [Zel05], [GW05].
graphics. The main applications of garment simula-
tion are in the entertainment industries, in the fashion              2. BACKGROUND
design industry and in electronic commerce when                       Previous work in cloth simulation
customers shop for garments on the web and try them                   Physically based cloth modelling has been a problem
on in a virtual booth.                                                of interest to computer graphics researchers for more
The graphics processing unit (GPU) on today's com-                    than two decades. First steps, initiated by Terzopou-
modity video cards has evolved into an extremely                      los et al. [TPBF87], characterised cloth simulation as
powerful and flexible processor [LHG*06]. The latest                  a problem of deformable surfaces and used the finite
graphics architectures provide huge memory band-                      element method and energy minimisation techniques
width and computational power, with fully program-                    borrowed from mechanical engineering. Since then
mable vertex and pixel processing units that support                  other groups have been formed [BHW94], [EWS96],
vector operations up to full IEEE floating point preci-               [CYTT92] challenging the cloth simulation using
sion. Architecturally, GPUs are highly parallel                       energy or particle based methods.
streaming processors optimized for vector operations,                 Provot [Pro95] used a mass-spring model to describe
with single instruction on multiple data (SIMD) pipe-                 rigid cloth behaviour, which proved to be faster than
lines. Not surprisingly, these processors are capable                 the techniques described above. Its major drawback
Permission to make digital or hard copies of all or part of           is the super-elasticity. In order to overcome this prob-
this work for personal or classroom use is granted without            lem he applied a position modification algorithm to
fee provided that copies are not made or distributed for              the ends of the over-elongated springs. However, if
profit or commercial advantage and that copies bear this              this operation modifies the positions of many verti-
notice and the full citation on the first page. To copy               ces, it may elongate other springs. Vassilev et al.
otherwise, or republish, to post on servers or to redistrib-          [VSC01] used a velocity modification approach to
ute to lists, requires prior specific permission and/or a fee.        solve the super-elasticity problem.




WSCG 2012 Communication Proceedings                              19                                        http://www.wscg.eu
The nature of the mass-spring system is suitable for                             The external forces can differ in nature depending
implementation on the GPU. NVIDIA [Zel05] have                                   on what type of simulation we wish to model. The
provided a free sample demo of a mass-spring cloth                               most frequent ones are gravity and viscous damping.
simulation on their graphics processors. Their cloth                             All the above formulations make it possible to com-
model is quite simple and does not simulate re-                                  pute the force fij(t) applied on cloth vertex pij at any
sistance to bending. Rodriguez-Navarro et al. have                               time t. The fundamental equations of Newtonian dy-
published two implementations of cloth model on the                              namics can be integrated over time by a simple Euler,
GPU. The first is based on a mass-spring system                                  Verlet or Runge-Kutta method [PTVF92].
[RNSS05] and the second on the finite element meth-
od [RNS06]. Georgii and Westermann [GW05] com-                                   Collision detection
pared two possible implementations of mass-spring                                Collision detection (CD) and response prove to be the
systems on the GPU and tested them with cloth simu-                              bottleneck of dynamic simulation algorithms that use
lation. A GPU accelerated mass-spring system has                                 highly discretised surfaces. Most CD algorithms be-
been used in other fields like surgical simulation                               tween cloth and other objects in the scene are based
[MHS05]. The advantage of the mass-spring system,                                on geometrical object-space (OS) interference tests.
described in this paper, is that it implements on the                            Some apply a prohibitive energy field around the
GPU a velocity modification approach for overcom-                                colliding objects [TPBF87], but most of them use
ing the super-elasticity, which results in a faster and                          geometric calculations to detect penetration between
more realistic simulation. Unlike other GPU imple-                               a cloth particle and a triangle of the object together
mentations we perform also the garment seaming                                   with techniques that reduce the number of tests.
process on the GPU and we use fabric property                                    Most common approaches are voxel or octree subdi-
measurements in order to approximate the behaviour                               vision [Gla98]. Another solution is to use a bounding
of real fabric to a good degree.                                                 box (BB) hierarchy [BW98], [Pro97]. Objects are
Mass-spring model of cloth                                                       grouped hierarchically according to proximity rules
The method, described in this work is based on the                               and a BB is pre-computed for each object. The colli-
cloth model described in [VSC01]. The elastic model                              sion detection is then performed by analysing BB
of cloth is a mesh of l×n mass points, each of them                              intersections in the hierarchy. Other techniques ex-
linked to its neighbours by massless springs of natural                          ploit proximity tracking [VM95] or curvature compu-
length greater than zero. There are three different                              tation [Pro97] to reduce the large number of collision
types of springs: structural, shear, and flexion, which                          checks, excluding objects or parts which are impossi-
implement resistance to stretching, shearing and                                 ble to collide.
bending, correspondingly.                                                        Another approach to CD is based on image-space
Let pij(t), vij(t), aij(t), where i=1,…, l and j=1,…, n,                         (IS) tests [SF91], [MOK95], [BWS99]. These algo-
be correspondingly the positions, velocities, and ac-                            rithms use the graphics hardware to render the scene
celerations of the mass points at time t. The system is                          and then perform checks for interference between
governed by Newton’s basic law:                                                  objects based on the depth information of the ren-
                                                                                 dered image. In this way the 3D problem is reduced
   fij = m aij,                                                       (1)        to 2.5D. Vassilev et al. [VSC01] applied this tech-
                                                                                 nique for detecting collisions between cloth and body
where m is the mass of each point and fij is the sum of                          when dressing virtual characters. They created depth,
all forces applied at point pij. The force fij can be di-                        normal and velocity maps using two orthogonal cam-
vided in two categories.                                                         eras that were placed at the centre of the front and the
The internal forces are due to the tensions of the                               back face of the body’s BB. The depth map was used
springs. The overall internal force applied at the point                         for detecting collisions, while the normal and velocity
pij is a result of the stiffness of all springs linking this                     maps were used for collision response. Since they
point to its neighbours:                                                         perform cloth simulation on the CPU they have to
                                                                                 read back the frame buffers from the GPU which is
                                                                               time consuming.
                                                       p kl pij 
    f int (pij ) = −   ∑     k ijkl  p kl pij − l ijkl
                                                    0
                                                                 ,   (2)        Heidelberger at al [HTG04] extended the image
                       k,l                             p kl pij                space based approach to also deal with self-collisions
                                                                
                                                                                 by creating separate layered depth images (LDIs) on
where kijkl is the stiffness of the spring linking pij and                       the GPU for front and back facing polygons. Their
pkl and lis the natural length of the same spring.                               approach only works with water tight volumes and
                                                                                 also requires the reading back of the frame buffer to
                                                                                 the CPU for analysis.




WSCG 2012 Communication Proceedings                                         20                                        http://www.wscg.eu
Allard et al [AFC*10] recently built on the LDI ap-             Two implementations of the mass-spring system are
proach but moved the whole simulation of deforma-               possible on the GPU, which were compared by
ble volumes to the GPU, avoiding the bottleneck of              Georgii and Westermann [GW05]. The first one is to
framebuffer readback. However, their approach also              directly follow the CPU implementation, which they
relies on watertight volume geometry.                           call edge-centred approach (ECA). The main difficul-
In Govindaraju et al. [GKLM07] collision detection              ty here is to distribute the spring forces to the correct
is regarded as a visibility problem and they use occlu-         mass points with the correct sign. To solve this they
sion queries on the graphics processor to detect colli-         use vertex shaders, but two additional render passes
sions at fast rates. Their performance tests show colli-        are required. The advantage of the ECA is that spring
sion detection at rates over 100ms though.                      forces are computed only once, but it has several
                                                                drawbacks:
Another approach to perform collision detection on
the GPU was introduced by Sud et al [SGG*06]. They              -   it requires more graphics memory for spring and
create a discrete Voronoi diagram (DVD) of the sce-                 force textures;
ne on the GPU and can therefore access proximity                - it requires at least four rendering passes;
and collision information. This is useful if the topolo-        - additive blending in the render target is used to
gy of the geometry can change, as for example in                    accumulate the force contributions, which has
fractures, etc. However, they report rates of several               precision problems on some cards.
hundred milliseconds just for creating the discrete             The second implementation uses only one for loop
Voronoi diagrams.                                               (for each mass point) and the computation of the
                                                                spring forces is the first step inside. As a result it can
The method described in this paper exploits the idea            be implemented in only one or two render passes,
of the image-space approach of Vassilev et al.                  requires less graphics memory and is more straight-
[VSC01] but we implement it entirely on the GPU                 forward to implement. Its only disadvantage is that
including the cloth simulation and therefore we elim-           each spring force is computed twice, but considering
inate the bottleneck of frame buffer readback and the           the parallel nature and tremendous power of recent
analysis of the framebuffer for collision detection and         GPUs this is negligible. Therefore the work in this
response on the CPU. Moreover, we extend the algo-              paper is based on the second method. The texture
rithm to test not only for cloth-body collisions, but           units needed for our algorithm are described in the
also to cloth-cloth collisions. Unlike [HTG04] and              next sections.
[AFC*10] our self-collision tests are not based on
LDIs but our method exploits the information we                 GPU data structures for the cloth model
have on the GPU about the separate layers of cloth.             On the GPU the mass-spring model naturally maps
Our method therefore does not rely on watertight                into several texture2Ds. Several important facts have
volume geometry and we are able to perform colli-               to be considered, when organising the data. If a tex-
sion detection and cloth-cloth collision detection at           ture is set as a rendering target, it is not available for
rates much higher than previously reported.                     reading. So, in order to compute the new velocities
                                                                and positions of cloth vertices, the old values have to
3. MASS-SPRING CLOTH ON THE                                     be stored in another texture, just for reading. We use
GPU                                                             the so called "ping-pong" technique [Göd07]: after a
Algorithm                                                       computational step, the two textures are swapped, and
Implementation of the mass-spring cloth model                   the texture from which was read before becomes the
[VSC01] on the CPU requires an algorithm similar to             new rendering target.
the following pseudo code:                                      Therefore, we need two textures (read/write) for ve-
For each spring                                                 locities, two textures (read/write) for positions and
  Compute internal forces                                       one texture for normal vectors of the cloth surface at
  Add forces to 2 end mass points                               each cloth vertex, which are also computed on the
                                                                GPU.
Endfor
For each mass point                                             The main idea of this work is to store information
                                                                about the springs connected to each mass point in an
  Add external forces
                                                                additional texture, which we call "spring connectivity
  Compute velocity                                              texture". A suitable constraint on the maximum num-
  Do collision detection and response                           ber of other vertices connected to a given mass point
  Correct velocities for over-elongated                         has to be imposed. For our cloth model this number is
     springs                                                    12, owing to GPU architecture it is simpler to reserve
  Compute new positions                                         16 values, therefore we have 4 values for future ex-
Endfor                                                          tensions. If the textures for velocities, positions and



WSCG 2012 Communication Proceedings                        21                                         http://www.wscg.eu
normals have size (texSize × texSize), then the spring          In order to use occlusion query in our case, the fol-
connectivity        texture       is       of       size        lowing steps have to be carried out:
(4*texSize)×(4*texSize). This spring matrix consists            -   Allocate a depth texture;
of 16 smaller matrices. Each entry in these 16 matri-           -   Set this texture as a depth buffer for rendering;
ces has 4 channels (RGBA) and keeps the following               -   Render a suitable geometry and perform an occlu-
information of a spring connected to the correspond-                sion query to retrieve the number of fragments
ing vertex: texture coordinates of the other spring end             that pass the depth test.
point, natural length and spring stiffness. If all chan-        After the new positions were computed by the mass
nels are equal to -1.0, this means that the entry repre-        spring simulation we call a shader which holds the
sents no connection.                                            sewn vertices together and also builds a seam depth
Seaming of garment pieces                                       texture with the following values: 0 if the vertex is
Our Virtual Try-On system reads a body file and a               not involved in a seam, 1 if the vertex is part of a
garment file and dresses the body. The garment file             seam but is not sewn yet and 0.5 if the vertex is part
holds information about the geometry of the cutting             of a seam, which is sewn. Then we set this texture as
patterns, physics parameters of the cloth and seaming           the default depth buffer. The z-buffer is turned to
information. The patterns are automatically posi-               read only, otherwise the depth values will be replaced
tioned around the body and external forces are ap-              with the ones of the incoming vertices. Next we ren-
plied along the seaming lines. The seaming lines are            der a quad with a depth value of 0.8 that covers the
discretized into groups of two or three cloth vertices          whole draw buffer. In this way the occlusion query
to be sewn together.                                            counts all fragments with a depth value greater than
                                                                0.8, that is the number of unsewn vertices. In fact we
The connectivity of cloth vertices into seams is stored         can render a quad with any depth value in the interval
in a similar texture as for the springs, which we call          (0.5, 1).
"seam connectivity texture". Each entry of the texture
keeps information about the other mass points to                The function for counting unsewn vertices does not
which the current cloth vertex has to be sewn. During           need to be called after every integration step. To
the simulation forces are applied which pull together           speed the simulation up it could be called after every
the corresponding vertices. When the vertices are               10 or 15 iterations.
closer than a certain threshold, they are marked as             Cloth Modelling Shaders
sewn and are attached to each other. The simulation             The following shaders are used in the system:
ends when all seams are done, or after a pre-defined
number of iterations which means the garment is too             Velocity shader. This is the main cloth simulation
small for the body.                                             shader. It computes the forces applied to each cloth
                                                                vertex due to springs' tension, gravity, damping and
Occlusion queries for counting sewn ver-                        seaming, then integrates over time to compute veloci-
tices                                                           ty and writes the result in the velocity texture. It also
In order to identify when the garment is sewn, the              checks for collisions, as described below and if there
number of vertices still to be sewn have to be count-           is a collision it applies a force and also modifies the
ed. Counting on the GPU can be performed using a                velocity to resolve the collision.
reduction approach similar to the max reduction, de-            Position shader. It reads from the velocity texture
scribed by Harris [Har05]. However, it cannot be                and computes the new cloth vertex positions.
applied directly. First, a 2D buffer has to be built,
                                                                Seam shader. It checks if the cloth vertices that par-
which contains ones for the not sewn vertices and
                                                                ticipate in a seaming line are close enough to be con-
zeros for the sewn ones. Then a sum reduction should
                                                                sidered sewn, holds the sewn vertices together and
be applied to the buffer, which will perform the re-
                                                                builds the seam depth texture, as described in the
quired count.
                                                                above section.
In our system we utilize GPU occlusion queries for
counting. Occlusion queries are implemented in                  4. COLLISION DETECTION BODY-
hardware on most of the recent graphics cards and               CLOTH AND CLOTH-CLOTH
they allow the programmer to render a 3D scene and              Cloth-body collision detection
to obtain the number of fragments that passed the               As explained in Section 2 this paper exploits the idea
depth test. There is an internal counter, which is ini-         of Vassilev et al. [VSC01] for collision detection.
tially set to zero, and during the rendering it is in-          However, we do not build velocity maps, because the
creased by one for each fragment that passes the                current system does not animate the virtual body rep-
depth test.                                                     resentation. The garment is dressed and simulated on
                                                                a static body. To build the normal maps more effi-



WSCG 2012 Communication Proceedings                        22                                        http://www.wscg.eu
ciently we use a simple vertex shader, which replaces           from zero. Our system only tests for collisions be-
the vertex colour RGB values with the XYZ coordi-               tween these different cloth layers.
nates of the normal. In addition, to reduce the number          The idea of this work is to use the same image-space
of texture units, the front and back maps are placed in         approach for detecting collisions between layers of
a single texture, as shown in Figure 1.                         cloth. For this purpose several maps are generated
                                                                (Fig. 2):
                                                                -   Map[0]: body depth and normals
                                                                -   Map[1]: body and cloth-layer 0 depth and nor-
                                                                    mals
                                                                - …
                                                                - Map[n]: body and cloth-layers 0 to n-1 depth and
                                                                    normals
                                                                The number of maps, n, depends on the number of
                                                                cloth layers we wish to simulate. Map[0] is generated
                                                                only once at the beginning of the simulation, because
  Figure 1: Front and back normal maps in one tex-
                                                                the body does not move in our case. All other maps
                          ture
                                                                have to be generated at each iteration step. If we want
And finally the normal and depth maps are placed in             to simulate garments on a body in motion, map[0] has
the same texture unit; RGB representing the normal              to be generated at each iteration as well.
coordinates and the alpha channel contains the depth.
This speeds up the simulation, because when testing             Collision checking
for collisions the velocity shader samples the colli-           The computation of internal, external forces and ve-
sion texture only once to get depth and normal values           locities, as well as collision detection and response is
of the front and the back of the body.                          performed in the velocity shader, as described above.
                                                                This shader is called only once per integration step.
                                                                When testing for collisions of a particular cloth ver-
                                                                tex, which belongs to cloth layer i, we have to check
                                                                if it collides with the body and all layers beneath it,
                                                                which means that we have to use map[i] for CD and
                                                                response. All maps are stored in a 3D texture in
                                                                which each slice corresponds to a depth/normal map,
                                                                as described above. The properties of each cloth ver-
                                                                tex, such as mass, elasticity, cloth layer number, etc.,
                                                                are stored in another texture. The velocity shader uses
                                                                the layer number to sample the appropriate slice of
                                                                the 3D texture, for example cloth layer with number i
                                                                samples slice i and uses it for collision detection and
                                                                response as described in [VSC01]. The depth value
                                                                of the current mass point is compared to the depth
                                                                value read from the depth map. If a collision is de-
                                                                tected a repulsive force is computed and applied to
                                                                the cloth vertex using the normal vector, retrieved
 Figure 2: Normal maps generated for collision de-              from the normal map. The velocity of the cloth vertex
                    tection                                     is also modified. The collision response enables us to
Cloth-cloth collision detection                                 simulate friction between layers, too.
Our system does not aim at detecting all cloth-cloth            Applying this approach allows us to simulate one-
collisions. It does not test for collisions in one piece        way interactions only. The lower layers push the up-
of cloth as such are less likely to happen in tight gar-        per layers back to prohibit penetration, but the upper
ments on static body we simulate. When constructing             layers do not affect the layers below. So, if a pair of
garments some pieces of cloth have to be placed on              jeans is dressed on top of a loose shirt, the jeans will
top of others, for example pockets, belt loops, etc.            not push the shirt towards the body.
When a person tries on two garments one is always               In order to model interaction in both directions we do
on top of the other for example a T-shirt and a pair of         the following. The faces on the cloth surface are
jeans. The cloth pieces are grouped into layers, to             numbered from one to the number of faces and each
know which layer is on top of other layers and we can           number is encoded as vertex colour. When generating
assign a layer number to each cloth piece, starting



WSCG 2012 Communication Proceedings                        23                                        http://www.wscg.eu
the depth and normal maps the cloth surface is ren-             as the default colour and depth buffers and render
dered using flat shading, so that these colours are not         cloth layer number (i-1). In this way the body and all
interpolated and the face colour encodes the face               cloth layers from 0 to (i-2) are already present in the
number. The fragment shader, used for the generation            frame buffer and do not have to be rendered again. So
of the maps, stores the XYZ values of the normal                we have to render only the pieces with a layer number
vector in the RGB values of the map and the alpha               (i-1).
value is computed as follows:
                                                                5. RESULTS
   map_alpha = depth + face_number,                  (3)        The system was implemented in OpenGL and GLSL.
                                                                Figure 3 shows an example of the simulation of a pair
where face_number is decoded from the colour val-               of jeans and gives a closer look of pockets and belt
ues. As the depth value is from 0 to 1 and the face             loops.
number is greater than or equal to one, the two values
can be easily separated. If a collision is detected the         In order to check the system performance, it was test-
velocity shader, in addition to applying repulsive              ed on several configurations. Two implementations of
forces and modifying velocities, also writes the fol-           the Virtual-Try-On system were compared: 1) pro-
lowing alpha in the velocity texture:                           grams running on the CPU and 2) GPU-based as de-
                                                                scribed in this paper. For the second one the textures
   velo_alpha = face_number.                         (4)        used to store the cloth vertices positions and veloci-
                                                                ties were of a size 64 × 64. The influence of the upper
If there is no collision the alpha is set to zero.              to lower layers was not simulated, as for a single
Another pair of vertex/fragment shaders is used to              garment it is not significant. The performance results
apply forces to lower layers of cloth. If a lower layer         of 3 GPUs and the fastest CPU are given in Table 1
cloth face has collided with an upper layer vertex, we          and Figure 4. They show that the virtual try-on runs
apply forces to each of the three vertices of this face,        very well on a modern laptop GPU, which is about 20
which are opposite to the face normal. These forces             times faster than the fastest tested CPU. One iteration
have to be summed up for each vertex, as a vertex               includes integration, collision detection and response.
can be part of several adjacent faces that have collid-
ed. In fact we integrate the forces over time and add
them to the velocities. One of the velocity textures is
set as a rendering target and the velocities are ren-
dered three times as points with additive blending,
once for each vertex of a triangular face. A uniform
variable is set to 0, 1 or 2 before the rendering to
define which face vertex is targeted. The vertex
shader checks the forth coordinate of the velocity. If
it is greater than zero, then this is a face which has
collided to an upper layer cloth vertex. The indices of
the cloth vertices of that face are read from an index
texture. Knowing the size of the velocity texture and
the index, the output position of the vertex shader is
computed so that it projects to the targeted cloth ver-
tex (number 0, 1 or 2 of the face) in the rendering
target. As a result the fragment shader is executed for
this cloth vertex and it applies a constant force oppo-
site to the cloth normal, multiplied by the time step,
and in this way pushes the cloth back. The magnitude
                                                                 Figure 3: Layers of cloth: pockets and belt loops
of the force is determined experimentally. If there
was no collision, the new position is computed so that          The average time of putting a garment on a virtual
it is outside the viewing volume and the fragment               body using the NVIDIA GTX 460 GPU is about 0.2
shader is not executed for this vertex.                         seconds depending on the garment complexity and
                                                                size.
Maps generation                                                 Figure 6 shows results of the simulation of two gar-
As mentioned above map[0] is generated only once at             ments, jeans dressed on top of a shirt. The jeans were
the beginning. After each integration step we have to           discretised with 3600 mass points and 2100 vertexes
render maps from 1 to n in each slice of the 3D tex-            were used for the shirt. Two modifications of the
ture. In order to speed the simulation up, when gener-          cloth-cloth collision detection algorithm are depicted:
ating map[i], we first copy map[i-1] to map[i], set it



WSCG 2012 Communication Proceedings                        24                                       http://www.wscg.eu
left – no impact of upper to lower layers. The simula-
tion is faster (0.20 sec for dressing the jeans), but not
satisfactory.
right – with impact of upper to lower layers. The
simulation is slower (0.34 sec for dressing the jeans),
but of much better quality. The time spent only on
collision detection and response is 0.21 seconds for
the whole cycle of dressing a pair of jeans, which
requires 675 iterations. This means that for each it-
eration 0.31 ms is spent on cloth-cloth collision de-
tection and response.


                      Time for 1000      Iterations per
                       iterations, s         second
Intel core i7, 2.2               8.65              116
GHz
ATI Radeon                       0.57             1769
HD4850
                                                                 Figure 6: Jeans dressed on a shirt; left: no impact
NVIDIA GeForce                   0.40             2491
GTX 560M laptop                                                   of upper to lower layers; right: with impact of
                                                                               upper to lower layers
NVIDIA GeForce                   0.30             3300
GTX 460                                                          6. CONCLUSIONS
                                                                 An efficient technique for dynamic garment simula-
Table 1: Performance of the system, measured on                  tion entirely on the GPU has been presented. It im-
               3 GPUs and a CPU                                  plements a mass-spring system with velocity modifi-
                                                                 cation to overcome super elasticity and exploits an
                                                                 image-space approach for collision detection and
                                                                 response. The following more important conclusions
                                                                 can be drawn:
                                                                 -  A general mass-spring system can be implement-
                                                                    ed on the GPU using several textures for storing
                                                                    data and several connectivity textures for keeping
                                                                    spring and seaming information.
                                                                 - Hardware assisted occlusion queries can be uti-
                                                                    lised for counting unsewn cloth vertices, which
                                                                    speeds simulation up.
                                                                 - The same image-space based approach can be
                                                                    applied for detecting collisions cloth-body and
    Figure 4: Performance of CPU and 3 GPUs                         cloth-cloth when layers of cloth are simulated.
The algorithms for cloth simulation and collision                   Multiple collision maps can be stored in a 3D tex-
detection and response were also implemented using                  ture.
NVidia CUDA and OpenCL. The comparison with                      The system can simulate approximately 20 garments
the GLSL implementation [Vas10] showed that                      per second on a PC with 2 dual GPU NVidia Ge-
GLSL outperforms CUDA (OpenCL). One of the                       Force graphics cards. This is currently sufficient for
main reasons is that CUDA (OpenCL) and OpenGL                    our web based Virtual Try On service.
have to share buffers for the rendering and this buff-           The system can be extended to simulating garments
ers should be mapped and later unmapped when used                on animated virtual characters. For this purpose ve-
in CUDA (OpenCL), which slows down the simula-                   locity maps will also have to be generated and stored
tion.                                                            in another 3D texture.
                                                                 7. ACKNOWLEDGEMENTS
                                                                 Tzvetomir Vassilev's work is partly supported by a
                                                                 National Research Fund project at the University of




WSCG 2012 Communication Proceedings                         25                                      http://www.wscg.eu
Ruse, Bulgaria. Bernhard Spanlang’s work is partial-         [MOK95] Myszkowski K., Okunev O. G., Kunii
ly supported by the ERC project TRAVERSE.                       T. L.: Fast collision detection between complex
                                                                solids using rasterizing graphics hardware. The
8. REFERENCES                                                   Visual Computer 11, 9 (1995), 497–512.
[AFC*10] Allard J., Faure F., Courtecuisse H., Fali-         [Pro95] Provot X.: Deformation constraints in a
   pou F., Duriez C., Kry P. G.: Volume contact                 mass-spring model to describe rigid cloth behav-
   constraints at arbitrary resolution. ACM Trans.              iour. In Proceedings of Graphics Interface (1995),
   Graph. 29, 4 (2010), 1–10.                                   pp. 141–155.
[BHW94] Breen D., House D., Wozny M.: Predict-               [Pro97] Provot X.: Collision and self-collision detec-
   ing the drape of woven cloth using interacting               tion handling in cloth model dedicated to design
   particles. In Computer Graphics Proceedings, An-             garments. In Proceedings of Graphics Interface
   nual Conference Series (1994), vol. 94, pp. 365–             (1997), pp. 177–189.
   372.                                                      [PTVF92] Press W. H., Teukolsky S. A., Vetterling
[BW98] Baraff D., Witkin A.: Large steps in cloth               W. T., Flannery B. P.: Numerical Recipes in C,
   simulation. In Computer Graphics Proceedings,                2nd. edition. Cambridge University Press, 1992.
   Annual Conference Series (1998), SIGGRAPH,                [RNS06] Rodriguez-Navarro X., Susín A.: Non struc-
   pp. 43–54.                                                   tured meshes for cloth gpu simulation using fem.
[BWS99] Baciu G., Wong W. S., Sun H.: Recode: an                In 3rd. Workshop in Virtual Reality, Interactions,
   image-based collision detection algorithm. The               and Physical Simulations (VRIPHYS) (2006),
   Journal of Visualization and Computer Animation              pp. 1–7.
   10, 4 (1999), 181–192.                                    [RNSS05] Rodriguez-Navarro X., Sainz M., Susin
[CYTT92] Carignan M., Yang Y., Thalmann N. M.,                  A.: Gpu based cloth simulation with moving hu-
   Thalmann D.: Dressing animated synthetic actors              manoids. In Actas XV Congreso Español de In-
   with complex deformable clothes. In Computer                 formática     Gráfica     (CEIG’2005)       (2005),
   Graphics Proceedings, Annual Conference Series               J. Regincós D. M., Thomson-Paraninfo E., (Eds.),
   (1992), vol. 92, pp. 99–104.                                 pp. 147–155.
[EWS96] Eberhardt B., Weber A., Strasser W.: A               [SF91] Shinya M., Forgue M. C.: Interference detec-
   fast, flexible, particle-system model for cloth              tion through rasterization. j-J-VIS-COMP-
   draping. j-IEEE-CGA 16, 5 (Sept. 1996), 52–59.               ANIMATION 2, 4 (Oct.–Dec. 1991), 132–134.
[GKLM07] Govindaraju N. K., Kabul I., Lin M. C.,             [SGG*06] Sud A., Govindaraju N., Gayle R., Kabul
   Manocha D.: Fast continuous collision detection              I., Manocha D.: Fast proximity computation
   among deformable models using graphics proces-               among deformable models using discrete voronoi
   sors. Comput. Graph. 31, 1 (2007), 5–14.                     diagrams. ACM Trans. Graph. 25, 3 (2006),
[Gla98] Glassner N. I. B. A. S.: 3d object modelling.           1144–1153.
   SIGGRAPH 12, 4 (1998), 1–14.                              [TPBF87] Terzopoulos D., Platt J., Barr A., Fleischer
[Göd07] Göddeke D.: Gpgpu::basic math tutorial,                 K.: Elastically deformable models. Computer
   2007.                                                        Graphics (Proc. SIGGRAPH’87) 21, 4 (1987),
[GW05] Georgii J., Westermann R.: Mass-spring                   205–214.
   systems on the gpu. Simulation Modelling Prac-            [Vas10] Vassilev T.I.: Comparison of several parallel
   tice and Theory 13 (2005), 693–702.                          API for cloth modelling on modern GPUs. In
[Har05] Harris M.: Mapping computational concepts               Proceedings of CompSysTech (2010).
   to gpus. In SIGGRAPH ’05: ACM SIGGRAPH                    [VM95] Volino P., Magnenat Thalmann N.: Collision
   2005 Courses (New York, NY, USA, 2005),                      and self-collision detection: Efficient and robust
   ACM, p. 50.                                                  solutions for highly deformable surfaces. In Com-
[HTG04] Heidelberger B., Teschner M., Gross M.:                 puter Animation and Simulation ’95 (1995), Ter-
   Detection of collisions and self-collisions using            zopoulos D., Thalmann D., (Eds.), Springer-
   image-space techniques. In Journal of WSCG                   Verlag, pp. 55–65.
   (2004), pp. 145–152.                                      [VSC01] Vassilev T., Spanlang B., Chrysanthou Y.:
[LHG*06] Luebke D. P., Harris M., Govindaraju                   Fast cloth animation on walking avatars. Comput-
   N. K., Lefohn A. E., Houston M., Owens J. D.,                er Graphics Forum 20, 3 (2001), 260–267. ISSN
   Segal M., Papakipos M., Buck I.: S07 - gpgpu:                1067-7055.
   general-purpose computation on graphics hard-             [Zel05] Zeller C.: Cloth simulation on the gpu. In
   ware. In SC (2006), ACM Press, p. 208.                       SIGGRAPH ’05: ACM SIGGRAPH 2005
[MHS05] Mosegaard J., Herborg P., Sørensen T. S.:               Sketches (New York, NY, USA, 2005), ACM,
   A gpu accelerated spring mass system for surgical            p. 39
   simulation. Studies in health technology and in-
   formatics 111 (2005), 342–348.



WSCG 2012 Communication Proceedings                     26                                      http://www.wscg.eu
      Improving Active Learning with Sharp Data Reduction
                        Priscila T. M. Saito† , Pedro J. de Rezende† , Alexandre X. Falcão† ,
                                     Celso T. N. Suzuki† , Jancarlo F. Gomes†‡
                   † Instituteof Computing, University of Campinas - UNICAMP, Campinas, SP, Brazil
                     ‡ Instituteof Biology, University of Campinas - UNICAMP, Campinas, SP, Brazil
                             {maeda, rezende, afalcao, celso.suzuki, jgomes}@ic.unicamp.br


                                                         ABSTRACT
Statistical analysis and pattern recognition have become a daunting endeavour in face of the enormous amount of
information in datasets that have continually been made available. In view of the infeasibility of complete manual
annotation, one seeks active learning methods for data organization, selection and prioritization that could help
the user to label the samples. These methods, however, classify and reorganize the entire dataset at each iteration,
and as the datasets grow, they become blatantly inefficient from the user’s point of view. In this work, we propose
an active learning paradigm which considerably reduces the non-annotated dataset into a small set of relevant
samples for learning. During active learning, random samples are selected from this small learning set and the
user annotates only the misclassified ones. A training set with new labelled samples increases at each iteration and
improves the classifier for the next one. When the user is satisfied, the classifier can be used to annotate the rest of
the dataset. To illustrate the effectiveness of this paradigm, we developed an instance based on the optimum path
forest (OPF) classifier, while relying on clustering and classification for the learning process. By using this method,
we were able to iteratively generate classifiers that improve quickly, to require few iterations, and to attain high
accuracy while keeping user involvement to a minimum. We also show that the method provides better accuracies
on unseen test sets with less user involvement than a baseline approach based on the OPF classifier and random
selection of training samples from the entire dataset.
Keywords: Pattern Recognition, Machine Learning, Active Learning, Semi-Automatic Dataset Annotation, Data
Mining, Optimum-Path Forest Classifiers.

1    INTRODUCTION                                                        datasets, performing them at each iteration is very inef-
The amount of available information has been increas-                    ficient or even computationally infeasible.
ing due to the advances of computing and data acquisi-                   In this paper, to overcome the aforementioned prob-
tion technologies, resulting in large datasets. Handling                 lems, we propose a new active learning paradigm which
and analysing such increasing volume of information                      is verifiably effective and more efficient in practice,
have become humanly infeasible and highly suscepti-                      when dealing with large datasets, than those based on
ble to errors, since it is extremely time consuming and                  the current state of the art. The proposed paradigm re-
wearisome. Hence, there is an increasing demand for                      lies on a significant reduction in the dataset size to cre-
the development of effective and efficient ways to an-                   ate a small representative set of samples, for the learn-
notate these datasets.                                                   ing process. By constructing the first instance of the
Active learning techniques have been explored and rea-                   classifier based on the knowledge of as many classes
sonably successful. However, these methods fall in a                     as possible, as well as incorporating the best samples
single paradigm which requires, at each iteration, the                   at each iteration, subsequent selection and classifica-
classification of the entire dataset under annotation, fol-              tion phases are much more efficacious. This approach
lowed by the organization of all these samples accord-                   differs from the traditional active learning methods, in
ing to some criterion, in order to select the most in-                   which all samples in the database have to be classified
formative samples to be used for training the classifier.                and re-organized at each iteration.
These phases are highly interdependent and, for large                    Being a paradigm, it can be implemented using differ-
                                                                         ent strategies. This paper also presents an instantiation
                                                                         (Cluster-OPF-Rand) which has been developed to illus-
Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without
                                                                         trate the effectiveness of this paradigm. It is based on
fee provided that copies are not made or distributed for profit          the Optimum Path Forest (OPF) classifier, while relying
or commercial advantage and that copies bear this notice and             on clustering and classification for the learning process.
the full citation on the first page. To copy otherwise, or re-           Cluster-OPF-Rand prevents the user from having to an-
publish, to post on servers or to redistribute to lists, requires        notate a large (and usually wasteful) number of training
prior specific permission and/or a fee.                                  samples. Moreover, it prevents poor selection of sam-



WSCG 2012 Communication Proceedings                                 27                                          http://www.wscg.eu
                                        Non-Annotated                                  Annotated
                                           Dataset                                      Dataset




                                                                  Classifier
                                           Selector
                                                                                       Training
                                        Classification


                                                                  Learning Cycle             Annotated
                                        Organization                                          Samples


                                                                                            user
                                          Selection                 Non-labeled
                                                                   (first iteration)
                                                                       Labeled

                           Figure 1: Pipeline of the traditional active learning paradigm.

ples from a large learning set, since this set is reduced         strategies for active learning. However, it is focused on
so as to contain essentially the most representative sam-         methods that classify all samples in the database, then
ples. After this reduction, the proposed paradigm en-             organize these samples according to certain criteria and
ables the organization of the learning samples to occur           subsequently select and display the most informative
beforehand (and only once). In this particular imple-             samples to be annotated by the user, at each learning
mentation, the organization of the reduced set occurs in          iteration. For large databases, these complete phases,
a randomized fashion.                                             at each learning iteration, are very inefficient or even
The experiments performed on three datasets show that             impractical to be done computationally.
Cluster-OPF-Rand is interactively and iteratively effi-           Figure 1 illustrates the execution pipeline of the tradi-
cient, in addition to providing high accuracies earlier.          tional active learning paradigm presented in prior liter-
That is to say, the number of learning iterations is sig-         ature. This paradigm is comprised of a learning algo-
nificantly reduced with better accuracies, while requir-          rithm and a selector. The selector consists of three mod-
ing the annotation of only a small number of sam-                 ules (classification, organization and selection) that are
ples, when compared to a baseline approach using the              highly interdependent. At each iteration cycle, the sys-
OPF classifier and random selection of training samples           tem presents to the user a set of samples that consists of
from the entire dataset. The results also showed impres-          either non-labelled samples (from the entire database,
sive reductions of over 90% in user effort, at the same           in the first iteration) or labelled ones (obtained through
time providing accuracies of over 97%.                            the classifier), all chosen by the selector. As these sam-
                                                                  ples are annotated by the user, they are included in the
The remainder of this paper is structured as follows.
                                                                  training set to retrain the classifier for the next cycle.
Section 2 summarizes the major active learning tech-
niques presented in the literature. Section 3 presents            Besides the aforementioned inefficiency, most of the
the clustering approach based on optimum-path forest              existing research in the traditional active learning
used. Section 4 details the active learning paradigm and          paradigm has focused on binary classification. Rel-
the reduced method proposed. Section 5 discusses the              atively few approaches [12, 20, 9, 16, 11, 10] have
experiments and the accomplished results. Finally, Sec-           been proposed for multiclass active learning and are
tion 6 presents the conclusions and future work.                  typically based on extensions of predominantly binary
                                                                  active learning methods to the multiclass scenario.
                                                                  In the ANN literature, although several works [4, 1, 7]
2   BACKGROUND AND TECHNIQUES
                                                                  have explored the use of active learning in the context
Recent works in active learning have yielded a vari-              of efficient network training, this approach shows the
ety of heuristics, which are designed mostly for binary           disadvantage of being computationally expensive.
classification and are applicable primarily to classifiers        Alternatively, SVM has been used in [19, 18], under the
such as Artificial Neural Network (ANN), Support Vec-             assumption that the samples closest to the separating
tor Machine (SVM), k-Nearest Neighbour (k-NN) and                 hyperplane are the most informative ones. During the
Optimum-Path Forest (OPF).                                        iterations of relevance feedback, the method finds the
In active learning techniques, the key idea relies on             optimal hyperplane separating relevant and irrelevant
the strategy used to select the most informative sam-             samples and presents to the user the samples closest to
ples such that they allow for the achievement of greater          this hyperplane. This hyperplane is adjusted throughout
accuracies with fewer training labels annotated by the            the iterations, and after the last one, the method presents
user. Much effort has been placed in investigating                the most relevant samples as being the farthest ones to



WSCG 2012 Communication Proceedings                          28                                          http://www.wscg.eu
the hyperplane. Extensions to the multiclass scenario             and a connectivity (path-cost) function is designed such
are typically based on extensions of binary classifica-           that the maximization of a connectivity map defines an
tion using pairwise comparisons or 1-vs-all strategy.             optimum-path forest rooted at the maxima of the pdf. In
In contrast, [10] introduced a probabilistic variant of           this forest, each cluster is one optimum-path tree rooted
k-NN. Although, this variant was designed specifically            at one maximum (prototype). The pdf estimation also
for multiclass problems, it involves learning a certain           requires multiple applications of the algorithm for dif-
number of parameters. Moreover, the performance of                ferent values of k in order to select the best clustering
the method is dependent on the similarity measure used.           result as the one that produces a minimum normalized
                                                                  cut in the k-NN graph. The clusters are found by or-
A strategy, similar to the one presented in [18], was pro-        dered label propagation from each maximum, as op-
posed in [6], using a faster and more effective classi-           posed to the mean-shift algorithm of [3] which searches
fier based on Optimum-Path Forest (OPF). They devel-              for the closest maximum by following the direction of
oped greedy (GOPF) [5] and planned (POPF) [6] active              the gradient of the pdf — a strategy that does not guar-
learning strategies for CBIR systems. For a given set of          antee the assignment of a single label per maximum,
relevant and irrelevant samples, the method computes              and presents problems on the plateaux of the pdf.
an optimum-path forest using samples from the query
set for training the classifier.                                  In order to handle large datasets, this approach esti-
                                                                  mates the pdf from random samples and fast propagates
Optimum-Path Forest (OPF) is a framework for devel-
                                                                  the group labels to the remaining samples of the dataset.
oping pattern classifiers (supervised, semi-supervised
                                                                  The best k for pdf estimation is found by optimization,
or unsupervised) which defines how the samples are
                                                                  but its search interval [1, kmax] may produce different
connected by an adjacency relation that gives rise to a
                                                                  numbers of groups. The parameter kmax represents an
graph, and how to measure the connectivity (the cost
                                                                  observation scale for the dataset. If kmax is too high,
of a path in the graph generated by the adjacency) be-
                                                                  it means that we are looking at the dataset from infinity
tween them by means of a function that gives rise to an
                                                                  and so, the result will be a single cluster. As we ap-
optimum path forest.
                                                                  proximate the dataset (reducing the value of kmax), the
The supervised and the unsupervised classifiers were              number of clusters increases up to some high number
described in [14, 17], respectively. Both learning ap-            for kmax = 1. Still, the number of possible solutions is
proaches are fast and robust for large datasets [13, 2].          low, because the method produces an identical number
In addition, the classes/clusters may present arbitrary           of clusters for several values of kmax. This shows the
shapes and have some degree of overlapping. Classi-               robustness of the method in finding natural groups in
fiers based on OPF have been widely used in several ap-           the dataset for distinct observation scales. In this work,
plications and have demonstrated that OPF-based clas-             we chose kmax so as to obtain a number of groups
sifiers can be more effective and much faster than ANN            higher than the number of classes known. Note that,
and SVM based ones [14].                                          we do not use any knowledge on the classes of sam-
The following Section details the OPF based clustering            ples, but we assume that we know how many classes
approach.                                                         are present in the dataset.

3   CLUSTERING BY OPTIMUM-PATH                                    4   PROPOSED PARADIGM
    FOREST
                                                                  We propose a new paradigm for active learning in or-
The data reduction approach we implemented is based
                                                                  der to select, more efficiently and effectively, a small
on clustering by Optimum-Path Forest (OPF) [17]. This
                                                                  number of the most representative samples for train-
is a non-parametric approach which estimates the num-
                                                                  ing a classifier. The execution pipeline of the proposed
ber of natural groups in a dataset as the number of max-
                                                                  paradigm is illustrated in Figure 2.
ima of its probability density function (pdf). In this ap-
proach, each maximum of the pdf will define a cluster             In the proposed paradigm, a classifier instance is gen-
as an optimum-path tree rooted at that maximum. It                erated at each iteration. After retraining the classifier
can handle plateaux of maximum, by electing a single              (a process that relies on user annotations), the selector
root (one prototype per maximum), groups with arbi-               displays the most informative samples to the user. As
trary shapes, and some overlapping among clusters.                the classifier improves, the user is required to correct
In this unsupervised learning algorithm, an unlabelled            fewer misclassified samples and progressively develops
training set is interpreted as a graph whose nodes are            a sense of when the learning process has reached a sat-
samples (images, in this paper) and each node is con-             isfactory state.
nected with its k-closest neighbours in the feature space         Active learning methods presented in the literature dif-
to form directed arcs. The pdf value at each node is              fer from one another in their learning algorithms and in
estimated from the distance between adjacent samples,             the selection strategies employed. The main difference



WSCG 2012 Communication Proceedings                          29                                         http://www.wscg.eu
                                                   Non-Annotated                                     Annotated
                                                      Dataset                                         Dataset




                                         Reduced
                             Reduction dataset                                   Classifier
                                and                   Selector
                                                                                                     Training
                            Organization


                                                     Selection                  Learning Cycle             Annotated
                                                       and                                                  Samples
                                                   Classification
                                                                                                          user
                                                                                  Non-labeled
                                                                                 (first iteration)
                                                                                     Labeled

                             Figure 2: Pipeline of the proposed active learning paradigm.

between the proposed paradigm and previously pro-                        paradigm. In Subsection 4.1, we develop and present
posed ones lies within the selector. Traditional methods                 an effective method for the learning process.
make use of three modules that correspond to classifica-
tion, organization and selection of samples (Figure 1).                  4.1    Instantiation of the proposed
In these methods, the selection criterion is based solely                       paradigm
on a classifier that is not yet reliable. When the clas-                 As it was mentioned, any method can be incorporated
sification accuracy is still low, the organization phase                 into the proposed paradigm in order to reduce the
becomes useless, since when samples are classified, in-                  learning set and later to organize the reduced one. In
formative samples may not be selected to participate in                  this section, we present an effective method called
the organization phase and therefore they will not be                    Cluster-OPF-Rand. Figure 3 illustrates an example of
shown to the user.                                                       the pipeline of Cluster-OPF-Rand.
The proposed paradigm is based on a priori data re-                      The proposed method is divided into two modules: (1)
duction and organization of the reduced dataset. It fo-                  reduction and organization, (2) selection and classifica-
cuses on reversing the process adopted by traditional                    tion. The reduction and organization module is com-
paradigms where an classification phase occurs before                    prised of two steps: clustering and reduction of the data
the organization phase. In the proposed paradigm, the                    (steps 1 and 2 of Figure 3, respectively). The selection
selector consists of only one module of selection and                    and classification module choose and label (steps 3 and
classification. A major advantage presented by the pro-                  4 of Figure 3, respectively) the most informative sam-
posed paradigm is that the reduction and organization                    ples of the reduced set chosen in a randomized fashion.
of samples can be performed only once, unlike tradi-                     Each sample is represented by a pair (id, lbl), where id
tional methods.                                                          corresponds to the identifier of the sample and lbl cor-
Thus, the selector becomes faster, especially consider-                  responds to the label given by the classifier. Note that it
ing large databases, since the improvement of the clas-                  does not classify all samples in the dataset, but only the
sifier at each iteration does not require rearranging all                selected subset.
samples; only the selection and classification phases are                Initially, clusters are computed in order to obtain sam-
required. Moreover, a remarkably faster selection phase                  ples of all classes, as described in Section 3. One or
is completed by the choice of a small subset of samples                  more clusters represent samples of all classes in the
and the classification of only these.                                    non-labelled set, so that each cluster comprises mostly
The strategy to be developed in order to select the                      samples of a single class. Then, their roots (highlighted
most informative samples itself occurs as preprocess-                    after step 1) cover samples of all classes and are defined
ing in the module of reduction and organization (Fig-                    as an initial training set for manual annotation. This is
ure 2). This strategy should not be based on a classi-                   fundamental to be able to train the classifier with sam-
fier, because it is still unreliable, but rather based on an             ples of all classes, since the first iteration. This clas-
absolute criterion previously established (for instance,                 sifier should be as good as possible because it is used
exploring the organization of the data in the feature                    in the classification of samples, providing an initial la-
space). The classification phase is performed a pos-                     belling, in which the user is not required to annotate all
teriori, supporting the choice of the most informative                   samples shown but only to correct a small number of
samples by the selector, which follows a predetermined                   misclassified ones.
order in the reduction and organization module. In                       Besides knowing which samples are roots of clusters,
this module, different methods can be applied in our                     it is possible to identify those that are boundary sam-



WSCG 2012 Communication Proceedings                                 30                                                 http://www.wscg.eu
                                                                                                                      (1,1)    (3,?)
                       1                                2                                  3                 4


                                                                                                                    (2,?)     (4,2)



                               Figure 3: An example of pipeline of the proposed method.

ples between different clusters. A sample s is consid-                iteration. Thus, one can easily note the gain obtained
ered a boundary sample if there exists, among its k-NN                by using clustering for dataset reduction, which induces
adjacent samples, at least one whose label (given by                  the knowledge of a large number of classes, resulting
the clustering) is different from that of s. The clus-                in an early increase in accuracy. Moreover, clustering
ter boundary samples are expected to correspond to                    also allows for the choice of random samples from the
the boundary between classes. This identification of                  reduced set comprised of good representative samples,
boundary samples allows for the reduction of the learn-               instead of a much larger set of data (as in OPF-Rand).
ing set to a small relevant set (consisting of boundary               The reported results were compiled from the average
samples), since these can be considered as the most rep-              of experiments run 10 times, with randomly generated
resentative samples for improving the classifier.                     learning sets and unseen test sets for accuracy mea-
In the first iteration of the learning phase, the roots of            sures. For all datasets used, we chose 80% of the avail-
the clusters are displayed to the user, who annotates                 able samples for learning, and 20% for testing.
their labels. These samples constitute the training set
for the first instance of the classifier. For all other itera-        5.1    The Dataset Description
tions, among the samples of the reduced set (boundary
samples of the clusters) a few randomly chosen ones are               To perform the experiments we have used real-world
selected for classification. Once classified, these sam-              datasets from very diverse domains. Due to space lim-
ples are submitted to the user for confirmation of the                itations, in the present paper there are only results ob-
labels assigned by the current classifier. Since only a               tained from three datasets.
small number of misclassified samples require annota-                 The first dataset was obtained from the University of
tion, the user’s time and effort are lessened. In fact, as            Notre Dame [8]. It was originally designed to study the
the classifier improves throughout the iterations the ac-             effect of time on face recognition. The images were
tions required from the user are increasingly reduced.                acquired in several weekly sessions with the participa-
After the labels are confirmed/corrected by the user, the             tion of distinct individuals. In these sessions, different
samples are incorporated into the training set and a new              expressions (neutral, smiling, sad) were captured. In
instance of the classifier is generated. This entire cycle            this work, we concentrated on a subset containing 1,864
is repeated until the user is pleased with the accuracy of            samples with 162 features and 54 classes. Figure 4 dis-
the classification.                                                   plays specimens from this dataset.
Moreover, it is important to emphasize that different
clustering techniques (such as k-means or k-medoids)
can be used in the data reduction phase. Similarly, dif-
ferent supervised classifiers can be used in the classifi-
cation and selection phases of the proposed paradigm.
We choose OPF-based clustering since it offers many
advantages, as mentioned in Section 2.
                                                                      Figure 4: Examples of images from the Faces dataset.
5    EXPERIMENTS
For evaluation, we developed a baseline approach                      The second dataset is composed of images of parasites,
(OPF-Rand) using the OPF-classifier and random se-                    provided by a research laboratory at the University of
lection of samples. At each learning iteration, the same              Campinas, where faecal parasitological examination is
number of random samples is selected from the entire                  performed for diagnosis of enteroparasitosis present in
dataset for OPF-Rand and, from the reduced dataset,                   humans. We used a dataset consisting of 1,660 faecal
for the Cluster-OPF-Rand. This number of samples is                   samples with 262 features and 15 classes. A particu-
equal to the number suggested by Cluster-OPF-Rand                     larity of this set is that each class contains a different
based on the clustering results – a fair choice. These                number of images varying from 33 to 163 depending on
samples are classified and presented to the user for                  the parasite species found on microscope slides. Figure
annotation. The user annotates the misclassified sam-                 5 displays samples from this dataset.
ples and they are added to the training set to improve                The third one is the Pen-Based Recognition of Hand-
the OPF classifiers used in each method for the next                  written Digits dataset obtained from the UCI Machine



WSCG 2012 Communication Proceedings                              31                                         http://www.wscg.eu
                        Faces           Accuracy (%)       Total Annotated Images (%)
                      Iteration Cluster-OPF-Rand OPF-Rand Cluster-OPF-Rand OPF-Rand
                          1           94.85          85.11       6.51          6.51
                          2           97.27          94.21       7.59          8.51
                          3           98.06          97.35       8.11          9.40
                          4           98.57          98.35       8.41          9.78
                          5           98.85          98.78       8.68          9.98
   Table 1: Accuracies and total annotated images for Cluster-OPF-Rand and OPF-Rand on the Faces dataset.


                                                                    In summary, Cluster-OPF-Rand started off with a bet-
                                                                    ter performance than OPF-Rand, for all datasets anal-
                                                                    ysed. Moreover, it achieves high accuracies sooner.
                                                                    To reach the same accuracies, the randomized method
                                                                    (OPF-Rand) required more samples annotated by the
                                                                    user as well as more learning iterations than Cluster-
                                                                    OPF-Rand.
                                                                    Using the Faces dataset (Table 1), both methods achieve
                                                                    similar accuracies and both can be improved with more
                                                                    user annotations and more learning iterations. How-
                                                                    ever, Cluster-OPF-Rand allows the learning process to
                                                                    stop earlier in comparison with OPF-Rand. Further-
                                                                    more, it is important to highlight that, out of 1,469 sam-
Figure 5: Examples of images from each class of the                 ples only 132.94 (about 9.05%) had to be annotated for
structures of intestinal parasites in the Parasites dataset.        the proposed method to achieve accuracy above 99%, in
                                                                    its last (9th ) iteration using all samples on the reduced
Learning Repository [15], that consists of 10,992 ob-               set. These results are similar to those for the remaining
jects in 16 dimensions, distributed in 10 classes corre-            datasets (Tables 2 and 3). This shows that our method
sponding to the digits [0...9]. The 16 dimensions are               can outperform OPF-Rand in effectiveness.
drawn by re-sampling from handwritten digits. This
                                                                    Considering the Parasites dataset (Table 2), in the first
digits database was built from a collection of 250 sam-
                                                                    iteration, Cluster-OPF-Rand achieves accuracies above
ples from 44 writers.
                                                                    92% with less than 2% of the learning samples anno-
5.2    Results                                                      tated by the user, while the randomized method OPF-
                                                                    Rand reaches similar accuracies only from the fourth
To compare the effectiveness of each method (Cluster-
                                                                    iteration on and requiring the user to annotate more
OPF-Rand and OPF-Rand), Tables 1-3 present the
                                                                    than 3% of the learning samples. Furthermore, out of
mean accuracy and the total annotated images using the
                                                                    1,323 samples only 77.7 (about 5.87%) had to be an-
datasets Faces, Parasites and Pendigits, respectively. It
                                                                    notated for Cluster-OPF-Rand to achieve an accuracy
is important to emphasize that comparisons were not
                                                                    above 97%, in its last (25th ) iteration using all samples
performed between Cluster-OPF-Rand and methods
                                                                    in the reduced set.
that require classifying and organizing all samples in
the database, at each learning iteration, due to this               For the Pendigits dataset (Table 3), our method obtains
process being infeasible in practice.                               high accuracies in all learning iterations. In the first
Notice that the proposed method creates a new classi-               one, it presents an accuracy of 88.80%. In the remain-
fier instance at each iteration. We would like to verify            ing iterations, the accuracies tend to increase continu-
the ability of Cluster-OPF-Rand in choosing the most                ously, reaching over 99%. Furthermore, out of 8,791
representative samples from a reduced set, as well as,              samples only 79.9 (about 0.90%) had to be annotated
in which iteration, whether the user might be pleased               for the proposed method to achieve accuracy above
with the classification accuracy. Therefore, we monitor             97% in the 30th iteration. In a practical situation, a user
the mean accuracy of each instance on the unseen sam-               would be very pleased at this point, mainly considering
ples of the test set. Furthermore, for each sample set              that the randomized method (OPF-Rand) learning pro-
selected at each iteration, we simulate the user interac-           cess consists of 440 iterations, when using all available
tion by correcting the misclassified labels given by the            learning samples.
current classifier instance. Tables 1-3 help compare the            Figure 6a-b illustrates the mean accuracies and the
total number of annotated images used to increase the               number of samples annotated by the user at each
training set.                                                       iteration for each dataset using Cluster-OPF-Rand,



WSCG 2012 Communication Proceedings                            32                                          http://www.wscg.eu
                     Parasites         Accuracy (%)       Total Annotated Images (%)
                     Iteration Cluster-OPF-Rand OPF-Rand Cluster-OPF-Rand OPF-Rand
                         1           92.68          79.44       1.98          1.98
                         2           94.12          88.50       2.54          2.66
                         3           94.94          91.60       2.91          3.06
                         4           95.30          92.67       3.12          3.29
                         5           95.21          93.64       3.36          3.54
 Table 2: Accuracies and total annotated images for Cluster-OPF-Rand and OPF-Rand on the Parasites dataset.

                    Pendigits         Accuracy (%)       Total Annotated Images (%)
                    Iteration Cluster-OPF-Rand OPF-Rand Cluster-OPF-Rand OPF-Rand
                        1           88.80          70.36       0.13          0.13
                        2           90.96          82.97       0.22          0.25
                        3           91.99          87.49       0.29          0.30
                        4           92.89          89.72       0.35          0.35
                        5           93.70          91.25       0.40          0.40
 Table 3: Accuracies and total annotated images for Cluster-OPF-Rand and OPF-Rand on the Pendigits dataset.


respectively. We used logarithmic scales, due to the               tions than those demanded by OPF-Rand. Addition-
size of these datasets. Our method requires a greater              ally, it relies on fewer interactions with the user whose
effort by the user in the first few iterations, since the          effort is reduced to almost none after a few iterations.
selected samples are the most difficult to classify.               Therefore, clustering improves the knowledge of sam-
However, looking at the end of the learning phase,                 ples from most/all classes. From the results presented,
one can observe that the proposed method demands                   we can see that clustering roots allow us to obtain high
less effort from the user, who annotates much fewer                accuracy since the first iteration. In the remaining it-
samples after some iterations (reaching almost no                  erations, the growth of accuracy is faster for Cluster-
annotations at all).                                               OPF-Rand, which also proves beneficial for the reduc-
The reduction strategy becomes very important in a pro-            tion strategy proposed.
cess where a goal is to limit the number of iterations to
as few as possible. In this context, selecting samples             6   CONCLUSION AND FUTURE
that speed up the improvement of the classifier through                WORK
the iterations becomes critical. The more difficult to             In this work, we introduced an efficient active learning
classify the selected samples in the current iteration are,        paradigm which enables the reduction and organization
the more useful they are to improve the classifier for the         of the learning set a priori. A first instantiation, Cluster-
next iteration. Therefore, the selection of hard to clas-          OPF-Rand, of the proposed paradigm was developed in
sify samples coupled with the early knowledge of all               order to illustrate its effectiveness. The data reduction
classes allow for higher accuracy sooner.                          is based on clustering and the organization uses a ran-
Note that, in the first iteration with all datasets (Ta-           domized choice of samples of the reduced set, which
bles 1-3), Cluster-OPF-Rand provides higher accura-                contains the most representative (boundary) ones for
cies than OPF-Rand. Using roots of each cluster for the            the learning process. Cluster-OPF-Rand enables us to
first classifier instance becomes really important due             achieve the desired results, by using the knowledge of
to its use in the next iteration. This reduces the time            both user and classifier, at each learning iteration, along
and effort by the user who mainly has only to confirm              with the reduction strategy developed.
the labels of the samples that have already been classi-           We concluded that our paradigm is more suitable to
fied. Hence, this first instance of the classifier should          handle large datasets than the traditional one where
be based on the knowledge of as many classes as pos-               methods require, at each learning iteration, the clas-
sible (ideally, all of them). In later learning iterations,        sification of all samples in the database, followed by
the performance gain depends on the choice of good                 their organization, and, finally selection. The proposed
samples. With the proposed method, it is possible to               paradigm enables the reduction and organization phases
improve these choices by reducing a large dataset to a             to occur only once, as pre-processing. In addition, clas-
small subset consisting of boundary cluster samples for            sification does not occur for all samples in the database,
the training of the subsequent classifiers.                        but to a small set of samples.
It is clear that Cluster-OPF-Rand, in addition to pro-             Experiments with datasets from distinct applications
viding high accuracies, requires fewer learning itera-             showed that Cluster-OPF-Rand, in addition to achiev-



WSCG 2012 Communication Proceedings                           33                                           http://www.wscg.eu
                                 (a)                                                                        (b)
Figure 6: Comparison of Cluster-OPF-Rand on the three datasets. (a) Mean accuracy on the test sets. (b) Total
annotated samples in each iteration (in percentage).

ing higher accuracy sooner, requires fewer learning it-                     [6]   A. T. da Silva, A. X. Falcão, and L. P. Magalhães. Active learn-
erations than those presented by OPF-Rand. Moreover,                              ing paradigms for CBIR systems based on optimum-path forest
                                                                                  classification. Pattern Recognition, 44:2971–2978, 2011.
it is important to highlight that the user’s time and ef-
                                                                            [7]   D. T. Davis and J. N. Hwang. Attentional focus training by
fort are reduced to almost none after just a few itera-                           boundary region data selection. In Intern. Joint Conference on
tions. Furthermore, experiments also demonstrated that                            Neural Networks (IJCNN), volume 1, pages 676–681, 1992.
it is possible to reduce the user’s effort by over 90%,                     [8]   Faces. Biometrics Database Distribution. The Computer Vision
obtaining a classification accuracy above 97%.                                    Laboratory, University of Notre Dame, 2011. www.nd.edu/
                                                                                  ~cvrl/CVRL/Data_Sets.html.
Considering that new technologies have provided large
                                                                            [9]   A. Holub, P. Perona, and M.C. Burl. Entropy-based active
datasets for many applications and that he traditional                            learning for object recognition. In CVPRW, pages 1–8, 2008.
paradigms for active learning present unacceptable                          [10] P. Jain and A. Kapoor. Active learning for large multi-class
training times, we conclude that the proposed paradigm                           problems. In IEEE Conference on Computer Vision and Pat-
is an important contribution to active machine learning.                         tern Recognition(CVPR), pages 762–769, 2009.
Future works include developing other ways to explore                       [11] A. Kapoor, K. Grauman, R. Urtasun, and T. Darrell. Gaussian
the reduction and organization of data, for instance, a                          Processes for Object Categorization. International Journal of
                                                                                 Computer Vision (IJCV), 88:169–188, 2010.
strategy that relies on an absolute criterion established
                                                                            [12] X. Li, L. Wang, and E. Sung. Multi-label SVM Active Learn-
a priori which explores the organization of the data in                          ing for Image Classification. In International Conference on
the feature space.                                                               Image Processing (ICIP), volume 4, pages 2207–2210, 2004.
                                                                            [13] J. P. Papa, A. X. Falcão, V. H.C. de Albuquerque, and J. M.R.S.
7     ACKNOWLEDGEMENTS                                                           Tavares. Efficient supervised optimum-path forest classification
                                                                                 for large datasets. Pattern Recognition, 45:512–520, 2012.
This work has been supported by grants from Con-
                                                                            [14] J. P. Papa, A. X. Falcão, and C. T. N. Suzuki. Supervised pattern
selho Nacional de Desenvolvimento Científico e                                   classification based on optimum-path forest. Intern. Journal of
Tecnológico (CNPq): 481556/2009-5, 303673/2010-9,                                Imaging Systems and Technology (IJIST), 19(2):120–131, 2009.
552559/2010-5, 483177/2009-1, 473867/2010-9; from                           [15] Pendigits. Pen-Based Recognition of Handwritten Dig-
Coordenação de Aperfeiçoamento de Pessoal de Nível                               its Dataset. UCI - Machine Learning Repository, 2011.
Superior (CAPES): 01-P-01965/2012; from Fun-                                     archive.ics.uci.edu/ml/datasets/Pen-Based+
                                                                                 Recognition+of+Handwritten+Digits.
dação de Amparo à Pesquisa do Estado de São Paulo
                                                                            [16] G.-J. Qi, X.-S. Hua, Y. Rui, J. Tang, and H.-J. Zhang. Two-
(FAPESP): 07/52015-0 and from FAEPEX/UNICAMP.                                    dimensional multilabel active learning with an efficient online
                                                                                 adaptation model for image classification. IEEE Transact. on
8     REFERENCES                                                                 Pattern Analysis and Machine Intel., 31(10):1880–1897, 2009.
[1]   D. Angluin. Queries and Concept Learning. Machine Learning,           [17] L. M. Rocha, F. A. M. Cappabianco, and A. X. Falcão. Data
      2:319–342, 1988.                                                           clustering as an optimum-path forest problem with applications
[2]   F.A.M. Cappabianco, J.S. Ide, A.X. Falcão, and C.-S.R. Li.                 in image analysis. Intern. Journal of Imaging Systems and
      Automatic subcortical tissue segmentation of mr images using               Technology (IJIST), 19(2):50–68, 2009.
      optimum-path forest clustering. In International Conference on        [18] S. Tong and E. Chang. Support vector machine active learning
      Image Processing (ICIP), pages 2653–2656, 2011.                            for image retrieval. In ICM, pages 107–118. ACM, 2001.
[3]   Yizong Cheng. Mean shift, mode seeking, and clustering.               [19] S. Tong and D. Koller. Support vector machine active learn-
      TPAMI, 17(8):790–799, 1995.                                                ing with applications to text classification. Journal of Machine
[4]   D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning               Learning Research (JMLR), 2:45–66, 2002.
      with statistical models. JAIR, 4:129–145, 1996.                       [20] R. Yan, J. Yang, and A. Hauptmann. Automatically labeling
[5]   A. T. da Silva, A. X. Falcão, and L. P. Magalhães. A new CBIR              video data using multi-class active learning. In IEEE Intern.
      approach based on relevance feedback and optimum-path forest               Conference on Computer Vision (ICCV), volume 1, pages 516–
      classification. Journal of WSCG, pages 73–80, 2010.                        523, 2003.




WSCG 2012 Communication Proceedings                                    34                                                  http://www.wscg.eu
                Exposing Proprietary Virtual Reality Software to
                           Nontraditional Displays
            Maik Mory                            Steffen Masik                          Richard Müller                 Veit Köppen
    Otto-von-Guericke-University         Fraunhofer IFF             University of Leipzig     Otto-von-Guericke-University
        maik.mory@ovgu.de        steffen.masik@iff.fraunhofer.de rmueller@wifa.uni-leipzig.de    veit.koeppen@ovgu.de


                                                                           Abstract
Nontraditional displays just started their triumph. In contrast to traditional displays, which are plane and rectangular, they do not
only differ in design and architecture; they also implicate different semantics and pragmatics in the rendering pipeline. We strive
for a generic solution that couples legacy applications with nontraditional displays. In this paper, we present an architecture and
a respective experiment, which exposes a proprietary virtual reality software to a 360 degree virtual environment. Therefore
we introduce a rigorous master-slave design. The proposed architecture requires discussion of the following details: how to
access a proprietary application’s OpenGL stream; how to transmit the OpenGL stream efficiently in a clustered rendering setup;
how to process the OpenGL stream for adaption to nontraditional display semantics; and how to deal with the arising code
complexity, withal. Our design decisions are highly interdependent. The presented architecture overcomes limitations, which
were implied by client-server design in earlier work. The proposed rigorous master-slave design is totally transparent to the
client software, and reduces interdependencies between rendering software and rendering clusters. Thus, it inherently reduces
network round trips and promotes the use of scalable multicast. Our architecture is tested in a reproducible experiment, which
provides a qualitative proof of concept.
Keywords: Nontraditional Display, OpenGL, Distributed Rendering, Multicast, Interoperability, Generative Programming

1      INTRODUCTION                                                               of distributed virtual reality software. An architecture,
                                                                                  which enables node-based OpenGL stream processing
Nontraditional displays (e.g., CAVEs, powerwalls,                                 with little interference to the communication between a
domes) just started their triumph. In the 1990s, non-                             central OpenGL client software and its local OpenGL
traditional displays were driven by special monolithic                            server hardware, is presented in Sections 3 and 4. That
rendering hardware. Together with the advent of cheap                             our proposed architecture is feasible is proven with a
general and graphics computation power driven by                                  reproducible experiment in Sections 5 and 6. Finally,
the computer games industry, research shifted towards                             we conclude with an outlook on future work.
rendering clusters made from off-the-shelf hardware,
during the first decade of our century.                                           1.1    Application Scenario
   In contrast to traditional displays, which are plane and
rectangular, nontraditional displays do not only differ in                        Today, every engineering process is supported by soft-
design and architecture; they implicate different seman-                          ware. Any reasonable engineering software has a visual-
tics and pragmatics in the rendering pipeline, too. This                          ization component. Note, the visualization component
paper discusses several solutions that were implemented                           is not necessarily a dominant or permanent element of
to bring legacy software to nontraditional displays. Our                          the user interface; we only require it to be available. We
predecessors inherited client-server semantics from the                           choose Bitmanagement’s BSContact [8] as an exemplary
OpenGL specification. We present an implementation                                sample of generic information and geometry visualiza-
that rigorously uses master-slave semantics to enhance                            tion. Basically, BSContact is a generic 3D file viewer
scalability of distributed rendering architectures, beside                        for VRML and X3D. Most engineering software is able
other minor optimizations. Throughout our discussion,                             to export these data formats. Although, for us BSCon-
we use a practical application scenario that is presented                         tact’s primary functionality is not that relevant. For us it
in this section’s remainder together with a problem state-                        is important, that BSContact uses the most widespread
ment. Section 2 provides background on interoperability                           patterns of three dimensional data visualization; it is
                                                                                  proprietary; and it renders interactive geometry.
                                                                                     Several nontraditional displays exist in various dimen-
     Permission to make digital or hard copies of all or                          sions and shapes. Most of them have an entertainment
     part of this work for personal or classroom use is granted                   background, but some of them are also used for research
     without fee provided that copies are not made or distributed for             and industrial applications. One of those systems is the
     profit or commercial advantage and that copies bear this notice
     and the full citation on the first page. To copy otherwise, or               ElbeDom located at the Fraunhofer Institute for Fac-
     republish, to post on servers or to redistribute to lists, requires          tory Operation and Automation (IFF) in Magdeburg,
     prior specific permission and/or a fee.
                                                                                  Germany. The ElbeDom is a large cylindrical virtual
                                                                                  environment. It has been designed to satisfy the demand



WSCG 2012 Communication Proceedings                                          35                                          http://www.wscg.eu
for immersive virtual reality (VR) and massive multi-             The distinction is made, whether an instance is more the
user collaboration in the areas of virtual manufacturing          one or the other.
and factory planning. A detailed description of the sys-             Coupling components is the subject of interoperabil-
tem and a comparison of similar projection systems are            ity. Interoperability is a field of active research. The
provided in [22]. Basically, the ElbeDom is representa-           most exhaustive, recent survey we know of was done
tive hardware for distributed, tiled rendering on curved          by Manso et al. [17]. They declare seven levels of in-
screens.                                                          teroperability: technical, syntactic, semantic, pragmatic,
   Briefly, the ElbeDom’s cylindrical screen is 6.5 m high        dynamic, conceptual, and organizational. In our con-
and 16 m in diameter. The 330 m2 screen is covered by             text it is sufficient to stick with a three level hierarchy
six LDT G2 laser projectors. The projectors operate at            of interoperability [14], which we briefly introduce as
1600×1200 pixels. Thus, the viewer is surrounded by               follows:
approximately eleven megapixels from -20◦ below hori-
zon to +30◦ above horizon and full 360◦ in horizontal.            Syntactic interoperability is data exchange with a
Six so-called warping engines geometrically adjust and              common set of symbols to which a formal grammar
blend the six projector’s pictures. Their input is gener-           applies.
ated by a cluster of six commodity nodes. Throughout
this paper we call the six nodes slaves. Among other con-         Semantic interoperability is information exchange
trol and automation nodes, there is a dedicated headed              with a shared, common vocabulary for interpretation
node for the operator. We will call it the master in this           of the syntactic terms.
paper. The commodity cluster is interconnected via off-           Pragmatic interoperability is contextual exploitation
the-shelf GBit Ethernet.                                            of applications and services through shared knowl-
                                                                    edge.
1.2    Problem Statement
Our primary goal is to provide interoperability between           Another aspect about couplings are messaging patterns.
arbitrary proprietary virtual reality software and arbi-          Most procedural, object-oriented, and distributed sys-
trary nontraditional displays. Looking at this paper’s            tems follow the client-server pattern. A server compo-
application scenario alone, several questions arise. Bit-         nent provides an interface. A client component requires
management’s BSContact has been developed for tra-                an interface. If a client’s required interface and a server’s
ditional displays connected to local graphics hardware.           provided interface are compatible, they can be connected.
Thus, one has to consider aspects of syntactical, se-             Then, the client uses the server. Servers or services can
mantical and pragmatical interoperability between the             be stateful or stateless. If the server is stateless, the
visualization software and the nontraditional display’s           effect to a request depends on the request only. If the
rendering pipeline. Challenges, how to interface a pro-           server is stateful, the reply depends on the request and
prietary software’s visualization component and how               on the server’s state.
to handle variant implementations of several hundred                The Gang of Four [7] identified an extreme variant,
OpenGL functions frame our considerations.                        where the request declares the client’s interest in a series
                                                                  of replies – the observer pattern. Other names are one-
                                                                  way messaging, publish-subscribe, producer-consumer,
2     BACKGROUND                                                  or master-slave as we call it in this paper. Master-slave
Engineers describe system architectures in terms of com-          tends to be loosely coupled, because in an ideal imple-
ponents and interfaces. Depending on the engineer’s               mentation the consumer requires no knowledge about
domain and preferred method, what we call a compo-                identity or number of the producers and vice versa as
nent may be called an object, device, service, module, or         well, for example.
other too. We focus on the domain of computer science.              An interface definition covers a slice of the interoper-
Therefore, a component is a definable software artifact.          ability hierarchy. Most interface definitions in computer
Connections of components are differentiated between              science, especially application programming interfaces’
tight couplings and loose couplings. Tight coupling ex-           documentations focus on syntactic and semantic interop-
ploits interdependencies and relations between compo-             erability. Software developers usually delegate technical
nents; the connected components are not supposed to be            interoperability to electrical engineers, who design com-
exchanged. A loose coupling minimizes dependencies                puter chips and network links. The upper half of the
and relations between the components to a well-defined            interoperability hierarchy usually is in the responsibility
specification of the interface; loosely coupled compo-            of software project’s stakeholders.
nents tend to be exchangeable. In real life, couplings are          In a system of n components, one may implement
not clearly the one or the other. Rather, real couplings          O(n2 ) adapters for each coupled pair of components.
distribute in a continuum with ideal loose coupling on            When there is a common concept, which is shared
one end and with ideal tight coupling on the other end.           among several interfaces, established protocols and other



WSCG 2012 Communication Proceedings                          36                                           http://www.wscg.eu
                                                                     1                                                                     2
interface specifications are reused for multiple compo-                                           original function
nents. This we call an interoperability platform. In a
system of n components, one implements O(n) adapters                (a) In unmodified software, the client requests the server function (1);
                                                                    then the server function returns a reply (2).
between each component and the interoperability plat-
form.                                                                1   detouring
                                                                                                  original function
   There has been vast work to establish interoperabil-                    code

ity platforms for VR applications. For example, Schu-                                       4
                                                                                                trampoline                  5
mann [23] uses the high-level architecture (HLA) for                                 2
                                                                                                      3
syntactical interoperability among distributed simula-
                                                                                                                                       6
                                                                                                      instrumentation function
tions; Ošlejšek [21] tries to establish semantic interoper-
ability with a unified scene graph definition. The crucial          (b) In intercepted software, the detouring code redirects the instruction
point in the design of an interoperability platform is              pointer to an instrumentation function (2). We internally invoke the
the common concept shared between participants. In                  original function through the trampoline (3,4,5). After processing the
our observation, there are two types of interoperability            request-reply tuple, we return to the client (6).
platforms: those which declare and impose an artificial                                  Figure 1: Binary Interception
common concept; and those which find and exploit an
existing common concept. We believe that the latter have            function overwrites the first bytes of the server’s func-
better chances to succeed in software evolution. Looking            tion with machine code, which detours the execution
at the abundance of VR software, there is one thing ob-             path to the intermediary’s function. When the OpenGL
viously common: OpenGL. The OpenGL specification                    client invokes an intercepted OpenGL server’s function,
defines syntax through function signatures together with            the overlaid detouring code is executed instead of the
a finite state machine and it defines semantics through             original code. In effect, the client invokes the interme-
human readable documentation for modules and func-                  diary’s function. The installation procedure produces a
tions.                                                              so-called trampoline function, which keeps the original
   We are not the first ones who exploit the well sup-              server’s function available. The trampoline contains a
ported OpenGL industry standard for interoperability.               backup of the server function’s machine code that was
The commercial software products TechViz XL [2] and                 overwritten during installation of the detour and addi-
ICIDO’s Capture [1] impressively show the potential.                tional machine code that repatriates the execution path
However, because they are closed-source they give lit-              to the unmodified remainder of the server function’s
tle value to our discussion. During our discussion we               machine code. In effect, invocations of the trampoline
mostly refer to selected aspects of Chromium [11, 16],              delegate calls to the server.
Lumino [25], and BroadcastGL [13].                                     Microsoft Windows’ implementation of OpenGL,
                                                                    which is determined to be compatible with OpenGL
3   EXPOSING A PROPRIETARY APPLI-                                   version 1.1, provides 2400 functions (c.f., Section 6),
    CATION’S OPENGL STREAM                                          which divide into 357 core functions, 1671 extension
                                                                    functions, and 372 alias functions. Core functions are
In [18], the authors evaluate four techniques, how to               provided in the opengl32.dll’s symbol table. We in-
intercept a proprietary application’s invocations of the            stall interceptions for every core function during startup
OpenGL API. Three out of the four techniques have                   time before the application is able to access them. Ex-
been used in multi-hosted rendering before. The re-                 tension functions are provided on the client’s demand on
link library technique (e.g., MPIglut [15], although it is          the server through the wglGetProcAddress func-
not a node-based stream processor) cannot be applied                tion. We install interceptions for every extension func-
to proprietary software. The replace dynamic library                tion when it is passed through the wglGetProcAd-
technique (e.g., Chromium [11]) is unreliable within                dress function for the first time. What is known as
MS Windows’ dynamic library facility. The virtual de-               alias functions are alias names for core or extension func-
vice driver technique (e.g., VirtualBox [26]) does not              tions. In the data structure that tracks installed intercep-
scale for complex application scenarios. We prefer the              tions, alias functions are associated with their respective
binary interception technique because it is flexible and            original functions.
robust at once.                                                        In result, the complete OpenGL API is instrumented.
   The binary interception technique was introduced by              Our instrumentation functions first transparently dele-
Hunt and Brubacher [12] to instrument and extend pro-               gate the client’s request to the original function through
prietary software. An injected intermediary manipulates             the trampoline. After the original function returned
the proprietary software’s binary image at runtime. We              (item 5 in Figure 1b), the request-reply tuple is available
illustrate the principle in Figure 1. For each function that        to the instrumentation function from a totally transpar-
should be instrumented, the intermediary installs what              ent observation. In our further discussion, a published
is referred to as detour. The installation procedure for a          sequence of request-reply tuples we call the OpenGL



WSCG 2012 Communication Proceedings                            37                                                        http://www.wscg.eu
                                                                              Thus, the nontraditional display’s semantics and prag-
                                                                              matics are opaque to the client. As an example for
                                                                              opaque semantics, all of our predecessors tried to map
                                                                              the semantics of windowing and camera setup (e.g., the
                                                                              glViewport function) from their tiled display setups
                                                                              to the clients’ calls.
                                                                                 In 2005, Ilmonen et al. [13] unveiled the potential of
                                                                              non-unicast stream distribution in the context of multi-
                                                                              tile rendering. They discovered, that unicast stream
                                                                              distribution does not scale with the number of tiles, be-
(a) Traditionally, the client’s requests are fanned to the distributed        cause shared commands that are used by n tiles have
renderer and replies are merged after a full round trip. The display’s
semantics and pragmatics are opaque to the client.
                                                                              to be sent n times. The more tiles a distributed graph-
                                                                              ics application uses, the more neighboring and blended
                                                                              regions share geometry data. Global state changes are
                                                                              shared between all tiles. Ilmonen et al. describe an
                                                                              experiment, where they use broadcast via UDP/IP for
                                                                              stream distribution and a TCP/IP backchannel to add re-
                                                                              liability and congestion control. Their main contribution
                                                                              is an empirical proof, that broad- and multicast OpenGL
                                                                              stream distribution scales with the number of tiles in a
                                                                              centralized application with distributed graphics.
                                                                                 Lorenz et al. [16] implement a modification of
                                                                              Chromium, which uses multicast for parallelizing
(b) With our approach, the server master uses a local server to gather        commands and unicast for serializing commands.
replies with minimal latency. Then it publishes the requests with             In their reasoning, serializing commands are those
attached replies to the slaves without network round trips. The slaves        commands that are unique to each remote server.
adapt semantics and pragmatics transparently.
                                                                              Serializing commands are different with respect to the
Figure 2: A node-based OpenGL stream processor that                           distinct peers, because they are adapted on the client’s
multicasts request-reply tuples has looser couplings.                         side of the network in the server fan. We argue that they
                                                                              could be parallel calls – and thus be appropriate for
stream. After publication of the new OpenGL stream                            multicast distribution – if the adaption stage would be
element, we return to the original OpenGL client using                        shifted from the server fan to the remote peers.
the reply from the original server function. Thus, we                            Ilmonen et al. [13] already shift adaption of the re-
have extracted the OpenGL stream from the client-server                       quests to the remote peers. In their discussion, they
coupling with minimal interference.                                           point out that commands which require merged replies
                                                                              from the distributed server stall the streaming. Neal et
4    FULL MULTICAST SEMANTICS                                                 al. [20], who advance efficiency in multicast OpenGL
     FOR OPENGL STREAM DISTRIBU-                                              stream distribution by applying compression techniques
                                                                              to the distributed stream, observe the same problem.
     TION                                                                     They identify, that commands which have replies effec-
Commonly, implementations of node-based, distributed                          tively are network round trips and hence cause blocking
OpenGL processing use unicast via TCP/IP for data dis-                        at the client. This leads us to the question: Is it really
tribution. We guess that this design decision has two                         necessary to aggregate state from the distributed server?
major reasons. Back in the days of the first hype about                       Neal et al. as well as Ilmonen et al. still use client-server
rendering on commodity clusters (for a survey see Chen                        semantics as it is defined in the OpenGL API specifica-
et al. [5]), TCP/IP was available, tried-and-tested, and                      tion. As an aside, they borrow a potential solution from
well supported. Further, for those systems it is a basic                      prior work, that round trips may be avoided by state
assumption, that the invocation actually happens on the                       management [4].
remote site; and thus, return values and argument alter-                         Chromium [11] and Lumino [25], for example, imple-
ations have to be propagated back from the distributed                        ment state management. State management emulates
OpenGL server to the central client. Figure 2a illus-                         the OpenGL state machine as a component of the stream
trates this in terms of distributed systems. The OpenGL                       processing framework. Stavrakakis et al. [25] claim that
client’s requests are fanned to multiple OpenGL servers.                      state management is necessary for two reasons: late join-
For operations with output, the server fan adapts the                         ers should be able to retrieve OpenGL machine’s state;
client’s request for the remote servers and merges the                        and operation accumulation can be used for compres-
remote servers’ replies to a singular reply for the client.                   sion of transferred data (i.e., a-priori-aggregated state of



WSCG 2012 Communication Proceedings                                      38                                           http://www.wscg.eu
    the distributed server). However, state management is
    expensive. With emulation software, it is tedious to keep
    track with the original implementations in functionality
    and in performance. When Chromium introduced state
    management, they assumed that the client may run on
    a platform without graphics acceleration. Today, every
    host has basic graphics hardware acceleration or at least
    a good software implementation. Hence, we consider
    the topic of emulated state management obsolete. Even
    more, we explicitly recommend using the central appli-
    cation’s local OpenGL implementation.
       This yields a novel architecture for centralized ap-
    plications with distributed graphics, which is depicted
    in Figure 2b. Our implementation of the intercepted
    OpenGL API, which we name Vanadium1 , first invokes
    the original local OpenGL server with unmodified com-
    mands from the client and returns unmodified output to
    the client. This renders Vanadium transparent in func-
    tional behavior to the OpenGL client and in appearance
    to the user at the host with the central application, as
    well. After the trampoline function has returned and
    before the decorator function returns (cf., Figure 1), the
    decorator publishes the OpenGL stream. In contrast to
    earlier work, the stream does not only contain opcode
    and relevant input arguments (i.e., the request), but also
    every output, like return values and referenced arrays                 Figure 3: VDTC’s ElbeDom driven by Vanadium. The
    (i.e., the reply). After publishing the stream, any further            transparently distributed application is Bitmanagement’s
    stream processing is asynchronous. Thus, there are no                  BSContact with a software visualization scene [19].
    round trips on the network anymore.
       We agree to Stavrakakis et al. [25], that there should
    be a possibility to join lately to the stream. Nevertheless,           BSContact as source and VDTC’s ElbeDom as sink (cf.,
    late joining is a very infrequent event. Thus, we abne-                Section 1.1 and Figure 3).
    gate the implementation cost and runtime cost of dedi-                    Please note, that we intercept the whole OpenGL and
    cated state management. In the infrequent case of a late               WGL API. In the decorator functions we first delegate
    join, we are able to retrieve the OpenGL machine state                 the call to the original function via the trampoline. After
    directly from the original driver vendor’s implementa-                 the trampoline function returned with the output from
    tion through the glGet function and other inspection                   the original server, the processing described in this sec-
    functions; at least unless the client uses no deprecated               tion takes place. After our processing, the decorator
    technique like display lists. The joined slave uses its                function returns the values from the local original server
    adaption (see Section 5) to map the master’s late state to             invocation to the original client (cf., Figures 1 and 2).
    a consistent slave’s state. Then, the stream processing                   First of all, we need to handle recursion. As we use
    can continue. Regarding stream compression, which                      binary interception, we get each and every call of the
    is a topic to all distributed OpenGL renderers, because                OpenGL API – literally. So, there are the calls actu-
    the network bandwidth bottleneck is very dominant, we                  ally made by the client; and there are recursive calls by
    refer to Neal’s work [20] and Lorenz’ work [16].                       the original server implementation to itself. For exam-
                                                                           ple, the wglDescribePixelFormat function calls
    5    EXAMPLES FOR DETACHED                                             itself; many functions call the glFlush function in-
         PROCESSING OF THE OPENGL                                          ternally. We distinguish client calls and recursive server
         STREAM                                                            calls by tracking stack depth with a counter. When there
    In this section, we clarify the architecture shift that we             is no active call from the client, the counter rests at mi-
    propose in Section 4. Therefore, we describe the process-              nus one. During client’s requests the counter is zero.
    ing chain as we implemented it with Bitmanagement’s                    When the server calls itself, the counter is greater than
                                                                           zero. Recursive calls are skipped in stream processing,
                                                                           because they reflect internal behavior of the original
1   Vanadium as used in the presented experiment is provided in the        local server and thus do not matter. Now, the stream
    additional material. It will be open-sourced, soon.                    contains every invocation actually made by the client.



    WSCG 2012 Communication Proceedings                               39                                          http://www.wscg.eu
                                                                  changed (i.e., pointer exists in shadow copy mapping
                                                                  and client’s array is not equal to shadow copy array),
                                                                  the changed array is transmitted to the slave before
                                                                  usage. At the slave, the modified array replaces the
                                                                  respective array, because obviously the client discarded
                                                                  the old content before. When distributing the arrays and
                                                                  when referring to them in draw commands, the master
                                                                  uses handles that are derived from its shadow copy
                                                                  index. During cache management commands, the slave
                                                                  maintains a mapping between the master’s handles and
                                                                  the cache’s pointers. After cache synchronization the
                                                                  master emits the draw command. Then the slave uses its
Figure 4: Vertex Array Cache – The master maintains
                                                                  master’s-handle-to-slave’s-pointer mapping to invoke
a shadow copy of the client’s vertex arrays. The vertex
                                                                  the draw command with valid data. This simple caching
array transmission to the slave is differential.
                                                                  technique doubles the client application’s memory
                                                                  consumption with respect to its vertex arrays. We do
   Looking at the stream of client calls, there is a huge         not recommend the use of hashes, because collisions in
amount of calls that are irrelevant to the remote dis-            the index may corrupt the stream fatally [16]. For us,
play. Most commercial OpenGL software use an off-                 usage of the memcmp function worked out by reducing
screen rendering technique for auxiliary calculations,            network bandwidth at negligibly increased CPU load.
like mouse pointer ray collision testing or occlusion                For networking, we use ØMQ [10], which provides
culling. In Bitmanagement’s BSContact, a temporary                us with superior inter-thread, inter-process, cluster-wide,
viewport is used, that is overdrawn by visible content            and world-wide messaging. One command is one mes-
before the next SwapBuffers invocation. BSContact’s               sage. For each command in OpenGL’s API specification,
hidden viewport is easily determinable, because it is al-         we derive a struct, which contains the opcode, any value
ways square (e.g., 100×100 pixels) and smaller than the           arguments, and the return value if applicable. Array
window’s rendering area. We skip all calls that are made          arguments are packed into submessages. Because Ze-
to the finally invisible viewport. Then BSContact sets            roMQ takes care of message sizes robustly, therewith
the viewport to the whole visible area and renders the            we significantly reduce any risk associated with wrong
visible content. We passthrough these calls for handling          array sizes in C/C++. As an optimization, we exploit
on the nontraditional screen. As can be seen from the             that call-by-value arguments already are packed in the
invocation log, another viewport is set, that is always           stack. Thus, we only need to copy a slice from the stack
86 pixels in height. Because we could not imagine a map-          into the corresponding slice of the message buffer struct.
ping from its two-dimensional content to the 360 degree           ZeroMQ offers various reliable multicast protocols for
screen of the ElbeDom, we skip the two-dimensional                data distribution. After message transmission through
content, too. The considerations in this paragraph are            ZeroMQ to the slaves, the commands are dispatched to
highly interdependent with the client software and thus,          handler functions based on their opcode. The slave’s de-
have to be reconsidered for every new client.                     fault handler implementation directly mimics the client
   To reduce bandwidth usage further, we apply a                  node’s invocation. Some handler functions are modified
caching technique to the vertex array facility (Figure 4).        to implement adaption of the stream at slave side.
To achieve this, our decorated glDrawElements                        Comparable to the mapping of array pointers the
function and related vertex array draw functions                  slave implements a mapping of OpenGL names. In
determine the array in CPU’s RAM that should be                   OpenGL’s terminology, names are numeric identifiers
drawn by pointer and by size. The master keeps a                  for objects in the OpenGL state machine. For example,
shadow copy that resembles the arrays in the slave’s              the glGenTextures function outputs integers to the
cache. We introduce cache management commands                     client, which the client should use to identify and re-
in the stream to advice the slave for modifications               fer textures unambiguously. Usually, equally replayed
of its cache. Initially the master’s shadow copy and              commands should yield equal names. However, we can-
the slave’s cache are empty. When there is a new                  not guarantee that for heterogeneous environments, for
array (i.e., pointer is not in shadow copy mapping),              late joiners, and when splicing command streams. The
it is added to the shadow copy and transmitted to the             mapping mechanism is simple. There are functions that
slaves. When the client draws a known array (i.e.,                generate (i.e., output) names and there are functions that
pointer exists in shadow copy mapping) whose content              use (i.e., input) names. The slave maintains an asso-
is unchanged (i.e., client’s array equals shadow copy             ciative array with the master’s names as keys and the
array), the slave is told to use the cached array. When           slave’s names as values. Please remember, that the mas-
the client draws a known array whose content has                  ter includes invocations’ outputs in the stream. When



WSCG 2012 Communication Proceedings                          40                                          http://www.wscg.eu
                                                                  rameters. Thus, the lighting is consistent through all of
                                                                  the ElbeDom’s six tiles.

                                                                  6   HANDLING CODE COMPLEXITY
                                                                  The functionality that we describe in this paper handles
                                                                  several hundred functions from the OpenGL API. More-
                                                                  over, we talk about variant functionality. During analysis
                                                                  of the application’s OpenGL stream, we need a variant
                                                                  that logs the OpenGL stream to disk. The sender is a
Figure 5: If lighting is calculated relative to the camera        variant that implements serialization. The receivers have
and the camera has different poses on different tiles then        to implement deserialization. Processing of the OpenGL
the light’s pose is different on each tile (left). Repos-         streams yields building blocks (i.e., filters, adapters),
ing light sources with respect to the tiles’ camera pose          that ideally should be individually and independently
differences compensates the visual inconsistency (right).         reusable with a broad variety of virtual reality software
                                                                  and virtual environments. Nevertheless, we expect that
                                                                  processing nodes need to be tailored with respect to
a command that generates names is called, the slave               functional and non-functional behavior (cf., Siegmund
extracts the master’s names from the arrived message. It          et. al. [24]) as soon as more than one application will
replays the command, which yields the slave’s names.              be supported. At a first glance, this requires a codebase
Then the pair is added to the map. When a name is used,           of several thousand repetitive functions. At the second
the message refers the master’s name. The slave decodes           glance, we see two core problems: repetitiveness and
the master’s name to its local name using the map. We             variability.
call this mechanism name mapping.                                    Under the bottom line, we deal with a component
   The ElbeDom is an exemplary virtual environment.               oriented system, where the components stem from a
The cylindrical, surrounding screen has little potential          software system family. Such a family includes a num-
for two-dimensional WIMP semantics. Hence, we do                  ber of systems that are similar enough from an archi-
not even try to map the client’s WIMP semantics to the            tectural point of view to be assembled from a common
system’s VE semantics. The filtering stage at the master          set of components. We achieve development efficiency
yields a singular stream with visually relevant content           through Generative Programming (GP) [6]. The main
only. Therefore, calls to the glViewport function,                goal of GP is to generate a partial or an entire software
which refer to the whole visible window area at the               system automatically from such implementation com-
master, are mapped to fullscreen rendering at the slaves.         ponents. The requirements for the desired result, i.e.
This leaves us to adapt the camera pose. The appropriate          the generate, are defined in a domain specific language
place in the stream to achieve camera adaption is client-         (DSL). A DSL is a specialized and problem-oriented
software-specific. With Bitmanagement’s BSContact,                language for domain experts to specify concrete mem-
the most robust solution is to modify invocations of the          bers of a system family. This specification is processed
glLoadIdentity function and the glLoadMatrix                      by a generator, which automatically builds the system
function where the model view matrix is selected. There           by combining components according to configuration
we premultiply the tile’s camera orientation. This gives          knowledge. The Generative Domain Model (GDM) in
us the basic adaption to the ElbeDom’s 360 degree view.           Fig. 6 illustrates the concept of this paradigm estab-
Additionally, the ElbeDom’s warping and blending fa-              lishing a relationship between the basic terms of GP. It
cility specifies asymmetric frusta for each tile. At the          consists of the problem space, the solution space, and
glFrustum function we discard the client’s inputs for             the configuration knowledge mapping both spaces. The
a symmetric view into the window and overwrite with               problem space includes domain specific concepts and
the tile’s asymmetric frustum configuration. This com-            features to specify requirements by means of one or
pletes camera adaption to the ElbeDom’s interleaved               more DSL(s). The solution space offers elementary
frustum configuration.                                            and reusable implementation components correspond-
   The adaption pipeline is completed by a pragmatic              ing to the system family architecture. The configuration
adaption of lighting. Like most other OpenGL software,            knowledge comprises illegal feature combinations, de-
BSContact uses lighting to give three-dimensional im-             fault settings, construction rules, and optimizations as
pression. BSContact’s lighting is designed relative to            well as related information. In order to instantiate this
the camera. Because we rotate the world to adapt the              theoretical concept, we perform a technology projec-
camera position, the lighting is inconsistent between             tion. Therefore, we identify concrete techniques for the
the tiles (cf., Figure 5). Hence, we install a filter on          elements of the GDM.
the glLight function family, which applies the tile’s                In particular, we enhanced Microsoft Visual C++ at
camera pose to the position and direction lighting pa-            Visual Studio 2010’s prebuild stage. First, our Python



WSCG 2012 Communication Proceedings                          41                                         http://www.wscg.eu
    Problem Space               Configuration           Solution Space              culling in legacy software. These techniques are highly
                                 Knowledge
                                                                                    optimized towards traditional displays. With BSContact,
                                                                                    we could switch them off through its ActiveX interface.
                              - Illegal feature
                                                                                    If culling was not disengageable, further investigation
                                combinations
     - Domain specific
                              - Defaults settings
                                                       - Elementary and             would be necessary. In the long term we are curious, if
       concepts and                                      reusable components
                              - Default dependencies
     - Features
                              - Construction rules
                                                                                    fully programmable pipelines may yield new rendering
                              - Optimizations                                       pipeline semantics, will our approach scale?
                                                                                       With our proposed rigorous master-slave design, the
                                                                                    nodes of an OpenGL stream processor are coupled more
                                                                                    loosely than in competing frameworks. Therefore, our
Domain Specific Language(s)      Generator(s)          Components & System
         (DSLs)                                         Family Architecture         architecture supports development and recombination
           Figure 6: Generative Domain Model [6]                                    of OpenGL stream processor nodes. For example, we
                                                                                    would be glad to see an effect processing engine (e.g.,
                                                                                    Haringer and Beckaus [9], or Brennecke et al. [3]) ap-
script parses the OpenGL API’s formal specification.
                                                                                    plied to our OpenGL stream processing approach. In
At the time of our experiment, the OpenGL specifica-
                                                                                    summary, we see great potential for innovative use cases
tion (revision 12819) defines 2269 functions. Microsoft
                                                                                    in entertainment and engineering.
Windows’ OpenGL windowing system (WGL) (revi-
sion 10796) adds 131 function definitions. Thus, an
OpenGL application on Windows has access to a repos-
                                                                                    ACKNOWLEDGEMENTS
itory of 2400 functions. Secondly, within our Python-                               We are thankful to Fraunhofer IFF’s Virtual Develop-
based domain specific language, a feature configuration                             ment and Training Center (VDTC), which provided its
is modeled. Both models together are applied to a tem-                              ElbeDom for our experiments.
plate engine, which generates C++ source code. Then,
the prebuild step, or code generation step respectively,                            REFERENCES
terminates and the Visual C++ tool chain builds the ex-
                                                                                    [1] IC:IDO – The Visual Decision Company.
ecutable software artifacts. Applying the means of the
                                                                                        http://www.icido.de/PDF/ICIDO_
generative paradigm, repetitive development tasks are
                                                                                        Broschuere_A4.pdf, Stuttgart, November
automated and the high variability of the virtual reality
                                                                                        2006. In German.
domain is made manageable.
                                                                                    [2] Emanuela Boutin-Boila. TechViz XL, March 2010.
7     CONCLUSION AND FUTURE                                                         [3] Angela Brennecke, Christian Panzer, and Stefan
                                                                                        Schlechtweg. vSLRcam – Taking Pictures in Vir-
      WORK                                                                              tual Environments. In WSCG (Journal Papers),
We presented an architecture for node-based OpenGL                                      pages 9–16, 2008.
stream processing. It is highly interoperable with pro-                             [4] Ian Buck, Greg Humphreys, and Pat Hanra-
prietary and legacy software, because we use a robust                                   han. Tracking graphics state for networked
technique for function interception, and especially be-                                 rendering. In Proceedings of the ACM SIG-
cause we adapt from OpenGL’s client-server pattern to a                                 GRAPH/EUROGRAPHICS workshop on Graphics
master-slave pattern, which is more feasible for stream                                 hardware, HWWS ’00, pages 87–95, New York,
processing. The adaption is as transparent as possible to                               NY, USA, 2000. ACM.
the client. Thereby, the proposed architecture removes                              [5] Yuqun Chen, Han Chen, Douglas W. Clark, Zhiyan
any interdependencies between rendering software and                                    Liu, Grant Wallace, and Kai Li. Software Envi-
rendering clusters. We show that there is no source code                                ronments For Cluster-Based Display Systems. In
access to the application required. The reduced inter-                                  Proceedings of the 1st International Symposium
dependency promotes to use scalable multicast, and re-                                  on Cluster Computing and the Grid, CCGRID
moves network roundtrips. The architecture is tested by                                 ’01, pages 202–210, Washington, DC, USA, 2001.
a reproducible experiment, which we comprehensively                                     IEEE Computer Society.
described in Section 5.
   For sake of clarity, we focused on the combination of                            [6] Krzysztof Czarnecki and Ulrich W. Eisenecker.
one virtual reality software with one nontraditional dis-                               Generative Programming Methods, Tools and Ap-
                                                                                        plications. Addison-Wesley, 2000.
play. We want to generalize our approach of course. At
the time of this writing, we are experimenting with other                           [7] Erich Gamma, Richard Helm, Ralph Johnson, and
rendering software, and we are experimenting with other                                 John Vlissides. Design Patterns. Addison Wesley,
nontraditional displays, which have different pragmatics                                Reading, MA, 1995.
than the ElbeDom. One open question we are research-                                [8] Bitmanagement Software GmbH. BS Contact 8.0.
ing is how to deal with frustum culling and occlusion                                   http://www.bitmanagement.de.



WSCG 2012 Communication Proceedings                                            42                                        http://www.wscg.eu
[9] Matthias Haringer and Steffi Beckhaus. Dynamic                    Kuhlen, Renato Pajarola, and Kun Zhou, editors,
     Visual Effects for Virtual Environments. In WSCG                 EGPGV, pages 21–29. Eurographics Association,
     (Full Papers), pages 49–56, 2010.                                2011.
[10] Pieter Hintjens. ØMQ – The Guide. iMatix Corpo-           [21]   Radek Ošlejšek. Virtual Scene as a Software Com-
     ration, http://zguide.zeromq.org/.                               ponent. In WSCG (Posters), pages 33–36, 2008.
[11] Greg Humphreys, Mike Houston, Ren Ng, Randall             [22]   Wolfram Schoor, Steffen Masik, Marc Hofmann,
     Frank, Sean Ahern, Peter D. Kirchner, and James T.               Rüdiger Mecke, and Gerhard Müller. ElbeDom:
     Klosowski. Chromium: A Stream-Processing                         360 Degree Full Immersive Laser Projection Sys-
     Framework for Interactive Rendering on Clusters.                 tem. In Virtual Environments 2007 - IPTEGVE
     In ACM SIGGRAPH ASIA 2008 courses, SIG-                          2007 - Short Papers and Posters, pages 15–20,
     GRAPH Asia ’08, pages 43:1–43:10, New York,                      2007.
     NY, USA, 2008. ACM.                                       [23]   Marco Schumann. Architektur und Applikation
[12] Galen Hunt and Doug Brubacher. Detours: Binary                   verteilter, VR-basierter Trainingssysteme. Disser-
     Interception of Win32 Functions. In Proceedings                  tation, Otto-von-Guericke-University Magdeburg,
     of the 3rd conference on USENIX Windows NT                       Magdeburg, November 2009. In German.
     Symposium - Volume 3, WINSYM ’99, page 14,                [24]   Norbert Siegmund, Martin Kuhlemann, Sven Apel,
     Berkeley, CA, USA, 1999. USENIX Association.                     and Mario Pukall. Optimizing Non-functional
[13] Tommi Ilmonen, Markku Reunanen, and Petteri                      Properties of Software Product Lines by means
     Kontio. Broadcast GL: An Alternative Method                      of Refactorings. In Proceedings of Workshop Vari-
     for Distributing OpenGL API Calls to Multiple                    ability Modelling of Software-intensive Systems
     Rendering Slaves. In WSCG (Journal Papers),                      (VaMoS), pages 115–122, 2010.
     pages 65–72, 2005.                                        [25]   John Stavrakakis, Masahiro Takatsuka, Zhen-Jock
[14] Veit Köppen and Gunter Saake. Einsatz von                        Lau, and Nick Lowe. Exposing Application Graph-
     Virtueller Realität im Prozessmanagement. Indus-                 ics to a Dynamic Heterogeneous Network. In Pro-
     trie Management, 2:49–53, 2010. In German.                       ceedings of the 14th International Conference in
[15] Orion Sky Lawlor, Matthew Page, and Jon Genetti.                 Central Europe on Computer Graphics, Visualiza-
     MPIglut: Powerwall Programming Made Easier.                      tion and Computer Vision, WSCG ’2006, pages
     In WSCG (Journal Papers), pages 137–144, 2008.                   71–78, 2006.
[16] Mario Lorenz, Guido Brunnett, and Marcel                  [26]   VirtualBox Service Desk. #475 – 3D acceler-
     Heinz. Driving Tiled Displays with an Extended                   ation support for VBox guests. http://www.
     Chromium System Based on Stream Cached                           virtualbox.org/ticket/475 Accessed
     Multicast Communication. Parallel Computing,                     2011-10-10.
     33(6):438 – 466, 2007. Parallel Graphics and Vi-
     sualization.
[17] Miguel-Ángel Manso, Monica Wachowicz, and
     Miguel-Ángel Bernabé. Towards an Integrated
     Model of Interoperability for Spatial Data Infras-
     tructures. Transactions in GIS, 13(1):43–67, 2009.
[18] Maik Mory, Mario Pukall, Veit Köppen, and
     Gunter Saake. Evaluation of Techniques for
     the Instrumentation and Extension of Propri-
     etary OpenGL Applications. In 2nd Interna-
     tional ACM/GI Workshop on Digital Engineer-
     ing (IWDE), pages 50–57, Magdeburg, Germany,
     2011.
[19] Richard Müller, Pascal Kovacs, Jan Schilbach, and
     Ulrich Eisenecker. Generative Software Visual-
     izaion: Automatic Generation of User-Specific Vi-
     sualisations. In 2nd International ACM/GI Work-
     shop on Digital Engineering (IWDE), pages 45–49,
     Magdeburg, Germany, 2011.
[20] Braden Neal, Paul Hunkin, and Antony McGre-
     gor. Distributed OpenGL Rendering in Network
     Bandwidth Constrained Environments. In Torsten



WSCG 2012 Communication Proceedings                       43                                         http://www.wscg.eu
WSCG 2012 Communication Proceedings   44   http://www.wscg.eu
                           Image Abstraction with
                      Cartoonlike Shade Representation
  GyeongRok Lee*                     HoChang Lee*                         TaeMin Lee*                  KyungHyun Yoon**
                                                   Chung-Ang University
                                                  Department for Computer
                                                  Science and Engineering
                                                   Korea (156-756), Seoul
                                     starz | fanpanic | kevinlee @ cglab.cau.ac.kr*
                                                   khyoon @ cau.ac.kr **


                                                      ABSTRACT
Luminance quantization, which maps the luminance values of an image to discrete levels, is widely used for
image abstraction and the expression of a cartoonlike effect. Existing luminance quantization techniques use
each pixel’s luminance value separately, leading to a noisy image. Additionally, they do not take the shape of the
imaged object into consideration. Thus, they suffer limitations in terms of cartoonlike shade representation.
We propose a new luminance quantization algorithm that takes into account the shape of the image. We extract
the silhouette from the image, compute edge-distance values, and incorporate this information intothe process of
luminance quantization. We integrate the luminance values of neighboring pixels using an anisotropic filter,
using gradient information for this filtering. As a result, ourquantized image is superior to that given by existing
techniques.
Keywords
Image Abstraction, Image cartoonize, cartoonlike shade generation, luminance quantization
1. INTRODUCTION                                                     they use the luminance values of individual pixels,so
The purpose of image abstraction is to simplify color               they are limited in representing the shade that reflects
and shape. In particular, image cartooning makes                    the shape of the object, especially since real-world
images look like a cartoon. Abstracted imagesmake it                objects are influenced by complex lighting. So,
easier to recognize a scene,because their color and                 conventional approaches can’t represent neat shade
shape information is very simple. For example                       like a cartoon.
cartoon is easier to recognize than real scene, so it is            We propose an improved luminance quantization
often used for children’s contents. Because of this                 algorithm and image cartooning process. Our
advantage, abstracted images are widely used in                     algorithm takes into account the shape of the
books and movies. Cartoon images have the                           depicted object. We extract the silhouette of the
following features:                                                 object and compute an edge-distance map, and then
1. Simple color and shade (not noisy)                               compute the gradient values of the pixels to create an
                                                                    anisotropic filter. Then we integrate neighboring
2. Silhouette-like object shade                                     luminance valuesusing this filter. Our algorithm thus
To express these features, we use luminance                         results in an image that is less noisy and has cartoon-
quantization, a simple and fast non-photorealistic                  like shade.
rendering (NPR) technique. Conventional approaches                  Our main contributions are the following:we
change an image’s luminance values to certain                       represent cartoon-like shade by considering the shape
discretelevels to give simplified images. However,                  information of the object, and we improve the quality
                                                                    of the luminance quantization image by integrating
 Permission to make digital or hard copies of all or part of        neighboring luminance values using anisotropic
 this work for personal or classroom use is granted without         filtering.
 fee provided that copies are not made or distributed for
 profit or commercial advantage and that copies bear this           2. RELATEDWORK
 notice and the full citation on the first page. To copy            There are several NPR approaches to abstract the
 otherwise, or republish, to post on servers or to                  color in a 2D image. The most conventional approach
 redistribute to lists, requires prior specific permission          is image segmentation and express with average
 and/or a fee.




WSCG 2012 Communication Proceedings                            45                                        http://www.wscg.eu
color of region, which can simplifies image color but           Shape information is very important featurefor image
cannot preserve the original shape and detail.                  abstraction. Many researchers have studied it: Kang
                                                                et al. [Kan07a] proposed the edge tangent flow (ETF)
More recently, filtering-based methods for image
                                                                technique that applies gradients to express a coherent
abstraction have gained popularity. The filtering
                                                                line eﬀect. Their method was extended to [Kan08a],
approach is simple and very fast, so it is used widely.
                                                                which simplifies the shape and color of the image.
Filters include the blurring filter, the Kuwahara filter
                                                                Hertzmann [Her98a] and Hays et al. [Hay04a]
[Kuw76a], and [Pap07a] that is a generalization of
                                                                determined the direction of the stroke by considering
various Kuwahara filters. Newer approaches include
                                                                the form of the image to express the painting eﬀect.
the bilateral filter [Tom98a] and the anisotropic
                                                                Hochang et al. [Lee10a] utilized extra information on
Kuwahara filter. Thebilateral filter takes into account
                                                                the image gradient from texture transfer and
color and edge information, so it preserves the
                                                                suggested a texture transfer technique with the
original shape. It is also very fast; Winnemoller et al.
                                                                direction. In their research on mosaics [Hau01a],
[Win06a] use this filter for video abstraction. The
                                                                Hausner used edge information to follow the
anisotropic Kuwahara filter expresses a painting
                                                                direction ofthe edge to determine the direction of the
effect by using an anisotropic filter within an existing
                                                                tile. Although there have been many 2D-based
Kuwahara filter. The above approaches express a
                                                                studies that consider the shape of images, the form of
cartoon-like effect through color simplification, but
                                                                the object has not been taken into account for the
cannot express cartoon-like shad. For this, we use
                                                                study of images’ shade eﬀects.
luminance quantization.
Quantization is a technique that is widely used with            3. SHAPE-DEPENDENT
video, image, and audio. The original quantization              LUMINANCEQUANTIZATION
[Cho89a, Zha95a] is a compression technique that is             Fig.2 shows our system flow. We use a 2D image as
pursuing the save input information and reduce                  an input and apply Mean Curvature Flow(MCF)
storage space. Apply this, Image quantization is used           [Kan08a]. This gives an image with simplified color
to reduce the cost on compressionand transmission               and shape. After that, we perform two steps. Firstly,
costs by reducing the color information of the                  we analyze the shape of the object. To do this, the
image.And quantization technique was used to                    silhouette is extracted and the distance of each pixel
representthe shading eﬀect in NPR, which is                     from the silhouette is calculated. In this step, flow
luminance quantization. Conventional luminance                  information is extracted too.This step performed only
quantization techniques individually change the                 once. After that, we integrate the luminance values of
luminance value of each pixel to a representative               neighboring pixels using flow information, perform
value. This leads to noise in the result image due to           luminance quantization and modify luminance values
rapid stylization of luminance at the boundary (Fig.1-          using silhouette distance values. This step performed
a). Therefore, in [Win06a] the tangent function is              iteratively for all pixels of image. Finally, we apply
used to represent a smooth boundary and a natural               MCF to shade shape clear up. Through these steps,
transition between each step, solving the problem of            we generate an image that expresses a cartoon-like
sharp differences between the steps (Fig.1-b). This             shading eﬀect.
technique is simple and presentsthe shading eﬀect
well, so it has been widely used to express cartoon
                                                                3.1 Shape analysis
                                                                In order to express shade that reflectsshape of object,
eﬀects. However, the study in [Win06a] considers
                                                                the shape information must be analyzed. Edge
only the luminance information of individual pixels
                                                                information is commonly used to reflect the shape of
too , and thus does not reflect the morphological
                                                                image. General edge detection techniques for image
information of the image.
                                                                processing would extract too much information, so it
                                                                is not suitable to extract result image which reflect
                                                                shape like cartoons. Therefore in this study, we used
                                                                the silhouette of the image and selected significant
                                                                edge as shape information.
                                                                It is a simple matter to get the silhouette line of an
                                                                object in a 3D environment since each object is saved
                                                                in a separate space. Because of this, it is easy for
                                                                extract boundary. In a 2D image, it is difficult to
             (a)                          (b)                   extract the silhouette of the foreground object since
  Figure 1. (a) General luminance quantization                  there is no depth information. (so, this is not perfect)
     (b) [Win06a]’s luminance quantization                      We use mean-shift segmentation (result shown in Fig.
                                                                3-b) followed by user selection of the foreground




WSCG 2012 Communication Proceedings                        46                                       http://www.wscg.eu
                                                 Figure 2. System Flow

object (Fig.3-c), which is then displayed as a white           Use this, we apply Gaussian smoothing to image
object on a black background                                   firstly. after that, we extract edge and use this as a
                                                               significant edge. Using a method used to calculate
Performing an edge detection procedure gives the
                                                               light direction in photography and videography, we
silhouette line (Fig.3-d). We then apply the edge-
                                                               use the highlight points (as seen in Fig.4)to calculate
distance transform, an algorithm that extracts the
                                                               the direction of the light. This allows us to generate a
distance between a pixel and the edge closest to it, on
                                                               virtual light source. After that, we extract the inner
the pixels of the foreground object. This leads to an
                                                               edge. Because the edge image is very complex when
edge-distance map (Fig.3-e). The value of each pixel
                                                               we use normal edge detection, we use a smoothed
of the edge-distance map has been normalized to lie
                                                               image as input of edge detection algorithm. Then we
between 0 and 1.Thisedge-distance is used as an
                                                               rule some edges which near to silhouette out inner
additional feature for luminance quantization, which
                                                               edge.
will be covered later in this paper.




                                                                Figure 4. light direction in the photography and
 (a)                (b)                                                            videography




       (c)                (d)              (e)
 Figure 3. (a)original image (b) segmented image
      (c) extracted object (d) silhouette line                       (a)               (b)                (c)
                  (e) distance map                               Figure 5. (a) light map (b) inner edge distance
In the cartoon, there are not only silhouette-like                  map (c) concept of inner edge distance
shades but also other shades, for example creases of
cloth and important curves. To express these shades,
we calculate the direction of light and extract the            We can get two important features from the light
significant inner edge. we suppose that strong edge is         point and inner edge. One is the light-distance map
significant edge. Strong edges have strong gradient            and the other is the inner edge-distance map. The
value, so it can be remained after apply smoothing.            light-distance map (Figure 5-a) is a map of the



WSCG 2012 Communication Proceedings                       47                                        http://www.wscg.eu
normalized distance between each object pixel and              the structure tensor using x,y direction information.
the light point. We use the light map to prevent omni-         Using this matrix, we deform the circle filter to an
directional shade generation, in other words, it               anisotropic form. This filter reflects the flow of
enables us to generate shade on only one side of the           pixels. (Fig.7) is the ETF field obtained throughline
object.                                                        integral convolution (LIC) [Cab93a], showing the
                                                               orientation of each pixel in the image. In Fig.8 we
                                                               can see how the anisotropic filter is applied. The
                                                               luminance of the purple region is used as P in
                                                               equation (1) below.




       (a)               (b)                (c)
Figure 6. (a) edge image (b) normal edge distance
map (c) example of edge movement


e calculatethe inner edge-distance map using a
modified edge-distance transform. As we see in
(Fig.6–a) and (Fig6-b), a normal distance transform
gives distance values on both sides of an edge.
Further, these values are spread over large area. This                       (a)                   (b)
would lead to an unnatural shade. Hence, we modify
the edge-distance transform to generate a narrow                   Figure 7. (a) Original image    (b)ETF field
distance value, and we use a moved edge image
made by moving the edge to the opposite side of the
light (Fig.6–c). This gives the inner edge distance
map. We see this in (Fig.5–b) and (Fig.5-c)
3.2 Luminance Modification and Quantization
In this step, we apply two processes to all the pixels
of image iteratively. The first is the use of an
anisotropic filter to merge the luminance of
neighboring pixels.This filtering applied anisotropic
filter to pixel which have strong gradient. For find                      (a)                       (b)
gradient, we use ETF field. On the contrast, if pixel
do not have strong gradient, filter will be isotropic                   Figure 8. (a) Anisotropic filtering
like (Fig.7) and (Fig.8)The second is the modification                        (b) Isotropic filtering
of luminance values using the silhouette distance
map, the inner edge distance map and the light map.
Finally, we apply quantization.                                We use quantization to express the shading eﬀect on
                                                               the smoothened input image. In order to express a
If we were to perform luminance quantization on the            shade thatfollows the borderline, we define fallowing
original input image, the result would be noisy                rule.
because luminance values are not distributed evenly
across an image. We need a coherent luminance                  - Darken the luminance as the silhouette gets closer,
distribution, for which we use a kernel that integrates        lighten it as the silhouette gets further.
the luminance values of neighboring pixels. We                 If the edge-distance value of the current pixel (P) is
perform anisotropic filtering which takes into
account image flow for the removal of image noise.
                                                                Pdis and its luminance value is Plum we calculate
                                                               ~
To facilitate image flow information, we use                   P lum as follows:
[Kan07a]’s method to utilize ETF (Fig.7-b). ETF,
which interpolates gradient values, can extract more           ~
accurate and coherent pixel directions than other,             P lum  Plum * (1  W )                            (1)
more general methods that use Sobel or Laplacian
filters. From the extracted ETF values, we calculate           W  log10 (k / WIL ) / 2 * LW                      (2)



WSCG 2012 Communication Proceedings                       48                                      http://www.wscg.eu
     (( Pdis *100)  1)                                        be seen in Fig.13.
k                                                  (3)
     (( EF *100)  1)                                                                ~
                                                               Finally, the obtained P lum value is used as the input
                                                               to the quantization process. We use the method used
                                                               by Winnemoller et al. (Fig.1-(b)). Through this
In order to calculate the luminance value that reflects        process, we get shade which is influenced by the
the current luminance information and the                      shape. Finally, we apply MCF again for the
information on the distance to the silhouette, we
                                                               arrangement of the shade.
applied weighting based on the log function. Formula
(1) shows the process of obtaining the weights W.              4. RESULTS
We have used the effect control factor (EF) to adjust          (Fig.9-a) is the input image, (Fig.9-b) is the result of
the value for log function. EF is the control variable         conventional luminance quantization. (Fig.9-c) is the
that can adjust the distance value from the silhouette         result of our anisotropic filtering and quantization.
to where the shading eﬀect is applied. The value of            (Fig.9-c) maintains coherence among neighboring
EF is between 0 and 1.As EF approaches 1, the value            pixels and we can see the shade, it is scattered.
of K becomes somewhere between 0-1.Because Pdis                (Fig.9-d) is the result of applying the MCF to (Fig.9-
is normalized into 0 - 1. Hence, the weighting value           c). in this image we can see much better shade,
w is always negative and the final result is darker.           although it is blurred. In (Fig.9-e), we enhance the
Conversely, if the value of EF equals 0, the value of          edges. In the (Fig.9-f) we clearly see the shade on the
                                                               object and we can check that shade arrangement by
K is between 1 and100 and the weighting gets a
                                                               MCF.
positive value.0.25 is generally used as the value of
EF. We divide this value k by WIL , the inner edge-            (Fig.10) shows the result for different values of EF.
distance value. Through this we can generate shade             A larger EF value results in an image with a larger
on the inner edge region. As the value range of log(K)         shaded area.
is -2 to 2, we use log(K) divided by 2. We multiple
that value by light weightLW . We can get this value
from the light map. The effect of this parameter can




                                                                   (a) 0.05           (b) 0.25           (c) 0.75
                                                                         Figure 10. Results by EF value

       (a)                (b)               (c)
                                                                  We compare our results with conventional
                                                               methods in (Fig.11).(Fig.11-a) is input image,
                                                               (Fig.11-b) uses conventional luminance quantization.
                                                               (Fig.11-c), which is the result of applying the
                                                               quantization described by (Fig.1-b), shows superior
                                                               definition in the shaded area compared to (Fig.11-b).
                                                               (Fig.11-d), which is our result, better represents
                                                               object shape than conventional approaches. (Fig.11-e)
                                                               is the result of the procedure described in [Win06a].
       (d)                (e)               (f)                (Fig.11-f) is the result of using the anisotropic
                                                               Kuwahara filter. Compared with these, our result
               Figure 9. Our results                           exhibits shade which reflects the object’s shape.
      (a)Originalimage (b)Conventional QT                      From (Fig.11-g) to (Fig.11-j) are other comparisons.
     (c)Our method (not MCF) (d)(c) + MCF
       (e) (d)+Line (f) detail from (c) and (d)




WSCG 2012 Communication Proceedings                       49                                        http://www.wscg.eu
                                                              the light is at the top right; in (Fig.13-d), it is at the
                                                              top left. In (Fig.13-e), the light is focused at the
                                                              center of the object. Thus, we can control the shade
                                                              effect through this parameter. We use this by making
                                                              distance map. If user select light spot, system
                                                              calculate light-distance map. This distance value is
   (a) Input        (b) Figure1-a (c) Figure 1-b              LW value. And if we don't use LW, we will get result




(d) Our result       (e) [Win06a]     (f) [Kyp11a]                             (a)              (b)




                                                                     (c)                (d)                 (e)
        (g) Input              (h) Our result                          Figure 13. Results by LW value

                                                              image like (Fig.13-e). That has omni-directional
                                                              shade. And (Fig.15) is our additional results.
                                                              5. Conclusion and Future Work
                                                              In this paper, we proposed an image abstraction
                                                              technique that is based onluminance quantization and
                                                              takes into account the shape information of an image.
      (i) [Win06a]               (j) [Kyp11a]                 We extracted the silhouette of the input image and
  Figure 11. compare results with conventional                utilized each pixel’sedge-distance from the silhouette
                  researches                                  as an additional feature. We were able to remove the
                                                              noise while preserving the shape through anisotropic
                                                              filtering. Through our algorithm, we could expressthe
In (Fig.12), where the luminance is uniform, the              shading eﬀects reflecting the shape of the image
shading is not expressed under conventional                   better than previous approaches.
quantization. However, our method can generate                The proposed algorithm has two advantages in terms
shade.                                                        ofthe shading eﬀect compared to conventional
                                                              luminance quantization. Firstly, the output we have
                                                              producedmaintainsbetterconsistency              among
                                                              neighborhood pixels. This is because of anisotropic
                                                              filtering, which considers the flow of the image in
                                                              contrast to simple isotropic filtering. Thus, the noise
                                                              was reduced compared to previous studies. The
                                                              algorithm should be applicable to video content as
                                                              well. Secondly, the shadow that tags along the shape
   (a) Input     (b) Conventional (c) Our result              of the image can be generated. Therefore, the shadow
                                                              eﬀect is more cartoonlike than in previous studies. In
    Figure 12. result of same-luminance image                 addition, through a simple parameter, the degree of
                                                              shading can be adjusted and this can easily express
(Fig.13) shows the results of varying the value of LW.        variety of eﬀects.
(Fig.13-a) is input image. (Fig.13-b) is result of            However, our algorithm has several limitations.
conventional luminance quantization. In (Fig.13-c),           Firstly, it is sensitive to the background of the image.




WSCG 2012 Communication Proceedings                      50                                           http://www.wscg.eu
If the background is complicated, the result of the             Acknowledgments
segmentation step is poor and object extraction                 This work was supported by a Korean Science and
becomes difficult.In (Fig14-a), we can see this                 Engineering Foundation(KOSEF) grant funded by
limitation. To overcome this, we could use superior             the Korean government(MEST) (No.20110018616).
segmentation techniques (e.g. matting-based                     And this work was also supported by a Seoul R&BD
techniques). Secondly, our algorithm has problem                Program(PA110079M093171).
when applied to very bright or dark image. Although
we use light-map, we can't generate shade. (Fig14-b)
Because dark image is very insensitive to luminance
change.on the contrary, bright image is very sensitive
to luminance change. So, it generates omni-
directional shade. Thirdly, the execution time of our
algorithm is very sensitive to the size of the
anisotropic kernel. However, our algorithm is
computationally parallelizable because it works on
individual pixels;thus we can overcome this
limitation by using a graphics processing unit (GPU).
Finally, our algorithm needs some user interactions.
If there are automatic object extract techniques, it can
be improved




                Figure 14. limitation
  (a) left-original image, right-segmented image
                                                                   Figure 15. Additional results(arrow : light)
     (b) upper-light map, lower-result image




WSCG 2012 Communication Proceedings                        51                                    http://www.wscg.eu
6. REFERENCES                                               [Kyp09a] J.E. Kyprianidis, H. Kang, J.Döllner :
                                                               Image and Video Abstraction by Anisotropic
[Bra97a] J.P Braquelaire, L.Brun : Comparison and              Kuwahara Filtering, Computer Graphics Forum,
   Optimization of Methods of Color Image                      Vol.28, No.7, 2009.
   Quantization, IEEE transactions on image
   processing, Vol.6, No.7, pp.1048-1052, 1997.             [Kyp11a] J.E. Kyprianidis, H. Kang : Image and
                                                               Video Abstraction by Coherence-Enhancing
[Cab93a] B. Cabral, L.C. Leedom :Imageing Vector               Filtering, Computer Graphics Forum, Vol.32,
   Field using Line Integral Convolution,                      No.8, pp.593-602, 2011.
   SIGGRAPH conference proceedings, pp.263-270,
   1993.                                                    [Lee10a] H. Lee, S.H. Seo, S.T. Ryoo, K.H. Yoon :
                                                               Directional Texture Transfer. Proceedings of the
[Cel09a] M.E. Celebi : Fast color quantization using           8th International Symposium on Non-
   weightedsort-means clustering. JOSA A, Vol. 26,             Photorealistic Animation and Rendering (NPAR
   No.11, pp.2434-244, 2009.                                   2010), pp.43-48, 2010.
[Cha94a] N. Chaaklha, W.C. Tan, Teresa H.Y.Meng :           [Moj01a] A. Mojsilovic, E. Soljanin : Color
   Color Quantization of Images Based on Human                Quantization and Processingby Fibonacci Lattices.
   Vision Perception. IEEE International Conference,          IEEE transactions on image processing, Vol.10,
   1994.                                                      No.11, pp.1712-1725, 2001.
[Cho89a] P. CHOU, T. LOOKABAUGH,                            [Ozd02a] D.Ozdemir, L.Akarun : A Fuzzy
   R.MGRAY : Entropy-Constrained Vector                       Algorithm for Color Quantization of Images,
   Quantization. IEEE   TRANSACTIONS ON                       Pattern Recognition, Vol.35, No.8, pp.1785–1791,
   ACOUSTICS,      SPEECH,    AND SIGNAL                      2002.
   PROCESSING, Vol. 31, No.1, pp.31-42, 1989.
                                                            [Pap07a] G. Papari, N. Petkov, P.Campisi: Artistic
[Hau01a] A.Hausner : Simulating Decorative                     edge andcorner enhancing smoothing. IEEE
  Mosaics. SIGGRAPH conference proceedings,                    Transactions on Image Processing,Vol.16, No.10,
  pp.573-580, 2001.                                            pp.2449-2462, 2007.
[Hay04a] J. Hays, I. Essa: Image and Video Based            [Qia09a] W.H. Qian, D.Xu, G.Zheng: A NPR
   Painterly Animation, NPAR, pp.113-120, 2004.                Technique     of     Abstraction     Effects,2nd
[Her98a] A. Hertzmann: Painterly Rendering with                International Congress on Image and Signal
   Curved Brush Strokes of Multiple Sizes.                     Processing, Vol.1721, No.828, pp.1-5 2009.
   SIGGRAPH conference proceedings, pp.453-460,             [Tom98a] C. Tomasi, R. Manduchi: Bilateral
   1998.                                                      ﬁltering for gray and color images. In
[Kan07a]H. Kang, S. Lee, C. Chui. : Coherent Line             Proceedings      international      Conference
  Drawing, Proc. ACM Symposium on Non-                        onComputer Vision, pp. 839-846, 1998.
  photorealistic Animation and Rendering, pp.43-            [Win06a] H. Winnemoller, C.S. Olsen, B.Gooch :
  50, 2007.                                                   Real-Time Video Abstraction. SIGGRAPH
[Kan08a] H. Kang, S. Lee : Shape-simplifying Image            Conference Proceedings,pp.1221-1226, 2006.
   Abstraction, Computer Graphics Forum, Vol.27,            [Zha95a] W. LI, Y.Q. ZHANG : Vector-Based
   No.7, pp.1773-1780, 2008.                                  Signal Processing and Quantization for Image and
[Kan09a] H. Kang, S. Lee, K.C. Chui :Flow Based               Video Compression. IEEE Processing, Vol.83,
   Image Abstraction, IEEE transactions on                    No.2, pp.317-335, 1995.
   visualization and computer graphics, Vol.15,             [Zha08a] H. Zhao, X. Jin, J. Shen, X. Mao, J. Feng :
   No.1, pp.62-76, 2009.                                       Real-Time Feature-aware Video Abstraction. The
[Kim96a] K.M Kim, C.S. Lee, E.J Lee, Y.H Ha :                  Visual Computer: International Journal of
   Color Image Quantization Using Weighted                     Computer Graphics, Vol24, No7-9, pp.727-747,
   Distortion Measure Of HVS Color Activity. IEEE              2008.
   TRANSACTIONS ON IMAGE PROCESSING,
   Vol.3, pp.1035-1039, 1996.
[Kuw76a] M. Kuwahara, K. Hachimura, S. Eiho, M.
   Kinoshita: Processing of ri-angio cardio graphic
   images, Digital Processing of Biomedical images,
   pp.187–203, 1976.




WSCG 2012 Communication Proceedings                    52                                     http://www.wscg.eu
             Contact Hardening Soft Shadows using Erosion
                                                                                                  Paul Obermeier
             Andreas Klein                             Alfred Nischwitz
                                                                                                MBDA Deutschland
           Munich University of                       Munich University of                            GmbH
            Applied Sciences                           Applied Sciences                         Hagenauer Forst 27
             Lothstrasse 64                             Lothstrasse 64                        86529 Schrobenhausen,
                                                                                                     Germany
         80335 Munich, Germany                     80335 Munich, Germany
                                                                                              paul.obermeier@mbda-
          andreas.klein@hm.edu                      nischwitz@cs.hm.edu
                                                                                                    systems.de


                                                         ABSTRACT
In this paper, we present an image based method for computing contact hardening soft shadows by utilizing an ero-
sion operator. Our method is based on shadow mapping and operates in screen space. By using object silhouettes
in hard shadows, we estimate the penumbra size and scale an erosion operator to generate the penumbra areas.
Furthermore, we present two solutions to generate the shadow factor for the penumbra areas. Our method works
best for small penumbras and can be easily integrated into existing shadow mapping based applications.

Keywords
Shadow Mapping, Soft Shadows.

1    INTRODUCTION                                                        screen space. Furthermore, it is suited for high shadow
                                                                         map resolutions as well as multiple shadow maps.
Shadows are an important part for the human percep-
tion. They give clues about the spatial relationship and                 2   RELATED WORK
the form of objects. Real world shadows can be divided
                                                                         The rendering of soft shadows has been studied exten-
into an umbra and a penumbra. An umbra occurs when
                                                                         sively over the last years. Therefore, we focus our re-
a light source is completely occluded and a penumbra
                                                                         view on publications closely related to our work. For
when it is partially occluded.
                                                                         an exhaustive survey on other methods see [Eis11a].
In real-time rendering, a popular method to generate                     Percentage closer filtering (PCF) [Ree87a] is a popu-
shadows is shadow mapping [Wil78a].           Shadow                     lar method for generating soft shadows. The idea is to
mapping assumes point light sources and thus, only                       make multiple shadow comparisons within a user de-
hard shadows are produced. However, real world light                     fined filter window. The shadowing factor is then built
sources are extended, and they generate penumbras,                       by averaging the result. To generate contact hardening
whose size often can be proportional to the light size                   soft shadows with PCF, Fernando [Fer05a] proposed
and the receiver-blocker distance.                                       percentage closer soft shadows (PCSS). He introduced
Current methods for generating contact hardening soft                    a blocker search as a preprocessing step, where he sam-
shadows are not suited for high shadow map resolu-                       pled the shadow map to calculate an average blocker
tions [Lau07a], require a high amount of texture fetches                 depth for each screen space pixel and approximated a
[Fer05a] or the performance decreases with the number                    penumbra width with a parallel planes approximation.
of shadow maps [Gum10a].                                                 Finally, he used the penumbra width to scale the PCF
                                                                         filter window.
We present an algorithm to produce contact hardening
                                                                         Arvo et al. [Arv04a] estimated the penumbra regions
soft shadows using an erosion operator. Our algorithm
                                                                         by detecting the edges in hard shadows and propagat-
is an extension to shadow mapping and operates in
                                                                         ing a visibility factor using a flood fill algorithm. Rong
                                                                         and Tan [Ron06a] accelerated this method using jump
                                                                         flood fill algorithms. Gumbau et al. [Gum10a] dilated
Permission to make digital or hard copies of all or part of              a shadow map to replace the blocker search of PCSS.
this work for personal or classroom use is granted without               Furthermore, they replaced the PCF filtering with a sep-
fee provided that copies are not made or distributed for profit
                                                                         arable Gaussian blur.
or commercial advantage and that copies bear this notice and
the full citation on the first page. To copy otherwise, or re-           There are several approaches to calculate soft shadows
publish, to post on servers or to redistribute to lists, requires        in screen space. Robison and Shirley [Rob09a] used
prior specific permission and/or a fee.                                  a screen space distance map to estimate a penumbra



WSCG 2012 Communication Proceedings                                 53                                         http://www.wscg.eu
width and blurred a hard shadow map with it. Hanjun               We calculate the penumbra width for each edge pixel
and Huali [Han10a] developed an algorithm that prop-              by using the parallel planes approximation of Fernando
agates a shadow factor using erosion and dilation that            [Fer05a]. Additionally, the kernel size is scaled based
is closely related to our work. However, we incorpo-              on the camera-receiver distance [Gum10a]:
rate contact hardening soft shadows as well as penum-
bra anti-aliasing. Aguado and Montiel [Agu10a] pre-                                       (dreceiver − dblocker )ωlight
                                                                           ω penumbra =
sented an approach where a penumbra size is propa-                                             dblocker dobserver
gated using a mipmap flood fill and the penumbra is
generated with a Gaussian filter in screen space. How-            where ωlight is the light dimension. We store the
ever, this approach produces light leaks which can be             penumbra width in the second texture channel.
reduced by using multiple layers. MohammandBagher
et al. [Moh10a] used a projected shadow map in screen             Erosion
space to estimate a penumbra size and to blur a hard              After the edges have been detected, they are eroded
shadow map                                                        with a variable sized erosion filter1 . We estimate the
                                                                  erosion by using a min-max mipmap [Isi06a, Dmi07a]
3   ALGORITHM OVERVIEW                                            (Figure 1). Recall that the edge texture stores a zero
Our algorithm computes the penumbra in screen space               for each boundary pixel in the first channel and the
and is an extension to existing shadow mapping ap-                penumbra size in the second channel. First we gener-
proaches. The algorithm proceeds as follows. First,               ate the min-max mipmap hierarchy for the edge texture
we render hard shadows with a shadow mapping al-                  by performing a min operation in the first channel and
gorithm. Second, we detect edges in the hard shad-                a max operation in the second texture channel. During
ows and store the blocker-receiver distance as well as            erosion we calculate a maximum search radius for the
the camera-receiver distance for the edge pixels. Now             given light dimension and the distance to the observer
we can calculate the penumbra width since it is pro-              in order to find the closest edge, as the penumbra widths
portional to the blocker-receiver distance and indirect           are only stored in the edge pixels. We choose a mipmap
proportional to the camera-receiver distance. Next, we            level based on the maximum search radius and query
erode the edges with a filter kernel that is scaled ac-           the min-max mipmap hierarchy. If the first channel con-
cording to the penumbra width and thus, estimating the            tains no boundary pixel, we immediately terminate the
inner and outer penumbra regions for contact hardening            erosion. Otherwise, we read the penumbra width from
soft shadows. In order to generate the final penumbra,            the second texture channel and choose again a mipmap
we propose two solutions. First, by filtering the shadow          level. Finally, we access the mipmap hierarchy in this
map in the penumbra regions with PCF and second, by               mipmap level and test for boundary pixels. If a bound-
directly calculating it during erosion.                           ary pixel is found, we store the penumbra width in the
                                                                  result texture. Otherwise we discard the pixel.
Rendering Hard Shadows
The first step in the algorithm is to render hard shadows         Determine the Shadow Factor
and auxiliary buffers. We assume that a shadow map                In the final pass, we use PCF to determine the shadow
has already been rendered. The scene is rendered from             factor for each pixel. We scale the PCF filter based on
the observer and we perform a standard shadow com-                the penumbra width and combine the result with the
parison for each screen space pixel. We store a one in            hard shadow map rendered in the first pass.
a screen space hard shadow buffer for each lit pixel and
a zero for each shadowed pixel. Additionally, the depth
                                                                  4   ESTIMATE A SHADOW FACTOR
difference between the blocker and receiver as well as
the distance to the camera is stored. Furthermore, we                 WITH EROSION
store the diffuse color in a separate texture to calculate        A second solution is to estimate a shadow factor di-
the final shading in the last pass.                               rectly during erosion. In order to realize this idea, some
                                                                  changes in the algorithm are necessary.
Edge Detection
                                                                  As the screen space hard shadow buffer is rendered
The second step is to estimate the penumbra regions               from the observer’s viewpoint, objects may occlude
by detecting the edges in the screen space hard shadow            shadowed areas and thus, the edge detector will pro-
buffer. As the hard shadow buffer is a binary image, we           duce edges which do not belong to penumbra regions.
can easily detect the edges with a 3 x 3 Laplacian filter,        As Arvo et al. [Arv04a] pointed out, this issue can
which needs five texture fetches:                                 be solved by testing the shadow map for silhouettes on
                                  
                       0 1 0
                     1 −4 1                                 1   In terms of image processing this operation is an erosion, as
                       0 1 0                                      the zeros in the edge texture are propagated.



WSCG 2012 Communication Proceedings                          54                                             http://www.wscg.eu
                                                                                                2.
                                                         1.



                                                               3.


Figure 1: Erosion of the edges using a min-max mipmap. First, we choose mipmap level based on a maximum
search radius and read the penumbra size from the max channel of the mipmap. Second, we calculate a mipmap
level based on the penumbra size and read the edge value stored in min channel. Finally, we output the penumbra
size if the edge value can be classified as a boundary pixel.

each detected edge pixel. We use the same 3 x 3 Lapla-              Due to aliasing in the screen space hard shadow buffer,
cian filter and compare the result against a threshold. If          this method replicates the aliasing in the penumbra
the result is within the threshold, the edge pixel is valid         (Figure 2).
and will be used in the next step. Otherwise, we discard            To increase the visual quality, we search for a best fit
it.                                                                 straight line by a least square method in a discrete envi-
In order to compute the shadow factor, we implemented               ronment around each edge pixel prior to the erosion and
the variable sized erosion in a gathering approach. First,          store the line parameters in an auxiliary texture. During
we determine a maximum kernel size and search for                   erosion, we read the line parameters from the texture
edge pixels within this area. If an edge pixel is found,            and calculate the vector vline from the edge pixel’s cen-
we calculate its penumbra width and check, whether the              ter to the line. Finally, we add vline to the vector from
current pixel is within its range. We continue until we             the erosion point to the edge, calculate the distance and
found the edge pixel with the smallest distance to the              use it during erosion (Figure 3).
current pixel. The shadow factor of the outer penumbra              This compensates parts of the aliasing in the screen
can then be directly calculated:                                    space hard shadow buffer and increases the visual qual-
                                                                    ity (Figure 2). However, there are still some incor-
                           ω penumbra − dmin                        rect pixels at the transition from the outer to the inner
                souter =
                              2ω penumbra                           penumbra. We will try to solve this issue in future work.

where dmin is the minimum distance to the edge and
ω penumbra the penumbra size. The inner penumbra is
simply calculated with sinner = 1 − souter .




Figure 2: Left: Artifacts due to aliasing in hard shad-             Figure 3: In order to improve the visual quality, we
ows. Right: Result after the distance correction. Note              search for a best fit line and offset the vector to the edge
that there are still some incorrect pixels at the transition        with the vector from the edge to the line. Finally, we
from the outer to the inner penumbra.                               use the distance of the resulting vector during erosion.


WSCG 2012 Communication Proceedings                            55                                           http://www.wscg.eu
5   RESULTS                                                       we implemented a second solution for generating the
We compared our algorithm against a PCSS implemen-                shadow factor with a PCF filter, which results in a su-
tation and a reference solution. This PCSS implementa-            perior image quality.
tion uses a Poisson disk for the blocker search and PCF           Nevertheless, this technique has limitations. As our
filtering. Both methods use 32 samples for the final              method is based on PCSS, it has the same limitations,
PCF. The reference solution is realized by approximat-            such as overestimating the penumbra size. Further-
ing the area light source with 512 point light sources.           more, the erosion size is bounded and thus, we may
The screen resolution was 1920 x 1080 pixels and the              miss relevant occluding information. This could result
shadow map size was 2048 x 2048. Figure 4 and 5                   in visible artifacts. Due to mipmap erosion and the
shows the resulting images and Table 1 the performance            scaling of the penumbra width based on the distance
results. Table 2 shows the duration of the algorithm              to the camera, our method introduces aliasing when the
steps in the buddha dataset. The performance results              camera is moved. Another limitation is that the visual
were obtained on an Intel Xeon E5620 CPU with 2.4                 quality is strongly dependent on the quality of the hard
GHz, 8 GB RAM and a NVIDIA GeForce GTX 680                        shadows. Consequently, aliasing reduction algorithms
graphics card with 2048 MB memory.                                such as cascaded shadow mapping (CSM) [Eng06a]
                                                                  and light space perspective shadow maps (LiSPSM)
              Cactus       Hairball      Buddha                   [Wim04a] should be used.
              (188K tris) (2.88M tris) (1.08M tris)
  Erosion     1.62         12.56         5.77                     7   CONCLUSIONS AND FUTURE
  PCSS 8      1.58         11.51         4.68                         WORK
  PCSS 32 1.60             11.89         5.11
                                                                  We proposed a method for generating contact hardening
  PCSS 64 1.66             12.44         5.69
                                                                  soft shadows in screen space. As with all image based
Table 1: Performance results in comparison with PCSS              methods, this technique works best for small penum-
using 8, 32 and 64 blocker search samples. The screen             bras and can be used to extend shadow mapping based
resolution is 1080p.                                              applications. Furthermore, we presented two solutions
                                                                  to generate a shadow factor for the penumbra. While
             Step               Time [ms]                         the mipmap erosion is fast and produces results com-
             Shadow Map         2.20                              parable to PCSS, the calculation of the shadow factor
             Hard Shadows       2.52                              during erosion still produces some artifacts.
             Edge Detection 0.31
                                                                  For future work, we wish to explore the possibility to
             Erosion            0.30
                                                                  replace the least square line fitting with a low pass filter
             Shading            0.44
                                                                  and try to reduce the remaining artifacts.
Table 2: Duration of the algorithm steps using the bud-
dha dataset.                                                      8   ACKNOWLEDGMENTS
6   DISCUSSION                                                    We thank the anonymous reviewers for their useful
                                                                  comments. The work of A. Klein is funded by MBDA
The bottleneck of PCSS [Fer05a] is the blocker search             Deutschland GmbH.
that is performed for each screen space pixel. Our moti-
vation is to replace the blocker search per pixel by oper-        9   REFERENCES
ating only on silhouettes of hard shadows. However, the
rendering chain of our method is more complex. Thus,              [Agu10a] Aguado, A. and Montiel, E. MipMapped
a speedup is only achieved when the blocker search is                  Screen Space Soft Shadows. In GPU Pro 2. 2010
performed with 64 samples per pixel and the penum-                [Arv04a] Arvo J., Hirvikorpi M., Tyystjarvi J. Ap-
bra area is small. In contrast to PCSS, our method does                proximate Soft Shadows with an Image-Space
not produce artifacts resulting from a small number of                 Flood-Fill Algorithm. Computer Graphics Forum
blocker search samples when rendering fine structured                  23, 271-279, 2004.
geometry, such as in Figure 4.                                    [Dmi07a] Dmitriev K., Uralsky Y. Soft shadows using
One possible issue in the method of [Gum10a] is that                   hierarchical min-max shadowmap. GDC 2007,
the algorithm operates on shadow maps. In contrast                     2007.
our algorithm operates on a screen space hard shadow              [Eis11a] Eisemann E., Schwarz M., Assarsson U.,
buffer, which makes it attractive for applications with                Wimmer M. Real-Time Shadows, Taylor & Fran-
multiple shadow maps.                                                  cis, 2011.
Compared to [Han10a] we incorporated variable sized               [Eng06a] Engel W. Cascaded shadow maps. In Shader
penumbras and increased the visual quality by calculat-                X5, Engel W., (Ed.). Chares River Media, 197-
ing the distance to a best fit straight line. Furthermore,             206, 2006.



WSCG 2012 Communication Proceedings                          56                                           http://www.wscg.eu
Figure 4: Visual results in the cactus dataset. Top Left: Our algorithm. Top Right: PCSS with 8 blocker search
samples. Notice the artifacts resulting from missed blockers due to the small number of blocker samples. Bottom
Left: PCSS with 64 blocker search samples. Bottom Right: reference solution.

[Fer05a] Fernando R. Percentage-Closer Soft Shad-              [Rob09a] Robison A. and Shirley P. Image space gath-
      ows. ACM SIGGRAPH 2005 Sketches, 2005.                        ering. In Conf.proc. High Performance Graphics
[Gum10a] Gumbau J., Chover M., Sbert M. Screen                      2009, 91-98, 2009.
      space soft shadows. In GPU Pro - Advanced Ren-           [Ron06a] Rong G., Tan T.-S. Utilizing jump flooding
      dering Techniques, Engel W., (Ed.). A.K. Peters,              in image-based soft shadows. In Conf.proc. VRST
      2010.                                                         ’06, ACM, 173-180, 2006.
[Han10a] Hanjun J., Huali S. Rendering fake soft shad-         [Wil78a] Williams L. Casting curved shadows on
      ows based on the erosion and dilation. 2nd Inter-             curved surfaces. In Conf.proc. SIGGRAPH ’78,
      national Conference on Computer Engineering                   ACM, 270-274, 1978.
      and Technology, 234-236, 2010.                           [Wim04a] Wimmer M., Scherzer D., Purgathofer
[Isi06a] Isidoro J. R. Shadow Mapping GPU-based                     W. Light space perspective shadow maps. In
      Tips and Techniques. GDC 2006, 2006.                          Conf.Proc. Eurographics Symposium on Render-
[Lau07a] Lauritzen A. Summed-area variance shadow                   ing, 2004.
      maps. In GPU Gems 3, Nguyen H., (Ed.).
      Addison-Wesley, 157-182, 2007
[Moh10a] MohammadBagher M., Kautz J.,
      Holzschuch N. and Soler, C. Screen-space
      percentage-closer soft shadows. ACM SIG-
      GRAPH 2010 Posters, 2010.
[Ree87a] Reeves W. T., Salesin D. H., Cook R. L.
      Rendering antialiased shadows with depth maps.
      In Conf.proc SIGGRAPH ’87, ACM, 283-291,
      1987.



WSCG 2012 Communication Proceedings                       57                                     http://www.wscg.eu
Figure 5: Resulting shadows from the hairball and buddha datasets. Note that the shadow softness increases with
the blocker-receiver distance. From left to right: Our method, PCSS with 32 blocker search samples, PCSS with
64 blocker search samples and reference solution.




WSCG 2012 Communication Proceedings                   58                                     http://www.wscg.eu
Diffusion-based parametrization of surfaces on 3D-meshes
            Martin Schmidt                             Michael Guthe
                                                                                                    Volker Blanz
          Philipps-University of                   Philipps-University of
                                                                                                University of Siegen
                 Marburg                                  Marburg
                                                                                                   Hölderlinstr. 3
          Hans-Meerwein-Str. 6                     Hans-Meerwein-Str. 6
                                                                                                   57076 Siegen
        Germany, 35032 Marburg                    Germany, 35032 Marburg
                                                                                               blanz@informatik.uni-
        schmidtma@informatik.uni-                  guthe@informatik.uni-
                                                                                                     siegen.de
               marburg.de                               marburg.de


                                                         ABSTRACT
This work presents a new approach towards parametrization of three-dimensional wireframe models. The method
is derived from a specific phenomenon of cellular development in nature. It recreates the effect of diffusion of
messengers through tissue, which leads to the formation of extremities and other anatomical structures depending
on the position on the tissue surface. This process of diffusion on the surface is analyzed and simplified for usage
as a parametrization of mesh surfaces. The presented approach uses the similarity of wireframe meshes and graphs
in order to carry out the mechanism of diffusion. For this it implements a specialized algorithm based on Dijkstra’s
algorithm for finding the shortest paths.
The results of this mechanism are saved and organized in a binary tree structure, which allows for simple and
efficient construction of correspondence between two distinct meshes. The paper concludes with an outlook on
possibilities of further development and enhancements of the approach.

Keywords
mesh, parametrization, diffusion, geometric analysis

1     INTRODUCTION                                                       Basics

Whenever computers analyze or display visual objects,                    An important aspect of 3D shape processing and anal-
they need to represent these objects in a suitable mathe-                ysis is the connection of objects with each other. By a
matical form that is appropriate to the processing tasks.                correct link between two objects, algorithms can trans-
The typical representation of an object is a polygon                     fer knowledge and properties from one object to the
mesh, but for many purposes such as texturing, object                    other. It is also possible to describe the similarity be-
retrieval, shape comparisons, differential geometry or                   tween these objects. This link is generated by a surface
for computing point-to-point correspondence between                      parametrization that is consistent across different ob-
pairs of objects, it is important to describe shapes as                  jects.
parameterized surfaces. Hence, the purpose of this pa-
per is to introduce a new approach in parametrization                    A parametrization of surfaces is defined as a mapping
and its evaluation.                                                      from a parameter domain onto the surface. This is
                                                                         also called Surface-to-Surface-Mapping, if the parame-
                                                                         ter domain is a surface itself. The parametrization of a
1.1    Parametrization                                                   parameter domain maps each point on the domain onto
                                                                         a certain point of the surface.
In the following, we give a short introduction on the
topic of parametrization, its applications and related                   Bijectivity is an important property of parametriza-
work. This is concluded by the analysis of the distinc-                  tions, as many applications rely on a complete cover of
tion of our approach in contrast to previous work.                       the originating surface without introducing ambiguity.
                                                                         That means each point on the surface maps to exactly
                                                                         one point on the parameter domain and vice versa.
Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without
                                                                         Possible applications in computer graphics
fee provided that copies are not made or distributed for profit
or commercial advantage and that copies bear this notice and
the full citation on the first page. To copy otherwise, or re-           There are several possible applications for parametriza-
publish, to post on servers or to redistribute to lists, requires        tions in computer graphics. The three most important
prior specific permission and/or a fee.                                  applications are briefly described in the following.



WSCG 2012 Communication Proceedings                                 59                                         http://www.wscg.eu
1.1.1   Transfer of detail:
One of the first times a parametrization of objects be-
came necessary was the application of texture mapping
in rendering (see Figure 1. As a part of the rendering              Figure 2: Correspondence between two surfaces
of a scene, texture mapping increases the detail of the
surfaces of objects by drawing a pregenerated image on           1.2    Related Work
the surface.
                                                                 The increasing requirements for parametrizations on
                                                                 surfaces have led to the development of different ap-
                                                                 proaches with different pros and cons. The following
                                                                 section summarizes these attributes.

                                                                 Criteria for the evaluation of parametrizations
              Figure 1: Texture mapping
                                                                 The attributes, by which parametrizations can be mea-
Further improvements of this approach include Normal             sured regarding their potential application, are:
Mapping, which transfers the shading of a high qual-
ity mesh consisting of thousands of polygons to a mesh           • the degree of distortion
with reduced polygon count to increase the detail on
the latter mesh without decreasing render speed. More            • the aspect of bijectivity
approaches like this are Bump Mapping and Displace-
                                                                 • the limitation on certain mesh types
ment Mapping, which – similar to Normal Mapping –
also apply more detail onto a mesh without significant
impact on the render time.                                       Distortions in the parameter domain are a direct re-
                                                                 sult of parametrization and can be of different type. It
                                                                 has been proven by differential geometry that there is
1.1.2   Remeshing:                                               no distance-preserving parametrization for generic sur-
Polygonal meshes are created by several methods like             faces [8]. The distortion can be minimized, but not fully
scanning with lasers and modeling by hand in a special           prevented.
software. This leads to meshes of different resolution           Possible types of distortion are the distortion of distance
and different surface generation techniques (e.g. trian-         – an irregular distribution of parameter values in one di-
gles, quads, mixes of both). Sometimes an application            mension – and angular distortion. Type and degree of
only allows for a certain kind of triangulation and reso-        distortion are important criteria in judging methods of
lution.                                                          parametrization. It is often dependent on the specific
                                                                 application which methods are more suitable than oth-
This is where Remeshing becomes important. Remesh-
                                                                 ers. This leads to compromises almost every time since
ing parametrizes a mesh and then maps a regular and
                                                                 no parametrization is completely free of distortion.
desired triangulation on the parameter domain to retri-
angulate the original mesh [13]. The application of sub-         The methods of parametrization also show different bi-
division on the parameter domain can also lead to good           jective behavior. This can be separated into global and
results in regard to desired mesh quality [17], [14].            local bijectivity. Global bijectivity is maintained over
                                                                 the whole mesh, while local bijectivity is given only for
                                                                 local regions on the surface. Not every parametrization
1.1.3   Correspondence:
                                                                 leads to bijectivity of one of these kinds.
If two meshes should be analyzed, it is sometimes de-            The third criterion is the limitation on certain mesh
sired to link those meshes to each other by means of             types. Some parametrizations need convex meshes,
correspondence. The correspondence of two meshes                 while other approaches can also use more complex
means that the relationship between a region A on the            meshes.
first mesh and a region B on the second mesh is known.
This can be used to transfer details from one mesh to
                                                                 Approaches
the other.
To create this link, both meshes need to be mapped               Perhaps the oldest method of parametrization was de-
to the same representation. By using a bijective                 veloped by William Tutte in 1963 [27]. Tutte used
parametrization, an algorithm can map each point on              graph embedding as the basis of the approach, which
the first mesh to the corresponding point on the second          led to a bijective parametrization with distortion in dis-
mesh simply by choosing the same parameter values.               tance and angular aspects. Using the same approach,



WSCG 2012 Communication Proceedings                         60                                          http://www.wscg.eu
two algorithms developed by Floater show similar be-             Diffusion helps cellular development and differentia-
havior, but at the same time reduce angular distortion           tion by giving hints on the position in the organism to
[10], [11].                                                      individual cells. The aim of this paper is to show that
Eck et al. use a parametrization for remeshing at differ-        the natural processes of diffusion can help in the devel-
ent resolutions [9]. This method is based on harmonic            opment of algorithms for parametrizations.
maps and therefore preserves angular dimensions.                 The proposed method guarantees local bijectivity on
DCP is a different parametrization developed by                  convex mesh parts, which are constructed from the orig-
Desbrun et al. [6] and combines the already known                inal mesh. The parametrization retains the proportions
Dirichlet-energy [20] with a new quadratic Chi-energy            of distance of the mesh parts. Because the mesh is
Eχ , which describes the inner angles of triangles. This         viewed as a graph, already available and highly op-
method is not bijective in every situation, but can be           timized algorithms from graph theory can be used to
used without limitations (e.g. only convex meshes).              achieve an efficient and stable parametrization.
Implementing the Least-Squares-method,              the          In the following, the course of the paper is to present an
parametrization called LSCM preserves the orien-                 introduction to physical and biological diffusion, fol-
tation of each triangle and angle. It is independent of          lowed by evaluation of algorithms which can be appro-
resolution but cannot guarantee bijectivity for every            priate for serving as a basis for further development.
mesh [18].                                                       Later on, the modifications to the chosen algorithm are
Linear parametrizations like those mentioned above               explained. The paper concludes with a discussion and
tend to create an increased distance-based distortion            and an outlook on possibilities for further research.
on meshes with sharp slopes.         Using non-linear
parametrizations helps to reduce these distortions. An
example is MIPS [15], which divides the mesh into                2     DIFFUSION
several linear maps. A special functional reduces the
distortion map by map and creates a parametrization              The basis of our approach originates from morphogene-
which is bijective and can be used without limitation.           sis. Morphogenesis controls the differentiation and de-
                                                                 velopment of cells in multicellular organisms to organs
The parametrization ABF differs from the mentioned
                                                                 and extremities, and it produces patterns on skin, fur or
methods, as it does no work on the vertices, but on the
                                                                 shells of animals. Well-directed flow and diffusion of
angles of the triangles [22]. It reduces angular distor-
                                                                 activators through the tissue leads to specific develop-
tion and shows local bijectivity. A variation of the al-
                                                                 ment of the stem cells depending on the structure they
gorithm called ABF++ increases calculation speed and
                                                                 are going to form.
stability on large meshes.
Kharevych et al. adopt a similar approach by defining            The gradients of concentrations of specific substances
circumcircles on every vertex [16]. Cutting circumcir-           form a metric in the tissue and on the surface. On dif-
cles define the angles between vertices, which are then          ferent shapes of the same type, these gradients and thus
minimized. This approach works best on Delaunay-                 the induced metric are similar, so they describe objects
triangulations [5].                                              regardless of position in space, scale, orientation and
                                                                 resolution of the mesh.
The introduced parametrizations are primarily based on
mathematical ideas and concepts from computer sci-
ence. These are the topics of differential geometry,             2.1    Physical diffusion
topology and graph theory, which are combined to rep-
resent a mesh in parameter domain.                               Diffusion occures whenever particles – for example
In the past, several approaches to carry concepts over           atoms, molecules or charge carriers – are aggregated
from natural processes to computer science were suc-             in a carrier medium and there exists a concentration
cessful and lead to groundbreaking and novel meth-               gradient between these particles. The reason for this
ods, for example genetic algorithms, routing algorithms          movement is called pedesis or Brownian motion [3].
and graph algorithms [19, 7, 1]. Other approaches that           The atoms and molecules are in an undirected motion,
use similar diffusion-based processes called reaction-           depending on temperature: Due to collisions, their di-
diffusion create textures automatically [26, 28].                rection changes randomly over time. If there is a con-
                                                                 centration gradient, there will be an overall net motion
In the topic of parametrizations, this transfer is yet to        along this direction, which forms the diffusion process
be made.                                                         described by the following equation:
1.3   Motivation
This work applies insights from biology and chemistry                         δu
                                                                                 = D · ∆u = D · div (grad u)           (1)
to the problem of consistent surface parametrization.                         δt



WSCG 2012 Communication Proceedings                         61                                         http://www.wscg.eu
2.2    Diffusion in developmental biology                         2.4    Diffusion as a description of surfaces
Diffusion plays an important role in biological pro-              To successfully compute the diffusion on the surface,
cesses. By diffusion through membranes cells are sup-             the physical and biological model needs to be simpli-
plied with nutrients and metabolic waste products are             fied. The first simplification is to consider the surface as
removed. Patterns on the skin or fur are also controlled          a two-dimensional carrier medium and discarding any
by diffusion of messengers.                                       effects of diffusion into depth. Both complexity and
                                                                  computational costs are decreased due to the fact that
This is described in detail by Gierer and Meinhardt,
                                                                  only two dimensions are considered.
who present a model which depicts the formation of
structures in biological cell structures [12]. The in-            We assume a dynamic equilibrium where a substance
teraction of two messengers – the so called activa-               is produced in a source point or a line-shaped set of
tor/inhibitor pair – play a major role in position depen-         sources, travels along the surface due to diffusion and
dent pattern formation. Both agents diffuse through the           is diluted, washed away, absorbed or deactivated chem-
tissue and lead to different concentrations depending on          ically over time. Therefore, in our simplified model,
position. This process – called morphogenesis – has               the concentration of the substance is proportional to the
been described by Turing [25] and was specified further           distance from the source.
by Gierer and Meinhardt. A gradient of concentration              To obtain relative distances on the surface, independent
values runs between the cells producing the activator             of the scale of the object, we introduce pairs of antago-
and between the cells producing the inhibitor. The re-            nist substances that have concentrations d1 and d2 and
sulting patterns of cell differentiation are aligned along        are defined on each vertex of the mesh. To calculate the
this axis.                                                        relative distance between each of the reference points,
One special form of this mechanism is observable in               the following equation is used:
the polyps of the genus Hydra. Their heads and feet are
shaped depending on the concentration of the activator                                             d1
                                                                                         dˆ =                             (2)
[12]. Gierer and Meinhardt showed that the diffusion                                            d1 + d2
of the morphogenes in Hydra takes a gradual course.
                                                                  This formula makes sure that dˆ takes the value 0 on the
This gradient of source density gives orientation in the
                                                                  source points of the first substance and value 1 on the
tissue over the longitudinal axis of the animal’s body.
                                                                  source points of the second substance. It then increases
The gradient serves as an indicator of relative position
                                                                  smoothly in between. The possible values of distance dˆ
within the animal.
                                                                  are clamped to the interval [0, 1].
2.3    Formulation of the idea                                    This operation is the necessary first step in parametriz-
                                                                  ing the whole mesh. To get an even distribution of the
Several criteria influence the parametrization and con-           parameters, sets of points form the source points. This
struction of correspondence, depending on the geome-              leads to a smooth and near parallel distribution of the
try and quality of the mesh. These are:                           isolines on the surface (see Figure 3).

• Scale of the mesh

• Position of the mesh relative to the coordinate sys-
  tem

• Rotation of the mesh

• Internal geometry of the mesh (for correspondence
  a rough similarity is sufficient)                               Figure 3: An ideal distribution of isolines of dˆ is ob-
                                                                  tained if sources for d1 (red) and d2 (green) are not only
There may be considerable variation in these parame-              single points, but sets of points along the edges
ters and properties, which makes many surface anal-               For a unique identification of each point a second axis
ysis problems challenging. In many cases, surface                 in parameter distribution is needed (see Figure 4). If
parametrizations help to find more simple and efficient           this second axis is orthogonal to the first direction of
solutions.                                                        diffusion, two linearly independent parameters result
Our diffusion-based parametrization is inspired by the            and are able to describe every point on the surface by
gradient of source density, which defines a polar ori-            a pair of the interval [0, 1]2 . We call these two axes the
entation. We will ignore most of the details of the               gradients of diffusion ∇u and ∇v. The parameters are
biological mechanism and develop the idea towards                 called u and v and form pairs (ui , vi ) for each point Pi
parametrization of surface patches.                               on the surface.



WSCG 2012 Communication Proceedings                          62                                           http://www.wscg.eu
                                                                    is used in order to determine the value of relative dis-
                                                                    tance dˆ in the calculation.
                                                                    We deliberately decided to propagate the values along
                                                                    edges. Alternatively one could choose the geodesics,
                                                                    i.e. direct paths along the surface which are not con-
                                                                    strained to points in the set. Geodesics, however, do
                                                                    not respect the structure of the mesh. Interpolation is re-
                                                                    quired to calculate the geodesic path through the poly-
Figure 4: A 2D parametrization (u, v) is obtained by                gons. Because of this computational increase the idea
two separate diffusion processes, one along the hori-               of geodesics had been discarded.
zontal and one along the vertical direction, with two
variables dˆu , dˆv                                                 2.7    Segmentation
                                                                    Whether global bijectivity of the parametrization can
2.5    Organization of the parameter values                         be achieved at all depends on the topology of a mesh.
       in a kd-tree                                                 A mesh which is topologically equivalent to a disc can
                                                                    be mapped to a parameter domain in the plane. These
By simply saving the parameter values to each point,
                                                                    simple meshes do not need to be partitioned and eas-
one can easily retrieve the tuple (u, v) for every given
                                                                    ily achieve global bijectivity. Other meshes with more
point. This is done in a linear array, which can be ac-
                                                                    complex topology cannot be mapped to a plane and
cessed in O(1). For an adequate use the other way
                                                                    therefore violate global bijectivity.
around is also important, therefore it is necessary to
have an efficient data structure for finding a point cor-           Consider the simple example of a sphere. From topol-
responding to a given value. Different data structures              ogy, we know that we cannot find a homeomorphism
exist, which help to find a given value in a set of points          from parameter space [0, 1]2 to the sphere, so we expect
quickly without comparing each point to the search                  singularities and ambiguities if we apply our algorithm
term. In our work we chose the kd-tree [2], which                   to the entire sphere. For the first parameter u and its
scales well with a certain number of points. The com-               relative distance function d,ˆ consider a line from pole
plexity of searching for a value lies in O(log n), where            to pole (half of a great circle) as a source for d1 , and
n is the number of vertices of the mesh.                            the other half of the same great circle as source for d2 .
                                                                    This defines a diffusion which spreads over both hemi-
                                                                    spheres. However, both poles will be singularities be-
2.6    Viewing surfaces as a discrete point
                                                                    cause they are sources of both d1 and d2 . On the other
       set                                                          hand, if we use two opposite points as sources for d1
Polygonal surfaces are usually intended to be approx-               and d2 , respectively, we violate the uniqueness criterion
imate models of smooth surfaces such as geometrical                 as soon as both parameters u and v are computed: As
primitives (spheres), vehicles, human faces or charac-              shown in Figure 5, two arbitrary isolines of each diffu-
ters. The higher the number of polygons is, the better              sion cintersect twice, so these intersection points obtain
the approximation of the surface can be. Because of                 the same parameter tuple (u, v).
this approximation it is difficult to model the process
of diffusion, as it runs through a continuous substance.
The triangulated surface has no continuous nature, but
instead has gaps between discrete points.
The algorithm to solve this problem needs to simulate
the continuous progression of the diffusion along the
surface with special regard to the distance between the
points. One algorithm with these characteristic is Dijk-
stra’s algorithm for shortest paths. With a given starting
point, Dijkstra’s algorithm assigns each following point
the shortest distance to the starting point. It propagates          Figure 5: Ambiguity on certain points on meshes that
the distance values iteratively in a process that is similar        are not homeomorphic to a disc.
to diffusion.                                                       The solution to this undesired effect is the segmentation
We modified the algorithm in such a way that it takes a             of the mesh into hemispheres. Two passes parametrize
complete set of points to start instead of a single point.          each hemisphere separately. This guarantees local bi-
After a successful pass, each distance d1 between point             jecitivity on each partial mesh. If the segmentation of
and start is measured. A second pass in the reverse di-             the mesh is adequate, the whole mesh can be partitioned
rection leads to the second value d2 of diffusion, which            in topological discs with guaranteed uniqueness.



WSCG 2012 Communication Proceedings                            63                                          http://www.wscg.eu
Global bijectivity is not given and therefore the direct
mapping of (u, v)-tuples from two meshes is not possi-
ble. A mechanism is required to assign regions of dif-
ferent meshes to the same or similar areas, which intro-
duces a new level of hierarchy.
The segmentation must lead to reproducible regions.
Only then is the parametrization useful, particularly in
regard to the correspondence between meshes. Several
criteria define the usefulness of the segmentation.
Besides the reproducibility these are the number of
segmented regions, similar segmentation on similar
meshes and interaction of the user to mark logical
areas.
To achieve a 1 : 1 connectivity between meshes, the
segmentation must separate both mesh A and mesh B
to each set of regions rA0 , rA1 , .., rAn and rB0 , rB1 , .., rBn .
Each vertex must be assigned to a region. Both sets of                                  Figure 6: Segmentation example
regions can be related to each other if both meshes have
the same number of regions. To create a meaningful
correspondence, the regions must map to similar parts
on the meshes in the first place. This means that the
segmentation must follow a fixed orientation over the
mesh, which also adds to the reproducibility.
Meshes from different classes of objects should seg-                           Figure 7: Edge stretching and angular distortion
ment to semantic similarities of the same type. The al-
gorithm should react on the user’s input, which denom-
inates the points of interest regarding the logical areas.                  Separating such meshes into two halfs helps preventing
This simplifies the finding of correspondence between                       these unfavorable results. By splitting the meshes in
meshes and enhances quality.                                                question before mapping them to the plane a lower dis-
Existing approaches for reproducible segmentations                          tortion is achieved, as the change of length of the edges
lack the possibilities to work on a predefined number                       is reduced.
of regions and only respect the user’s input marginally.                    After choosing the regions carefully to avoid bad
Therefore, we chose to implement a simple, but                              segmentation, the next step is the selection of starting
efficient approach that gives the user the freedom of                       and ending edges. As mentioned before, the process
choice in segmenting the mesh (see Figure 6).                               of diffusion needs the four edges of the surface to
The idea is that the user paints the regions on the sur-                    be parametrized. The edges resemble the sets of
face manually with an interactive tool. This defines the                    starting and ending points for the modified Dijkstra.
segments of the patchwise parametrization. Moreover,                        Because the diffusion is not limited to four-sided
and this is the core idea of the user interface, the bound-                 surfaces, it must be possible to choose four logical
aries of the segments are a natural choice of source-sets                   edges on arbitrary convex surfaces, leading to edges
for the diffusion algorithm. Depending on the label of                      Eu1 , Eu2 , Ev1 , Ev2 . The finding of suitable edges can be
the adjacent region, a boundary vertex will be a source                     based on the segmentation into regions, as the borders
point of d1 or d2 for parameter u or v, respectively. The                   between two regions already form suitable edges.
goal of the segmentation is to produce patches with
closed and connected boundaries that can be divided                         In the case that there is only one continuous border be-
into 4 parts, similar to a rectangle.                                       tween two regions, the edge is simply the overlapping
                                                                            part of both regions. In order to not include a set of
Those parts which form the ends of extremities (e.g.                        points in two edges at once, the method can choose to
hands and feet in Figure 6) of the depicted mesh need                       include or exclude these vertices. This discrimination
special treatment. They could be seen as single regions,                    is necessary if a point takes part in both regions as seen
because they fulfill the topological criterion. The prob-                   in Figure 8.
lem of this naive approach is the nature of their edges.
These edges overly stretch during the mapping to the                        A user who is familiar with the process of segmentation
plane (as seen in 7). This leads to increased angular                       can handle it very quickly. The segmentation in Figure
distortion, with higher extremes in parts with longer                       6 did not take longer than 10 minutes with the interac-
stretched edges.                                                            tive, mouse-based tool.



WSCG 2012 Communication Proceedings                                    64                                           http://www.wscg.eu
                                                                    graph, until reaching its end (see Figure 9 for an illus-
                                                                    tration). Since the relaxation affects the value of dif-
                                                                    fusion, which in turn is the key to the min-heap, the
                                                                    residing vertices are sorted after each update to reflect
                                                                    the correct sequence of increasing key values.




   Figure 8: Overlapping of vertices in two regions

2.8     Process of diffusion
The diffusion processes each region on the mesh sep-
arately. This is done by separation and transformation
of the whole mesh into partial graphs. As all regions
are combined to the original mesh, the partial graphs
of each parametrized region form the whole graph that
represents the mesh.                                                Figure 9: The process of diffusion illustrated with a
                                                                    simplified mesh.
The construction of partial graphs shifts the process of
diffusion away from the original mesh. Each partial                 As soon as the min-heap is empty, the process of dif-
graph leads to an own uv-map, which in turn is indepen-             fusion finishes. At this point, each vertex holds its dis-
dent to the uv-maps of other partial graphs, thus solving           tance to the set of starting points. This distance is the
the global bijectivity problem. But as shown later, the             path to the closest vertex in the set of starting points.
local bijectivity on each partial graph is given.                   This algorithm is run twice, once for d1 and once for
                                                                    d2 . From these, dˆ is computed, which is equivalent to
The modified Dijkstra algorithm is initialized just like
                                                                    the parameter u. With another pair of source sets, the
the original algorithm. Each vertex that does not belong
                                                                    parameter v is computed in the same way.
to the starting points is assigned di = ∞. Unlike the
original Dijkstra’s algorithm, which starts from a single           After the completion of the second two-pass, each ver-
vertex, it is here the initial starting (source) set that is        tex holds the values du1 , du2 , dv1 and dv2 . These values
initialized as diS = 0. After inserting each point of the           are the direct values of distance to their corresponding
whole partial graph into a min-heap, the algorithm can              starting edge. By inserting these into the two equations
start.                                                              4, we normalize the values into the interval [0, 1]:
Taking the first element out of the min-heap gives the                                   du1                   dv1
point with the minimal value of diffusion dimin . In                          dˆu =                 dˆv =                   (4)
                                                                                      du1 + du2             dv1 + dv2
the first iterations this will be the whole set of start-
ing points, but later on this will govern all points in the         These values of diffusion dˆu and dˆv are then saved in the
whole partial graph. Each extracted point is treated in             kd-tree. By combining the kd-tree with a simple linear
exactly the same way. Every edge of this vertex will be             array, we get a uv-map that supports fast retrieval of
relaxed, using the following equation 3.                            (u, v)-tuples for a given vertex and also grants an effi-
                                                                    cient search for a vertex, which lies as close as possible
           (                                                        to a given (u, v).
            dA + w(A, B), if dA + w(A, B) < dB
      dB =                                              (3)
            dB ,          otherwise                                 3   RESULTS
                                                                    We implemented the modified Dijkstra’s algorithm. We
The function w(A, B) is the weighting function which                parametrized a scan from a human and a cow and man-
returns the length of the edge between vertices A and B.            ually developed a segmentation of the meshes. The hu-
Vertex A is the vertex that is currently being processed            man mesh (see Figure 10) containes about 11k vertices
and vertex B is the target of the edge that the algorithm           with 3 to 5 edges adjacent to each vertex. The cow (see
actually relaxing. Therefore, A stays the same for the              Figure 11) is made up of ca. 3.1k vertices, also con-
vertex that is processed and B changes to each of the ad-           nected to neighbors with 3 to 5 edges. The parametriza-
jacent vertices during relaxation of the adjacent edges.            tions were done in 68 ms for the cow and 224 ms for the
This relaxation leads to updated values of diffusion                human mesh on a Pentium i7-2600 3.40 GHz.
in each adjacent vertex. By updating the value, the                 The parametrization of the whole mesh took place as
process of diffusion iteratively spreads over the whole             a sequence of parametrizations on the list of partial



WSCG 2012 Communication Proceedings                            65                                            http://www.wscg.eu
meshes after the segmentation. Like the original algo-
rithm by Dijkstra our modified approach has a complex-
ity lying in O(|E|log|V |), where |E| is the number of
edges in the mesh and |V | is the number of vertices in
the mesh. We then applied a grid on the mesh, using the
parametrized values as texture coordinates (see Figure
10).




                                                                        Figure 11: Parametrization of another object

                                                                  the cow are longer than the edges on other parts of the
                                                                  mesh. The apparent gap between the isolines shows that
                                                                  the edges on this side are almost double in length. Right
                                                                  behind the right shoulder of the cow there is a loop that
                                                                  continues on the bottom side of the mesh. This loop
                                                                  is part of the isoline that moved downwards due to the
                                                                  larger distance between the vertices on the side.
                                                                  An important aspect is the examination of the bound-
                                                                  aries between the regions. They show irregularities
                                                                  which probably stem from the different parameter val-
                                                                  ues that overlap on these vertices. Regardless of which
                                                                  set of (u, v)-values is chosen, there will always be a
                                                                  residual distortion, even though the algorithm contains
                                                                  mechanisms that guarantee a continuous parametriza-
                                                                  tion across boundaries of segments: Consider a seg-
                                                                  ment pattern that looks like a distorted rectangular grid
                                                                  on the 3D surface, and a boundary segment S between
                                                                  two patches A and B. Let the parameter u be 1.0 along
                                                                  the boundary in A, and 0.0 in B, while v varies from
 Figure 10: The parametrization visualized by a grid              0.0 to 1.0 continuously. Then v is determined by the
                                                                  boundaries (source sets) adjacent to S in A and in B. If
From these pictures we can judge the quality of the               these adjacent boundaries in A and B fit together as one
parametrization. The distribution depends on the mesh             continuous curve, like in a rectangular grid, the param-
structure and it is clearly visible that this leads to an         eter v on S will be the same in A and B, so the patch-
increased jitter of parameter values.                             wise global parametrization is continuous across patch
On the large areas like chest and back of the human               boundaries.
mesh (Figure 10) there is less distortion than on struc-
turally smaller areas. These areas (e.g. the arms and             3.1    Comparison
legs) show a ratio between horizontal and vertical dis-           Based on Alla Sheffer’s work [24], we compared
tribution which is not aspect-preserving.                         our parametrization with other approaches. Table 1
The different length of the edges in a mesh is directly           shows various parametrizations [24]. We added our
visible in the grid. For example, the edges on the side of        parametrization in the last line.



WSCG 2012 Communication Proceedings                          66                                         http://www.wscg.eu
The images show clearly that the algorithm does hardly            5     REFERENCES
prevent distortion. Especially in parts with complex
                                                                  [1]   Yehuda Afek, Noga Alon, Omer Barad, Eran Hornstein, Naama
curvature (e.g. the head and breast of the cow) angular                 Barkai, and Ziv Bar-Joseph. A biological solution to a funda-
distortion increases. Deficiencies on the mesh (e.g. dif-               mental distributed computing problem. Science, Vol. 331, No.
ferent edge lengths, jumps, doubled vertices) further in-               6014:183–185, 2011.
crease the distortion. Adding the evaluation of an error          [2]   Jon Louis Bentley. Multidimensional binary search trees
metric before the parametrization can value the quality                 used for associative searching. Communications of the ACM,
                                                                        18:509–517, 1975.
that can be expected.
                                                                  [3]   Robert Brown. A brief account of microscopical observations
Especially the uniform, harmonic and mean-value                         made in the months of june, july and august, 1827, on the
weighted parametrizations [27, 9, 11] show similarities                 particles contained in the pollen of plants; and on the general
                                                                        existence of active molecules in organic and inorganic bodies.
between different patches on the surface, e.g. the
                                                                        Philosophical Magazine, Vol. 2:161–173, 1828.
breast region of the cow. Algorithms like DCP and
                                                                  [4]   Patrick Degener, Jan Meseth, and Reinhard Klein. An adapt-
LSCM have similar problems preserving areas and                         able surface parameterization method. In 12th International
distances, but are better in preserving angles than our                 Meshing Roundtable, 2003.
algorithm. This is because they sacrifice the distance            [5]   Boris N. Delaunay. Sur la sphère vide. Bulletin of Academy of
preservation in favor of minimized angular distortion.                  Sciences of the USSR, 7:793–800, 1934.
Our algorithm has problems with angular distortion in             [6]   Mathieu Desbrun, Mark Meyer, and Pierre Alliez. Intrinsic pa-
                                                                        rameterizations of surface meshes. Computer Graphics Forum,
specific regions.                                                       Vol. 21:209–218, 2002.
Singularities like the ears of the cow show cycles                [7]   Gianni DiCaro and Marco Dorigo. Antnet: Distributed stig-
that are direct result of the diffusion process. Other                  mergetic control for communications networks. Journal of
parametrizations handle such singularities more grace-                  Artificial Intelligence Research, Vol. 9:317–365, 1998.
ful. Circle patterns [16] and stretch minimizing [21]             [8]   Manfredo P. do Carmo. Differential geometry of curves and
                                                                        surfaces. Prentice Hall, 1976.
approaches excel at these parts.
                                                                  [9]   Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe,
                                                                        Michael Lounsbery, and Werner Stuetzle. Multiresolution anal-
4   CONCLUSION AND FUTURE                                               ysis of arbitrary meshes. In ACM SIGGRAPH, 1995.
    WORK                                                          [10] Michael S. Floater. Parameterization and smooth approxi-
                                                                       mation of surface triangulations. Computer Aided Geometric
In this paper we showed the development of a new ap-
                                                                       Design, Vol. 14, No. 3:231–250, 1997.
proach in parametrization, inspired from nature. Our
                                                                  [11] Michael S. Floater. Mean value coordinates. Computer Aided
approach can lead to a patchwise bijective parametriza-                Geometric Design, Vol. 20, No. 1:19–27, 2003.
tion, which concentrates on local bijectivity. User inter-        [12] Alfred Gierer and Hans Meinhardt. A theory of biological
action makes global bijectivity possible. The main tar-                pattern formation. Kybernetik, 12:30–39, 1972.
geted application is the creation of correspondence be-           [13] Xianfeng Gu, Steven J. Gortler, and Hugues Hoppe. Geometry
tween two objects. Our approach simplifies this by us-                 images. In ACM SIGGRAPH, 2002.
ing a combination of a kd-tree and a linear array named           [14] Igor Guskov, Kiril Vidimce, Wim Sweldens, and Peter
                                                                       Schröder. Normal meshes. In ACM SIGGRAPH, 2000.
uv-map, which stores tuples of (u, v) and provides fast
                                                                  [15] Kai Hormann and Günther Greiner. MIPS: An Efficient Global
and efficient two way searches.
                                                                       Parameterization Method. Vanderbilt University Press, 2000.
The approach is limited by the heavy dependency on                [16] Liliya Kharevych, Boris Springborn, and Peter Schröder. Dis-
the users input. The segmentation process is entirely                  crete conformal mappings via circle patterns. ACM Transac-
controlled by the user as is the assignment of regions                 tions on Graphics, Vol. 25, No. 2:412–438, 2006.
between two parametrized meshes. Here lies further                [17] Aaron Lee, Hugues Hoppe, and Henry Moreton. Displaced
                                                                       subdivision surfaces. In ACM SIGGRAPH, 2000.
potential for improvement. A fully automated segmen-
                                                                  [18] Bruno Lévy, Sylvain Petitjean, Nicolas Ray, and Jérome Mail-
tation method, which leads to reproducible partition and               lot. Least squares conformal maps for automatic texture atlas
comparable results between different, but similar ob-                  generation. In ACM SIGGRAPH, 2002.
jects, would enhance the application of this approach.            [19] Melanie Mitchell. An Introduction to Genetic Algorithms. MIT
We will investigate current and future segmentation al-                Press, Cambridge, 1996.
gorithms for suitability.                                         [20] Ulrich Pinkall and Konrad Polthier. Computing discrete mini-
                                                                       mal surfaces and their conjugates. Experimental Math, Vol. 2,
Our limitation to convex patches is also subject to pos-               No. 1:15–36, 1993.
sible research, as other parametrizations do not depend           [21] Pedro V. Sander, John Snyder, Steven J. Gortler, and Hugues
on convex meshes. By surpassing this limitation it                     Hoppe. Texture mapping progressive meshes. In ACM SIG-
could be possible to completely omit the segmentation                  GRAPH, 2001.
and any user input. We will investigate this, too.                [22] Alla Sheffer and Eric de Sturler. Surface parameterization for
                                                                       meshing by triangulation flattening. In 9th International Mesh-
                                                                       ing Round Table Conference, 2000.
ACKNOWLEDGEMENTS
                                                                  [23] Alla Sheffer, Bruno Lévy, Maxim Mogilnitsky, and Alexander
We want to thank the anonymous reviewers for their                     Bogomyakov. Abf++: Fast and robust angle based flattening.
valuable comments.                                                     ACM Transactions on Graphics, Vol. 24, No. 2:311–330, 2005.




WSCG 2012 Communication Proceedings                          67                                                 http://www.wscg.eu
                                             Distortion mini-
                  Name                                                  Bijektivity        Boundary    Source
                                             mized
                  Uniform                    none                       yes                convex          [27]
                  Harmonic                   angular                    no                 convex           [9]
                  Shape preserving           angular                    yes                convex          [10]
                  Mean-value                 angular                    yes                convex          [11]
                  DCP                        angular                    no                 free             [6]
                  LSCM                       angular                    no                 free            [18]
                  MIPS                       angular                    yes                free            [15]
                  ABF/ABF++                  angular                    yes (only local)   free       [22]/[23]
                  Circle patterns            angular                    yes (only local)   free            [16]
                  Stretch minimizing         distance                   yes                free            [21]
                  MDS                        distance                   no                 free            [29]
                  Adaptable surface          area                       yes                free             [4]
                  Our approach:
                  Diffusion-based            to a degree:
                  distance                   yes (only local)           convex
                                 Table 1: Comparison of parametrizations with our approach

[24] Alla Sheffer, Emil Praun, and Kenneth Rose. Mesh parameteri-
     zation methods and their applications. Foundations and Trends
     in Computer Graphics and Vision, Vol. 2, No. 2:105–171, 2006.
[25] Alan Mathison Turing. The chemical basis of morphogene-
     sis. Philosophical Transactions of the Royal Society of London,
     Series B, Vol. 237, No. 641:37–72, 1952.
[26] Greg Turk. Generating textures on arbitrary surfaces using
     reaction-diffusion. SIGGRAPH Comput. Graph., 25:289–298,
     1991.
[27] William T. Tutte. How to draw a graph. London Mathematical
     Society, Vol. 13:743–768, 1963.
[28] Andrew Witkin and Michael Kass. Reaction-diffusion textures.
     In Computer Graphics, pages 299–308, 1991.
[29] Gil Zigelman, Ron Kimmel, and Nahum Kiryati. Texture map-
     ping using surface flattening via multi-dimensional scaling.
     IEEE Transactions on Visualization and Computer Graphics,
     Vol. 8, No. 2:198–207, 2002.




WSCG 2012 Communication Proceedings                                    68                                  http://www.wscg.eu
   Realistic facial expression synthesis of 3D human
   face based on real data using multivariate tensor
                         methods
          Jacey-Lynn Minoi                             Duncan F. Gillies                   Amelia Jati Robert Jupit
Faculty of Computer Science and                  Department of Computing             Faculty of Computer Science and
    Information Technology                       Imperial College London                 Information Technology
  Universiti Malaysia Sarawak                   180 Queen’s Gate, London,              Universiti Malaysia Sarawak
    94300 Kota Samarahan                             SW7 2RH, U.K.                       94300 Kota Samarahan
        Sarawak, Malaysia                          d.gillies@imperial.ac.uk                  Sarawak, Malaysia
       jacey@fit.unimas.my                                                                  rjajati@fit.unimas.my


                                                         ABSTRACT
This paper presents a novel approach of facial expression synthesis and animation using real data sets of people
acquired by 3D scanners. Three-dimensional faces are generated automatically through an interface provided by
the scanners. The acquired raw human face surfaces went through a pre-processing stage using rigid and non-
rigid registration methods, and then each of the face surface is synthesized using linear interpolation approaches
and multivariate statistical methods. Point-to-point correspondences between face surfaces are required in order
to do the reconstruction and synthesis processes. Our experiments focused on dense correspondence, as well as,
to use some points or selected landmarks to compute the deformation of facial expressions. The placement of
landmarks is based on the Facial Action Coding System (FACS) framework and the movements were analysed
according to the motions of the facial features. We have also worked on reconstructing a 3D face surface from a
single two-dimensional (2D) face image of a person. After that, we employed tensor-based multivariate statistical
methods using geometric 3D face information to reconstruct and animate the different facial expressions.

Keywords
Three-dimensional facial animation, facial expression synthesis, face reconstruction.


1. INTRODUCTION
Facial animation is complex and difficult to achieve                 According to [Fas02], the deformation approach does
realistically. Facial features that contribute the most              not necessarily require extensive facial movement,
to facial expressions are the eyelids, eyebrows and                  which makes the animation process faster and
mouth. Wrinkles and budges also contribute to the                    simpler. However, this approach is unreliable in
change of facial appearances. Movements or the flow                  creating exaggerated realistic face shapes and facial
of features can be measured and then used to animate                 expressions.
facial expressions. This approach is known as                        A large number of facial modelling and facial
feature-based deformation. [Ste81] used landmark                     animation works have employed muscle-based
information to deform face shapes and models while                   approaches [Ter90, Lee95]. Synthetic facial
[Wat87] used pseudo muscles for face expression                      movements are generated by mimicking the
animation. Work by [Gue98, Pig98] used facial                        contraction of facial muscles. This can be done by
movement information.                                                firstly defining the functionality and locations of the
                                                                     facial muscles on the face model and then applying a
                                                                     combination of muscle contractions [Wat87]. The
 Permission to make digital or hard copies of all or part            combinations of the muscles are defined by Action
 of this work for personal or classroom use is granted               Units (AUs) from the Facial Action Coding System
 without fee provided that copies are not made or                    (FACS) framework. Using AUs could reduce the
 distributed for profit or commercial advantage and that             amount of work in characterising facial expression
 copies bear this notice and the full citation on the first          data.
 page. To copy otherwise, or republish, to post on
 servers or to redistribute to lists, requires prior specific        Many face animators, [Fox05] for example, imitate
 permission and/or a fee.                                            facial muscles movements to generate facial
                                                                     expressions. Similar to the prior muscle-based



WSCG 2012 Communication Proceedings                             69                                       http://www.wscg.eu
animation problems, this approach only creates a               optical flow-based tracker to estimate 3D shape
limited set of facial expressions.                             movements. In addition, they used a multilinear
Multi-layer models supplement the use of facial                model to manage the face attributes separately.
muscles. A multi-layer model is built from the                 Theoretically, by using multilinear models on a larger
anatomical structure of the face, facial muscles, skin,        collection of faces with different expressions, faces
soft tissues and etc. [Ter90, Lee95, Sif05, Wil97].            with any expressions can be generated. However, the
[Ter90] proposed facial animation by contracting               collection used in Vlasic‟s work is limited in size.
synthetic facial muscles embedded in a face skin               Nevertheless, the advantage of this technique is that
model. This approach improves the realism of the               visible facial markers or special face-spanning
synthetic facial expressions; however the use of               equipment is not required.
sophisticated biomedical models requires accurate              In order to simulate a realistic facial expression, a
simulation methods and high computational costs                larger collection of facial expression examples is
[Lee95]. Furthermore, animating expressions in                 required. When using muscle information, accurate
complex multi-layer structures requires non-linear             muscle descriptions or templates are needed to
methods to simulate dynamic deformation of the skin.           produce visually correct facial movements. Using
Failing to create a detailed skin deformation (such as         fewer facial expression resources means that
wrinkles) may result in less realistic facial                  expressions may appear artificial and unrealistic.
expressions [Ers08].                                           Methods to optimise the animation computation may
The geometry warping approach is another method                also be needed to allow real-time facial animations. It
for synthesizing facial expressions. The facial                should be noted that facial animation field has grown
expression information is measured from two images             into a complicated and broad subject. Facial
– one with a neutral expression and another with a             animation applications are extensively used in various
particular facial expression. The calculated facial            areas, including movie industries, computer games,
movement difference vectors are transferred to a               medicine and telecommunication.
target image of a neutral face [Pig98, Sha01, Par96,           The remainder of this work is organised as follows.
Wil90]. The facial movement differences can be                 In Section 2, we start with a description of the data
controlled by using linear interpolation. The                  set of 3D face scans from which our synthesis of
disadvantage of this approach is that the overall              facial expression model is built.
shapes of the face, including the geometric shapes,
poses and orientations, and facial expression, are             In Section 3, we briefly present the pre-processing
calculated and computed together. It is therefore not a        technique used on raw 3D face surfaces, followed by
perfect solution for generating the in-between facial          a description on the algorithm for synthesizing and
expressions [Ers08]. Recent work has been                      animating     facial   expressions   using     linear
undertaken to overcome this weaknesses by using                interpolation based on landmark placements, is given
non-linear interpolation or by combining linear and            in Section 4.
non-linear interpolation.                                      In Section 5, we describe PCA and LDA approaches
[Par96] used simple geometric interpolation to                 used in this work and then introduce our idea of
synthesize expression on 3D face models. The feature           applying tensor model to those two approaches on
points are manually digitised on each face model.              generating a variety of facial expressions that can be
This was followed by automatic expression synthesis            applied in differing degrees. Following that, we look
where the data of real actors are captured and                 at reconstructing 3D faces from 2D photographic
analysed [Wil90, Ber85, Ess96]. The captured face              images of faces with only neutral expression and then
surfaces are represented using a structured mesh,              map facial expression onto the reconstructed face
along with texture information.                                surface.
Segmenting face models into smaller regions is also                Section 6 then describes all the experiments that
employed with the aim of synthesizing only the                 were carried out in the study, and present the results
relevant parts of the face contributing to an                  of the synthesis on 3D face data. Finally, in Section 7,
expression. [Jos03] applied this approach on 3D face           we conclude the paper, summarising its main
models. On the other hand, [Bla03] employed a                  contributions and describing possible future work.
morphable technique to animate facial expressions on
existing 2D images and videos. The advantage of the            2. FACE DATA SETS USED
morphable modelling approach is that it can work on            In our experiment, we have used four face data sets of
faces without acquiring examples of facial expression          real human faces: the Notre Dame 3D face data set,
data of a person. [Vla05] mapped facial movements              the Imperial College 3D face data set, SUNY
from a recorded video to a target face using an                Binghampton 3D face data set, and the FERET 2D
                                                               face data set.



WSCG 2012 Communication Proceedings                       70                                        http://www.wscg.eu
The Notre Dame data set was acquired at the                     a landmark point once the correspondence is
Computer Vision Research Lab at the University of               established. In our experiments, correspondence
Notre         Dame         (see       web        page,          between 3D face models was established using the
http://www.nd.edu/~cvrl/CVRL/Data_Sets.html). A                 method proposed by [Pap05]. Pre-processing is
total of 150 subjects participated in the image                 applied to each raw face surface to regularise the
acquisition sessions, giving a total of 300 three-              position, scale and surface tessellation, and to
dimensional face surfaces. The 3D data was captured             establish a correspondence between faces.
using a Minolta Vivid camera [Kon] which uses a
structured light sensor to scan surfaces. The captured          4. SYNTHESIZING EXPRESSIONS
faces are only frontal faces of neutral facial                  USING LINEAR INTERPOLATION
expression.
                                                                BASED ON LANDMARK
In the Imperial College face data set was acquired at           PLACEMENTS
the Department of Computer Science at Imperial
College London [Pap05]. The 3D face surfaces are                In this section, we describe the use of features points
captured using a VisionRT stereo camera [Vis]. It               that gives parameters of facial expression and
contains a set of 60 individual face surfaces of which          deformation. Using these parameters, one can
we used two expressions, one where the subject is               generate any possible facial expressions according to
frowning and another smiling – totalling to 120 three-          the selected landmark points.
dimensional face surfaces. Each face is associated
with greyscale texture image.                                   In our experiment, we selected two sets of
                                                                identifiable landmark points, whereby set 1 has thirty-
In the SUNY Binghampton data sets [Wan06], the                  three landmarks and set 2 has forty-three landmark
3D face surfaces consists of 7 different emotional              points. Figure 1(a) illustrates thirty-three landmarks
facial expressions, namely anger, disgust, happiness,           and they are placed along the eyebrows, the corners
sadness, surprise, fear and neutral. Each of the                of the eyes and crowfeet, the glabella, the upper part
emotional expression contains 4-levels of expression            and the tip of the nose, the mouth and areas around
intensity ranging from low to high. In total, there are         the mouth and lips, the chin and the cheeks. Figure
2,500 faces from 100 subjects.                                  1(b) shows set 2 landmark points. The chosen
                                                                landmark points are based on the landmarks used in
FERET face data set is a well-known 2D standard                 craniofacial anthropometry and muscle-based
face image data set normally used for evaluating face           landmarks in the FACS framework.
recognition     performance.    See     web      page,
                                                                The main source that contributes to expression
http://www.itl.nist.gov/iad/humanid/feret/feret_master
                                                                variation is facial muscular movements. We
.html).                                                         employed a registration framework [Pap05], based on
                                                                the selected facial landmarks, to create expressions
3. PRE-PROCESSING                                               on surface model of the face.
The obtained 3D face surfaces using current
technologies would require pre-processing procedure             The step is similar to the geometric warping
before they can be further analysed. A raw surface              approach, whereby we calculate the facial movement
may have holes or spikes caused by acquisition and              difference vectors from two face surfaces – one face
measurement errors. Raw surfaces also have different            with a neutral expression and another with a
alignment and surface areas in addition to having a             particular facial expression – using landmark points.
different number of points in the surface                       Then, we use linear interpolation to transfer the
representation. The first step in any technique using           generated facial expression to a neutral face.
statistical shape modelling is to normalise the surface
with respect to the orientation and the surface area.
The second step is then to establish point-to-point
correspondence between the surfaces in the input
database.
As a result of these steps, extraneous portions of a
surface are removed so that all the surfaces cover the
same features and are represented by the same
number of corresponding vertices. The dense
correspondence is established so that, ideally, each
point on a surface represents the same anatomical
position on all the other surfaces. Thus, each vertex is                Figure 1(a). A set of 33 selected points.




WSCG 2012 Communication Proceedings                        71                                       http://www.wscg.eu
                                                                 necessarily represent the important discriminant
                                                                 directions to separate sample groups. Therefore, we
                                                                 employ the idea of using the discriminant weights
                                                                 given by separating hyperplanes to select among the
                                                                 principal components the most discriminant ones.
                                                                 Linear Discriminant Analysis (LDA) is computed to
                                                                 separate samples of distinct groups by maximising the
                                                                 ratio of the determinant of the between-class
                                                                 separability to the determinant of the within-class
                                                                 variability. The performance will degrade if there are
                                                                 only a limited number of total training samples N
        Figure 1(b). A set of 43 selected points.                compared to the dimension of the feature space n.
                                                                 This critical issue is the singularity and instability of
                                                                 the within-class scatter matrix. In order to avoid these
5. SYNTHESIZING EXPRESSIONS                                      problems, we propose to use Maximum uncertainty
USING MULTIVARIATE                                               Linear Discriminant Analysis (mLDA) approach. The
                                                                 idea of mLDA is to regularise the eigenvalues. The
STATISTICAL METHODS                                              details of the mLDA method can be found in
In this section, we present another approach to
                                                                 [Tho06].
generate and synthesize facial expressions using
multivariate statistical method based on PCA and
mLDA (Maximum uncertainty Linear Discriminant                    Multivariate statistical method
Analysis). And then we introduce a tensor-based                  The multivariate statistical method is essentially a
multivariate statistical method to construct new face            two-stage approach, the first stage is to characterise a
shapes with a range of different face variations.                type of variation and the second stage is to
Active Shape Model (ASM) [Coo95] is a commonly                   reconstruct faces. The method is used to find the most
used approach to build statistical shape models of the           significant direction of change between two classes,
human face. The modelling of anatomical face                     and to reconstruct and visualize intermediate data
structures are from statistical information found in a           between two classes. This method is based on
training set. Unlike the multivariate statistical method         Principal Component Analysis (PCA) and Maximum
and tensor-based multivariate statistical approaches,            uncertainly Linear Discriminant Analysis (mLDA)
new face shapes are created based on statistical                 separating hyper-plane. This technique was first
information from pre-defined classes of specific                 applied by [Kit06] to extract and characterise the
features or face variations found in the training set.           most discriminant changes between two groups of 2D
                                                                 probed images.
Our idea of using tensor model on the multivariate
statistical method is to use all the face features with a        The initial training set of 3D face data consisting of N
variety of facial variants simultaneously rather than            training examples on n variables is managed by
separating them into two numbers of classes. The                 dividing the training data into two groups or classes,
advantage with this method is that it is practical to            C1 and C2. The training datasets can be projected
generate a variety of face shapes applied in different           from the original vector space of N by n to a lower
degrees. Additionally, the transition between face               dimensional space using a full rank PCA
shapes is also continuous and natural.                           transformation. The principal component space forms
                                                                 an n x m transformation matrix, where m = N – 1.
Following that, we applied the tensor-based method               This step may or may not be necessary to overcome
to the reconstructed 3D faces from 2D photographic               the singularity of the within class covariance matrix.
images and synthesize facial expressions.                        If N ≥ n, then PCA transformation is not required. It
                                                                 is possible that after PCA dimensionality reduction,
                                                                 the within-class scatter matrix Sw may still be less
Revisiting PCA and LDA                                           than full rank. If so, the mLDA approach is used to
Principal Component Analysis (PCA) is one of the                 ensure that the scatter matrix Sw is non-singular.
most successful methods to reduce the dimensionality             Ordering the eigenvectors is not necessary for this
of the original space with a minimum loss of                     process. As there are only two classes, g = 2 and the
information by finding the projection directions that            resulting mLDA is a unidimensional vector of length
maximise the total scatters across all data. However,            m (the linear discriminant eigenvector has dimension
in the covariance structure of PCA, the first principal          m x 1).
component with the largest eigenvalue does not




WSCG 2012 Communication Proceedings                         72                                        http://www.wscg.eu
Back projecting into the original data space, the most          order quantities. The Nth order tensor is written as
discriminant feature is a n x 1 vector. The final step                           , where the IN represents the mode-n
in the first stage is to calculate the mean of each             space. A tensor is then flattened (see [Lat00] for
group and the corresponding variances on the                    flattening details) into a matrix form, An, along any
unidimensional space. This is a very fast computation           dimension n where n = I1, I2, ..., IN.
because we are dealing with one-dimensional data
from two-group classification problem. To                       Starting with a dataset of 3D face surfaces, we
reconstruct these discriminant points (based on                 organise the data in a tensor model according to N
standard deviations and means) on the original space,           varieties of face shapes. In our experiment, the
we do the inverse steps. Once the classifier has been           training set is arranged into a tensor explicitly
constructed, we then extract and project the                    accounts for facial expression variation, where the
discriminant vector. This can be done simply by                 core tensor manages the interaction between the
converting the discriminant to its corresponding n x 1          indices of the 9-mode matrices, (Isubject x Ianger x Idisgust
dimensional face vector.                                        x Ifear x Ihappy x Isad x Isurprise x Ineutral x Ipoints). Next, we
                                                                perform matrix unfolding to retrieve a set of the basis
The final stage of the PCA+mLDA method is the                   matrices for all the 9-mode matrices. Then, we
reconstruction step. If we project the most                     compute the left singular value matrices using
discriminant vector found for the two classes into the          Singular Value Decomposition (SVD) method to
original data space, we will obtain a n x 1 vector.             obtain UN matrices. Each of the UN matrix can be
Moving a point in the original data space in this               thought of as the principal components in each mode,
direction will change the point from an example of              and they may not necessarily be of the same
one class to a maximum likelihood estimate of that              dimension as the tensor. The generalised N-mode
point in the other class. Assuming that the spread of           SVD can be written as follows, and can be interpreted
each class follows a Gaussian distribution, the limits          as a standard linear decomposition of the data
of variation can be set to ±3sdi, where sdi is the              ensemble.
standard deviation of each class i. By moving along
the n x 1 dimensional most discriminant features
based on the mean of each group and the                         An = Un • Dn (Un-1 ○...○ U1 ○ UN ○ ... ○ Un+2 ○ Un+1)T
corresponding standard deviations of each group, the
face shapes according to the class variant can be
reconstructed in the original face domain. As stated            The „○‟ is denoted as Kronecker product [Rao71] and
before, since there is only one dimensional data from           it is applied to compute the product of the matrices.
a two group classification problem, the computation             All the computed I sets of non-zero eigenvalues are
is very fast. Figure 2 shows the geometric overview             extracted and stored differently according to the
of the two-class multivariate statistical approach. This        features of interest and are not ordered.
method is then extended by using tensor model to 3D
                                                                Since we have I-group classification, there are gi=gi-1
face shapes to allow multiple numbers of classes.
                                                                + (i - 1) number of discriminant vectors, where
                                                                                          is the number of group
                                                                classification based on the feature of interest and each
                                                                represent the most expressive features. The resulting
                                                                mLDA now has multiple coefficient vectors in a
                                                                dimension of {g x (m x 1)} depending on the choice
                                                                of facial expression transformations. Next, is to
                                                                compute the second stage of multivariate statistical
                                                                method. Based on the feature of interest, we then
                                                                determine the most discriminant vectors that best
                                                                characterise the particular change in facial features.
                                                                Tensor-based multivariate statistical method is used
      Figure 2. The geometric overview of the                   on the SUNY Binghampton 3D face dataset and the
     multivariate method between two classes.                   reconstructed 3D faces from 2D images of neutral
                                                                faces.

Tensor-based multivariate statistical
method                                                          3D Face Reconstruction
A tensor is a multidimensional matrix or mode-n                 The reconstruction method has four distinct steps,
matrix and is useful for the description of higher              which are: (1) 3D-2D alignment, (2) texture mapping,




WSCG 2012 Communication Proceedings                        73                                               http://www.wscg.eu
(3) illumination adjustment, and (4) shape estimation.        of the synthesis of a smile on 3D human faces using
The detailed methods can be found in [Has07]. First,          33 and 43 landmark points. The results show an
the 2D image, which is to be reconstructed, is                obvious geometric distortion to the overall shapes of
landmarked manually by hand using a set of uniquely           the face (see the distortion on the nose at the bottom
identifiable points with the same landmarks already           row of the figure), and using only 33 landmarks do
known in the 3D statistical shape model. This                 not reconstructed a noticeable smile.
landmarking is used to establish correspondence
between the 2D image and the 3D model, which then
allows mapping of the 2D image texture to the 3D
face surface. The objective of the shape estimation is
to optimize the match between the projection of the
3D shape model and the original 2D image. This is
carried out by adjusting the shape parameters with the
most discriminant vector. Figure 3 illustrates the 2D-
to-3D reconstruction approach.
                                                              Figure 4. Synthesis of a smile using 33 landmarks
                                                                 (top row) and 43 landmarks (bottom row).


                                                              Figure 5 shows an example of an unrealistic face
                                                              shape with a reconstructed smile using higher number
                                                              of landmark points, which are placed mainly around
                                                              the cheek. This shows that the number and the
                                                              placement of landmark points on a face are critical to
                                                              produce a realistic facial expression.
                                                              The disadvantage of using the linear interpolation
                                                              approach is that the overall geometric shape of the
                                                              face and the facial expression are computed together.
                                                              Furthermore, there is no way to generate the in-
                                                              between facial expression to allow a smooth synthesis
                                                              and animation of facial expressions.




      Figure 3. The overall process of 3D face
     reconstruction (adapted from [Has07]).
Here, we look at reconstructing 3D faces from 2D
photographic images of faces with no facial
expression and then generating realistic human facial
expressions. In this case, frontal images from FERET
and Notre Dame Face databases are used for                        Figure 5. (a) Original smile; (b) Reconstructed
evaluating facial expression synthesis on unseen                                caricature smile.
subjects.

6. EXPERIMENTS AND RESULTS                                    In the second experiment, we employed tensor-based
Three facial expression synthesizing and animation            multivariate statistical method to the SUNY
experiments with three different face data sets were          Binghampton face data set. We compared the output
performed.                                                    of the synthesis with ASM.
                                                              The results of the reconstructed facial expressions
In the first experiment, we selected two sets of
                                                              using the most expressive features captured using
identifiable landmark points and then employ linear
                                                              ASM is as illustrated in Figure 6. The reconstructed
interpolation method. Figure 4 compares the results



WSCG 2012 Communication Proceedings                      74                                      http://www.wscg.eu
faces are restricted by limiting the change in each
principal component to ±3√{λi}, where λi are the
corresponding largest eigenvalues. The first mode
describes the vertical stretch along the centre of the
face. The second mode models the variations in the
horizontal direction. The third mode captures
variation around the mouth and cheek to create
expression changes from distorted frowning
expression to a caricature smile.
Figure 7 illustrates the facial expression
transformations of the first four largest principal
components captured by ASM on the Imperial
College data set. The first mode describes the
horizontal stretch around the cheek and mouth. The
second mode models the variation in the vertical
direction. The third mode captures variations around
the nose and eyes areas. The fourth mode captures the
horizontal variation of the geometric shape of the
face.                                                                Figure 7. Synthesis of facial expressions
                                                              reconstruction using the most expressive principal
    By examining Figure 6 and Figure 7, we see that
                                                                components captured by ASM using Imperial
the face shapes are not properly grouped according to
                                                                            College face data set.
facial expressions. The changes of face shape are
global to the data set which makes it impossible to
synthesize individual facial expression. Thus, ASM
                                                              Figure 8 shows the results of the reconstruction for a
method is not suitable to capture specific facial
                                                              neutral and an angry face expression using the tensor-
expression variations.
                                                              based multivariate statistical method. Figure 9
                                                              illustrates the synthesis of facial expressions between
                                                              a surprise and a frowning face.
                                                              This method is effective to capture facial expressions
                                                              variation and it is able to find the most characteristic
                                                              direction of change involved in an expression. This
                                                              magnitude of change can be controlled by a single
                                                              scalar magnitude. We explore the reconstruction and
                                                              synthesis of face shapes by moving the point from
                                                              one side of the dividing hyper-plane to the other,
                                                              respecting the limits of the standard deviation and the
                                                              measured mean of each sample group.




                                                                Figure 8. Synthesis from a neutral to an angry
                                                                expression using most characteristic direction
       Figure 6. Synthesis of facial expressions               captured by tensor-based multivariate statistical
reconstruction using the most expressive principal                                 method.
         components captured by ASM.




WSCG 2012 Communication Proceedings                      75                                        http://www.wscg.eu
                                                                Figure 11. The reconstruction and synthesis of the
                                                                  most characteristic component when using 3D
                                                                   Notre Dame faces along the smile and frown
                                                                                   expressions.


                                                                Examining Figures 8 to 11, we can clearly see that
                                                                the tensor-based multivariate statistical approach
                                                                effectively extract the 3D facial expression changes.
   Figure 9. Synthesis of a surprise to a frowning
                                                                In fact, this approach is also able to generate a
expression using the most characteristic direction
                                                                gradual change on facial expressions that is not
captured by tensor-based multivariate statistical
                                                                explicitly present in the training data sets.
                     method.
                                                                7. CONCLUSIONS
In the third experiment, we work on synthesizing                This paper describes 3D facial expression animation
realistic facial expressions on a reconstructed 3D real         using real human faces. We have analysed the
human face given only a single frontal 2D face                  placement of landmarks based on FACS for
image. We tested the technique using the 2D FERET               deformation and synthesis of facial expressions.
face data set. Figure 10 shows the reconstructed 3D             Unfortunately, landmark-dependent may not create
face shapes from faces taken from FERET face                    realistic facial expressions. We introduce another
images and the synthesized facial expressions to the            method, which is the multivariate statistical method,
reconstructed 3D face shapes. The fourth column                 which differs from many other synthesizing and facial
from the left of the figure displays the original faces.        expression animation approaches in terms of using
As we move from the original to the left side of the            the whole face data points instead of selecting feature
figure, a range of smiling expressions is generated.            points or landmarks on the face for shape variations.
Similarly, when we move to the right side of the                This approach could extract facial expression
figure, a range of frowning expressions is generated.           characteristic discriminant information efficiently,
Having texture embedded to the 3D face surfaces                 providing a gradual transformation on the 3D faces.
makes the expression change smoother and more                   The strength of this work is the realism of the facial
visible. For example, the raised cheeks and eyebrows,           expression generated as we use and extract only
and the opened mouth show a smile.                              facial expressions from real human faces. We could
Figure 11 shows the synthesis and animation of facial           also generate facial expressions at varying intensities
expressions on the Notre Dame 3D face data set,                 for a subject without prior examples of expression.
given that Notre Dame 3D faces only contain neutral             The concept of using PCA+ mLDA approach to
face (the fourth column from the left of the figure).           discriminate pattern of interest is not new. However,
                                                                in the work of real human 3D faces, synthesizing and
                                                                analysing facial expression is still in its preliminary
                                                                stage. We have also implemented to using tensor
                                                                model to extend the two-class problem to several
                                                                classes.

                                                                8. ACKNOWLEDGMENTS
                                                                The authors would like to thank Prof. Carlos Thomaz
   Figure 10. The reconstruction of smiling and                 for his technical advices, the referees for their
     frowning expressions using tensor-based                    constructive comments which helped to improve this
        multivariate statistical approach.                      paper, and Universiti Malaysia Sarawak (UNIMAS)
                                                                for the travelling fund.




WSCG 2012 Communication Proceedings                        76                                       http://www.wscg.eu
9. REFERENCES                                                        [Ess96] I. Essa, S. Basu, T. Darrell, and A. Pentland.
[Ste81] Stephen M. Platt and Norman I. Badler.                          “Modeling, tracking and interactive animation of faces
    “Animating facial expressions.” SIGGRAPH Computer                   and heads using input from video.” IEEE International
    Graphics, 15(3):245–252, 1981.                                      Conference on Computer Animation, 00:68, 1996.

[Wat87] K. Waters. “A muscle model for animation three-              [Jos03] P. Joshi, W.C. Tien, and M. Desbrun. “Learning
   dimensional    facial    expression.”  International                  controls for blend shape based realistic facial
   Conference on Computer Graphics and Interactive                       animation.”   ACM      SIGGRAPH       Eurographics
   Techniques, pages 17–24, 1987.                                        Symposium on Computer Animation, pages 187–192,
                                                                         2003.
[Gue98] B. Guenter, C. Grimm, D. Wood, H. Malvar, and
   F. Pighin. “Making faces.” Annual Conference on                   [Bla03] V. Blanz, C. Basso, T. Poggio, and T. Vetter.
   Computer Graphics and Interactive Techniques, pages                  “Reanimating faces in images and video.” Annual
   55–66, 1998.                                                         Conference of the European Association for Computer
                                                                        Graphics, 22(3):641–650, 2003.
[Pig98] F. Pighin, J. Hecker, D. Lischinski, and R.
    Szeliski. “Synthesizing realistic facial expressions from        [Vla05] D. Vlasic, M. Brand, H. Pfister, and J. Popovic.
    photographs”. In SIGGRAPH Computer graphics,                        “Face transfer with multilinear models.” ACM
    pages 75–84, Orlando; FL, 1998. ACM Press.                          Transactions on Graphics, 24(3), 2005.

[Fas02] B. Fasel and J. Luettin. “Automatic facial                   [Kon] Konica Minolta, Vivid 910. [Online].
   expression analysis: a survey”. IDIAP Research Report             Available: http://www.minoltausa.com
   99-19, 2002.                                                      [Pap05] T. Papatheodorou and Daniel Rueckert.
[Ter90] D. Terzopoulos and K.Waters. “Physically-based                  “Evaluation of 3d face recognition using registration
   facial modeling, analysis and animation”. Journal on                 and PCA.” In Takeo Kanade, Anil K. Jain, and Nalini
   Visualisation and Computer Animation, 1(2):73–80,                    K. Ratha, editors, AVBPA, volume 3546 of Lecture
   1990.                                                                Notes in Computer Science, pages 997-1009. Springer,
                                                                        2005.
[Lee95] Y. Lee, D. Terzopoulos, and K. Walters. “Realistic
   modeling for facial animation”. In SIGGRAPH on                    [Vis]       VisionRT.          [Online].        Available:
   Computer Graphics and Interactive Techniques, pages                  http://www.visionrt.com
   55–62, New York, NY, USA, 1995.                                   [Wan06] J. Wang and L. Yin and X. Wei and Y. Sun.
[Fox05] B. Fox. “Barrett fox character animator”.                       “Facial expression recognition based on primitive
   Internet: http://www.barrettfox.com/, 2005.                          surface feature distribution.” in the IEEE International
                                                                        Conference on Computer Vision and Pattern
[Sif05] E. Sifakis, I. Neverov, and R. Fedkiw.
                                                                        Recognition (CVPR 2006), New York, 2006.
    “Automatic determination of facial muscle activations
    from sparse motion capture marker data.” ACM                     [Coo95] Cootes, T., Taylor, C.J., Cooper, D.H., Graham, J.
    Transactions Graphics, 24(3), 2005.                                 “Active shape models - their training and application.”
                                                                        Computer Vision and Image Understanding, 61(1):
[Wil97] J. Wilhelms and A.V. Gelder. “Anatomically
                                                                        3859. 1995.
   based modeling.” Annual Conference on Computer
   Graphics and Interactive Techniques, 1997.                        [Tho06] C.E. Thomaz, E.C. Kitani and D.F. Gilles: “A
                                                                        Maximum       uncertainty LDA-based approach for
[Ers08] N. Ersotelos and F. Dong. “Building highly
                                                                        limited size problems with applications to face
    realistic facial modeling and animation: a survey.” The
                                                                        recognition.” Journal of the Brazilian Computer
    Visual Computer, 24(1):13–30, 2008.
                                                                        Society, Vol 12(2), pg 7-18, 2006.
[Sha01] A. Shashua and T. Riklin-Raviv. “The quotient
                                                                     [Kit06] Kitani, E.C., Thomaz C.E., Gillies, D.F. “A
   image: class-based re-rendering and recognition with
                                                                         Statistical discriminant model for face interpretation
   varying illuminations.” IEEE Transactions on Pattern
                                                                         and reconstruction.” In: 19th Brazillian Symposium on
   Analysis and Machine Intelligence, 23(2):129–139,
                                                                         Computer Graphics and Image Processing. 2006.
   2001.
                                                                     [Lat00] L.D. Lathauwer, B.D. Moor, and J. Vandewalle.
[Par96] F.I. Parke and K. Waters. “Computer facial
                                                                        “A multilinear singular value decomposition”. SIAM
   animation”. A K Peters, 1996.
                                                                        Journal on Matrix Analysis and Applications,
[Wil90] L.     Williams.    “Performance-driven     facial              21(4):1253–1278, 2000.
   animation.” Annual Conference on Computer Graphics
                                                                     [Rao71] C.R. Rao and S. Mitra. “Generalised inverse of
   and Interactive Techniques, pages 235–242, 1990.
                                                                        matrices and its applications”. Wiley New York, 1971.
[Ber85] P. Bergeron and P. Lachapelle. “Controlling
                                                                     [Has07] S. Hassan Amin and Duncan Gillies.
   facial expressions and body movements in the
                                                                        “Quantitative analysis of 3d face reconstruction using
   computer-generated animated short "tony de peltrie".”
                                                                        annealing based approach”. In: IEEE International
   In ACM SIGGRAPH Advanced Computer Animation
                                                                        Conference on Biometrics: Theory, Applications, and
   seminar notes, 1985.
                                                                        Systems,     BTAS       2007,    pg     1–6,     2007.




WSCG 2012 Communication Proceedings                             77                                          http://www.wscg.eu
WSCG 2012 Communication Proceedings   78   http://www.wscg.eu
Procedural generation of meandering rivers inspired
                    by erosion
                                                    Michał Kurowski
       Warsaw University of Technology, the Faculty of Electronics and Information Technology
                                          Pl. Politechniki 1
                                     00-661, Warsaw, Poland
                                               M.Kurowski@ii.pw.edu.pl



                                                     ABSTRACT
This paper describes a method of procedural generation of meandering rivers inspired by erosion, which can
enhance visual realism of virtual terrains. Terrain is represented using an adaptively subdivided triangle mesh
with additional information (e.g. amount of soft deposit) stored in vertices. Water is simulated using Smoothed
Particle Hydrodynamics (SPH), modified in order to model erosion occurring within meanders. Most
experiments were performed on an initially flat terrain, so in order to provide the initial disruption of an
otherwise straight flow, a simple force simulating an exaggerated Coriolis effect was introduced.
Keywords
Computer graphics, procedural terrain generation, meanders, Smoothed Particle Hydrodynamics, erosion.
1. INTRODUCTION                                                    for low cost and low development time instead of
Creation of “virtual worlds” used in computer games,               high complexity within an acceptable budget.
simulations, movies or art requires a significant                  Terrain is present in many “virtual worlds” and often
amount of various content, including, but not limited              strongly influences the final product: beautiful
to, textures, object models, sounds and terrains. This             landscapes create atmosphere in movies and
content can be either handcrafted by skilled                       complement the action, while maps for strategy
professionals or generated procedurally. This                      games decide the gameplay style. A complete and
distinction is not very sharp, as various packages                 practical solution should be able to produce a
offer the ability to combine both approaches (e.g.                 visually complex scene in an acceptable time. This
map editor in “Earth 2150” game), by allowing their                scene should contain features desired by an artist or a
users to procedurally generate some elements of the                level designer. Unfortunately, pure procedural
content (e.g. fractal terrain for use in a Real-Time               approaches are often either hard to control or produce
Strategy game) and then manually combine and                       results which look artificially. Methods based on
tweak them to match specific requirements (in case                 physical simulations are often more intuitive and
of the previous example: create a level patch of                   easy to integrate with other solutions, but they are
terrain for player’s base or place resources and                   also either very slow or too simplified to produce
bridges). The growing computational power of                       certain phenomena, such as for example meanders.
devices in the hands of end-users is followed by their             This paper describes a method that can be used to
growing expectations for visual quality, which is                  enhance an existing terrain model (created
often linked to visual complexity [Mus02]. This in                 procedurally or manually) with meandering rivers,
turn leads to the growing amount of required work,                 thus introducing some amount of physically inspired
which translates to development time, staff size and               realism. The presented heuristic solution produces
budget size, making the procedural approaches more                 meanders by eroding the outer river banks
noteworthy. Automatic generation is already very                   horizontally and depositing the eroded material
attractive to indie game developers who are aiming                 mainly near the inner banks. It uses SPH for water
Permission to make digital or hard copies of all or part of        simulation and an adaptive triangle mesh for terrain
this work for personal or classroom use is granted without         representation. The most important contributions of
fee provided that copies are not made or distributed for           this work are: the introduction of separate material
profit or commercial advantage and that copies bear this           particles, simulating an exaggerated Coriolis effect to
notice and the full citation on the first page. To copy            initiate the meandering and using a low number of
otherwise, or republish, to post on servers or to                  particles to represent water in order to achieve
redistribute to lists, requires prior specific permission          acceptable performance. The resulting method can be
and/or a fee.
                                                                   used interactively by an artist or a level designer, can



WSCG 2012 Communication Proceedings                           79                                        http://www.wscg.eu
take into account an existing terrain topography and            the lowest neighbor until they find another body of
can be further enhanced to allow for different                  water. If a section of the river is found to be gently
material properties.                                            sloping, meanders are generated as an alternating
                                                                curve defined by “wavelength”, which is proportional
2. RELATED WORK                                                 to the flow of the river and “meander angle” set by
Due to the practical applications of procedural terrain         the user.
generation many different methods were proposed. In
[Mus02] Musgrave covers terrain synthesis using                 In a more recent work [Teo09] the same author
multifractals. Various erosion based approaches were            proposed another approach in which ridge lines are
presented in [Nei05], [Sta08] and [Kel88]. [Ben06a]             created by the user, while the river network is grown
by Benes is a short paper focused on hydraulic                  inward into the land mass from randomly placed
erosion introducing some problems that need to be               river mouths. Each river is generated by adding
addressed: scene size, simulation speed, user input,            consequent, randomly rotated segments and then
interactivity and covering of different scales. Some            fitting an alternating curve through them. In order to
of these issues are addressed by a GPU based method             achieve varied terrain, different rivers have different
introduced in [Val11], which uses an adaptively tiled           “SegmentLength”          and      “MeanderCurvature”
“virtual layered terrain” together with pipe model for          parameters. The heightmap is then created to
water simulation and allows interactive editing. The            accommodate the generated ridges and rivers.
authors of [Cen09] present automatic terrain                    The authors of [Ben06b] proposed a method based on
generation based directly on user’s sketches, while             the Navier-Stokes equations and a regular voxel grid.
Zhou, Sun and Turk propose a method which                       In one of their experiments they simulated a riverbed
combines a “feature map” sketched by the user with              with meanders flooded by a wave. The simulation
example data from a digital elevation model [Zho07].            produced excellent results, with the river eroding the
A similar idea, where a user specifies terrain                  outer banks, breaking through the meanders and
primitives which are then matched against a database            leaving two billabongs. However, the algorithm has
of “terrain units” manually extracted from real-world           high computational cost and is unsuitable for
elevation data is presented in [Chi05]. Brosz [Bro06]           simulating large terrains (the experiment was
proposes a method of enhancing user created model               conducted on a 120x32x120 grid). Also, the initial
with high-resolution details extracted from another             meanders were not created during the simulation.
terrain. A fairly recent general overview of                    A recent work on hydraulic erosion using CUDA for
procedural techniques for creating 3D environments              acceleration [Bez10], while not dealing with
can be found in “A survey of Procedural Methods for             meanders, has some similarities with the method
Terrain Modelling“ [Sme09].                                     presented in this paper. Both use particles for water
The problem of meandering rivers is rarely                      simulation and adaptive triangle meshes for terrain.
addressed. One of the exceptions is work by                     However, the solution proposed by Bezin performs
Prusinkiewicz and Hammel [Pru93], where a river                 the subdivision during an off-line pre-processing and
modeled using a squig curve is incorporated into a              constrains the movement of vertices to the vertical
mountainous terrain created using midpoint-                     axis (the authors use the term “adaptive heightfield”).
displacement algorithm. The process involves
recursive subdivision of the triangles within the               3. PROCEDURAL MEANDERS
terrain’s mesh and classification of the triangles’             Proposed Mechanism
edges into river entry, river exit and neutral ones. The        Flow of water within a meandering channel is a
proposed solution produces complex fractal terrains,            complex 3D phenomenon which can be resolved into
but lacks some realism: the river flows on constant             a primary downstream and a secondary transverse
altitude in an asymmetric valley. The authors also              components [Nal97]. Conducted experiments
mention certain issues with tributaries which are not           [Ram99] show that these components change with
solved in the article.                                          depth, distance from banks and along the length of
                                                                the meander. An exact physically correct simulation
Belhadj [Bel05] proposed an enhanced midpoint-
                                                                (like [Ben06b]) on a large scale would require a
displacement algorithm which is constrained by a
                                                                substantial amount of computations. On the other
pre-computed set of ridge lines and a river network.
                                                                hand, a simple curve fitting algorithms (like in
The network is created by tracing and combining the
                                                                [Teo08]) do not take into account local terrain
trajectories of randomly seeded “river particles”. The
                                                                variations. This paper proposes a simple erosion
results look promising and contain meander-like river
                                                                inspired heuristic algorithm, which is less complex
paths, but their origin is not presented in the paper.
                                                                than an exact simulations, but can also accommodate
Teoh [Teo08] presented a different approach in                  these variations and produces plausible results.
WaterWorld. Rivers can be “seeded” by the user and
                                                                The presented method takes into account both
then grown downward cell-by-cell in the direction of
                                                                erosion and deposition. The “erosion force” is



WSCG 2012 Communication Proceedings                        80                                       http://www.wscg.eu
                                                               experiments in order to avoid issues with self-
                                                               deepening or oscillating bed, which occurred in
                                                               places where water was semi-stationary. However,
                                                               further research into the mechanics of erosion
                                                               revealed the existence of “critical shear stress”. The
                                                               introduced threshold is applied to speed, not the
                                                               actual shear stress, but as both parameters are
                                                               correlated, it serves the same purpose.
                                                               The emergence of meanders is the result of both the
                                                               horizontal erosion of the outer bank and the
                                                               deposition of sediment near the inner bank. In order
                                                               to facilitate the latter process, the eroded material
                                                               carried by water is attracted toward the inside of the
                                                               meander (Figure 1). The strength of this attraction is
                                                               user defined.
                                                               Both erosion and deposition create a positive
                                                               feedback loop which causes the growth of meanders,
                                                               but also requires some initial curvature. This
                                                               curvature can be supplied by the shape of the terrain
Figure 1. Attraction of the sediment to the inner              or by an external force. In one of his speeches,
banks of curves within the river channel. Green                Einstein [Ein26] mentioned the role of Coriolis force
particles – water; violet particles (with vectors) –           in the non-uniform distribution of velocities within a
sediment.                                                      flowing river. The magnitude of this force is
                                                               dependent on the Earth’s rotation speed, the current’s
proportional to the speed of water multiplied by its           velocity and the geographical latitude. In order to
amount       and a “directional factor”, which is              reduce the amount of user-controlled parameters, the
interpolated from 0 at the inner bank of the meander           presented method assumes that the changes of
to 1 at the outer bank. The amount of material that            latitude are negligible within the simulation’s
can be potentially eroded      depends on the , the            domain. This leads to a simplified formula, which is
area     for which      is calculated, the material’s          intuitively consistent with Baer’s law and produces
softness and a user specified multiplier :                     satisfactory results:


The actual amount of eroded material      is                                             [ ]       ,
clamped so that water is not oversaturated with
sediment. If it weren’t for this condition, the                where is the force,       is a user defined factor, is
multiplication and the consequent division by                  the velocity of water projected onto the X axis and
wouldn’t be necessary. Direction in which eroded                   is the mass of water to which the force is applied.
terrain is displaced consists of two components:
                                                               Water Simulation
downward      , which is constant, and horizontal [ ],         The presented method requires the water simulation
                                                               to provide not only the speed and mass of water in
pointing at the outer bank. The magnitude of [ ] is            certain areas, but also information whether the stream
equal 0 where water flows straight and grows to a              is curving and in what direction. The process of
user defined value when the flow is curved. The                meandering depends on small, smooth variations in
terrain’s displacement is calculated as follows:               the direction of river flow, which should not be
                                                               dampened in time. For performance reasons, the
                                                               simulation should be conducted only in areas
                             [   ]                             containing water, which usually occupy a fraction of
                                                               the entire domain.
The “directional factor” ensures that the outer bank is        Existing water simulation methods can be divided
                                                               into Eulerian (operating on meshes or fixed grids)
eroded more than the inner one and [ ] introduces
                                                               and Lagrangian (operating on particles). Although
lateral erosion, thus ensuring growth of the meander.          hybrid methods exist and are used in computer
It should be noted, that the erosion occurs only when          graphics, erosion simulation usually employs pure
water speed is greater than a certain user-defined             approaches ([Ben06b] and [Ben09]). The Eulerian
amount. This measure was introduced during early               methods usually produce excellent, physically correct




WSCG 2012 Communication Proceedings                       81                                       http://www.wscg.eu
results, but are computationally expensive, especially            river representation using the lowest possible amount
when applied to large 3D domains. They also reveal                of particles and requires the sediment to be attracted
a tendency to dampen movement which isn’t aligned                 towards the inner banks, so that the center of its
with the used grid. There are works which solve                   concentration may not overlap with the center of
some of these problems by introducing non-regular                 water mass. For this reason the presented solution
adaptive meshes or additional particles, but they are             uses separate sediment particles that are influenced
usually complicated. Interaction with solid                       by the flow of water, gravity and attraction to the
boundaries is either complex or requires high                     insides of the meanders.
resolution. On the other hand, even simple                        Each particle’s speed       is calculated as:
Lagrangian methods do not suffer from anisotropy
and can be easily optimized for large, sparse
domains. Current consumer hardware is capable of
simulating tens of thousands of particles at                                                                      [ ]      ,
interactive rates using GPGPU [Gos10]. Physical
accuracy of the simplified methods doesn’t match                  Where          is the particle’s speed in the next
that of the Eulerian ones, but it is usually good                 simulation step,       is the particle’s speed in the
enough for applications in computer graphics.                     current simulation step, is the gravity vector, is
The presented paper uses Smoothed Particle                        the strength of repulsion from the terrain, is a user
Hydrodynamics, which was introduced in [Mul03]                    defined “acceleration factor”,     is a user specified
and combines relative simplicity with great                       attraction factor,     is the average speed of the
flexibility. The characteristic feature of this method            surrounding SPH particles and is the time delta
is that certain quantities (e.g. pressure) defined at             between the simulation steps.
discrete particle locations can be evaluated also in              If new sediment is added to the simulation, the
their neighborhoods as continuous values. This is                 nearest sediment particle is searched for. If such
achieved by accumulating contributions from                       particle is not found, then a new one is created. If the
individual particles weighted by radial symmetrical               search is successful, a certain amount of the sediment
smoothing kernels. The SPH particles used in this                 is added to the existing particle, so that the capacity
paper are enhanced with additional quantities for use             of the particle is not exceeded. The amount that could
only in the erosion algorithm – scalar “curve                     not be added is returned to the erosion algorithm and
accumulator” and vector “curve direction” . is                    is used to clamp the          value introduced earlier.
defined initially as 0 for each particle. If a particle’s         This stops erosion when water is oversaturated with
horizontal velocity vector changes its direction more             eroded material. However, the oversaturation can
than a certain value,        is increased or decreased            occur if multiple particles concentrate in a small
depending on whether the velocity was changed to                  volume (there are no collisions between them, so
the left or to the right, while     is set to left. If the        they can be arbitrarily close to each other). In this
absolute value of          is greater than a certain              case the sedimentation process is initiated to get rid
threshold, the stream is assumed to be curving in the             of the excess material. The sedimentation also occurs
direction defined by        . This value is then used as          as the particle ages, when it loses contact with water
[ ]. The performance of SPH method relies on fast                 (which rarely happens) or its speed is lower than a
                                                                  threshold     , calculated based on user-controlled
finding of neighboring particles. A simple regular 2D             parameter        and the distance from the center of
grid containing indices of particles within a certain             water mass :
volume is used for this purpose.
                                                                                          (        ‖        ‖)
Sedimentation
Eroded soil is carried with water as sediment. In                 where       is the particle’s position and       is the
other works using SPH this information is usually                 smoothing radius of the SPH simulation. This
embedded in water particles. The sediment flow and                formula is purely heuristic and was developed by
concentration is influenced not only by the water                 experimentation. The situation in which the sediment
velocity, but also for example by gravity and                     particle leaves water is undesired, so it’s movement
diffusion. The authors of [Ben09] solve this problem              is restricted to the volume within       of the nearest
using a donor-acceptor scheme, where the movement                 SPH particle. This solution works most of the time,
of imaginary sediment particles is simulated by                   but breaks if water velocity changes rapidly.
material transfer between water particles. This                   However, these rare stray sediment particles do not
approach produces realistic results, but depends on a             seem to introduce any problems to the simulation.
large amount of particles and allows the sediment                 For performance reasons, if two particles are close
concentration to be defined only in and between                   enough and do not exceed the saturation limit, they
them. The method described in this paper strives for



WSCG 2012 Communication Proceedings                          82                                           http://www.wscg.eu
are merged together into a larger one. The search for
neighbors in a given area is accelerated by a 2D grid
identical to the one used in water simulation.
Terrain Representation
The author’s first experiments with meandering were
conducted on a commonly used heightmap
(Figure 3). The directional feature of the erosion was
simulated by eroding two points at the same time –
the one where the erosion parameters were calculated
and its neighbor in the direction of the erosion. The
meanders started to form, but their evolution ended
fast. When the elevation difference between the river
bed and its bank increased, the simplified lateral
erosion was less effective, while the sedimentation
performance was constant. This lead to silting and
overflowing of the bed. A better terrain
representation was required.
Further experiments were conducted using
                                                                Figure 2. Terrain represented using a triangle
“deformable voxels” introduced by the author in
                                                                mesh. The river channel is adaptively subdivided.
[Kur11a]. The initial results were encouraging, but as
it turned out, it was difficult to maintain coherence of        moved closer together. This results in an
the grid. An attempt to refine the terrain                      oversampled mesh and while it doesn’t pose any
representation was made. However, before a                      problems to the correctness of the simulation, it can
satisfactory solution was found, rising computational           significantly reduce the performance. Smooth
complexity made the idea impractical and without                merging of triangles without causing sudden changes
much room for optimization.                                     in terrain geometry is yet to be implemented in this
                                                                solution. The mesh can become degenerated when
Because of the difficulties with voxel representations,
                                                                two surfaces get close enough to penetrate each
a polygonal mesh was used. The collision surface is
                                                                other. This issue was solved in [Mul09], but can be
defined by triangles, while the properties of the
                                                                difficult in case of an almost unconstrained mesh
terrain are stored within vertices, which are displaced
                                                                with arbitrary resolution used in the presented paper.
by erosion and sedimentation. Triangles are
                                                                A simple solution requires the previous problem to
adaptively subdivided (Figure 2) in order to ensure,
                                                                be addressed first. However, this degeneration seems
that no water particle can touch a triangle without
                                                                to occur only in the last stages of a meander’s
influencing the erosion of at least one of its vertices.
                                                                evolution, so while it certainly limits the simulation,
One of the obvious advantages of such a mesh is that
                                                                it is still possible to achieve decent results.
any surface can be easily represented. There are
                                                                Additional issues occur if the vertices defining
however some important issues to deal with. If some
                                                                consequent triangles on the river bed are at
vertices are moved further apart, then some others are




Figure 3. Evolution of a meander on a heightmap terrain (green particles – water; violet particles –
sediment; green terrain – deposition; red terrain - erosion). The initial bends are due to the simulated
Coriolis force and shape of the terrain. The positive feedback loop ensures the growth of the meander,
which is unfortunately slower and slower, until it stops completely. It should also be noted, that the bend
closest to the source is the most prominent, which is undesirable.




WSCG 2012 Communication Proceedings                        83                                       http://www.wscg.eu
significantly different distances from the flow of             needed to implement and test various approaches
water. This results in irregular erosion, which                without compromising the introduced separation.
produces uneven bed surface which significantly                Water sources and sinks can be added, modified or
slows the flow of water, creating unwanted small               removed before and during the simulation. SPH,
ponds. One solution is to introduce denser                     erosion and deposition parameters can also be
subdivision, which results in poorer performance.              adjusted in run-time.
Other option which was also used was to perform
smoothening of the eroded terrain after each 10                5. RESULTS
simulation steps.                                              First experiments were conducted in an artificial
The amount of eroded or deposited material is                  valley with cross-section in the shape of a flattened
proportional to the magnitude of the displacement              sinusoid. Meandering in such a terrain occurs
and the area supporting the displaced vertex. The              naturally if the stream of water is not perfectly
deposition is always upwards and the erosion is                aligned with the valley. However, many rivers
assumed to be mainly downwards. Therefore the area             meander on flatlands and so further experiments were
is calculated as a sum of areas of all the triangles to        conducted on a completely flat surface (4096 x 768
which the given vertex belongs, projected on the               units in size) surrounded by barriers from 3 sides. In
horizontal plane.                                              order to enforce the flow of water, the surface was
                                                               tilted towards the side without any barrier. An
Usually a hard bedrock or cohesive soil is harder to           artificial source producing one particle per second at
erode than fresh sediment. In order to take into               a random position within a radius of 10 units was
account this feature, each vertex stores the thickness         placed on the top. Water and sediment that fell from
of a “soft layer”. It indicates what part of the vertex        the surface after reaching its lower end was removed
displacement is due to the accumulation of material            by an artificial sink. The created system was thus
that was eroded elsewhere. If this value is larger than        open. The CM factor regulating the force induced by
0, we assume that we are eroding soft, fresh                   Coriolis effect was set to 0.005. SPH radius was 25
sediment. The introduced earlier material softness             units. Other user-controlled variables (over 20 in
of the base terrain is set to 1, while of this layer is        total, their complete description is beyond the scope
controlled by the user. While the thickness of the soft        of this article) were different in consequent
layer is increased by deposition, it is decreased both         experiments and sometimes tweaked in run-time.
by erosion and time. The latter is introduced to
simulate the hardening of the sediment into rock. The          The simulation contained usually around 300 SPH
soft layer prevents sedimentation in places which are          particles and a similar - sometimes slightly larger -
being constantly eroded and facilitates the transport          amount of sediment particles. One step of the
of the material.                                               simulation took around 0.02s on Intel Core 2 Quad
                                                               2.66 GHz with 4GB of RAM. The approximate time
In order to facilitate the performance of adaptive             spent in different subsystems is as follows: drawing ~
subdivision, area calculation and collision detection,         1%, erosion ~ 25%, adaptive subdivision of terrain ~
vertices are indexed in a 3D grid and contain                  5%, SPH simulation ~ 30% (~25% is spent in
information about all their neighbors and all the              calculating collisions with ground), sediment
triangles they belong to.                                      transport and deposition ~30%. Precise percentages
4. SIMULATION SOFTWARE                                         depend strongly on the state of the simulation.
The simulation software was written in C++ and uses            The artificial river bends, the outer banks are
FLTK library for user interface, OpenGL for                    intensively eroded and the deposition occurs mainly
visualization and OpenMP for parallel computations,            on the inner ones. Meanders start to form and then
so that it can take advantage of the modern multi-             grow (Figure 4). Dense subdivision is visible within
core processors. These libraries were chosen to                the river channel, especially on the outer banks. The
enable the application to be compiled and run both on          horizontal shape of the river looks convincing.
Windows and Linux systems. Water simulation,                   However, the cross-section reveals one unwanted
sediment transport and terrain representation were             feature – the smoothing of the eroded terrain
carefully separated in order to enable easy swapping           introduced to ensure the undisturbed flow of water
of different terrain implementations. The code uses            causes the river channel to be too flat, lowering the
lambda functions introduced in the new C++11                   bed near the inner banks (where most deposition
standard. While they do not have any noticeable                occurs) and raising the bed near the outer ones
impact on performance, they greatly reduced the time           (Figure 5).




WSCG 2012 Communication Proceedings                       84                                      http://www.wscg.eu
Figure 4. Meanders generated using the proposed method using a triangle mesh terrain representation.
Green particles denote water and the yellow area corresponds to the river channel. It should be noted that
the curves started to form due to the introduced Coriolis effect on an initially flat surface. The further
downstream, the larger the meanders are, which gives them a plausible appearance.
6. CONCLUSIONS                                                After refining the existing functionality, certain
The main goal of the experiments – to produce                 features will be added. At this moment the only
growing meanders – was achieved. Introduction of              sources of water are artificial and placed by the user.
the simplified Coriolis force lead to satisfactory            This results in the amount of water being
results. Triangle mesh seems to be the best choice for        approximately constant along the channel’s length,
this type of simulation, but there are certain issues         while natural rivers tend to be small at the source and
that require more work. In order to ensure the proper         then grow larger due to additional supply from
shape of the cross-section, the smoothing algorithm           tributaries. Such tributaries should be automatically
needs to be refined or completely replaced by a better        generated based on a rainfall simulation, probably
solution with a possibly low negative effect on the           similar to [Teo08]. The present method assumes the
performance. An effective solution to the mesh                same material softness for the entire base terrain. It
degeneration must be implemented in order to enable           was shown in [Kur11b] and [Ben01] that introduction
the full evolution of meanders up to the formation of         of several, possibly intersecting, layers of terrain with
billabongs. Additional tests on more complicated              different parameters may significantly enhance the
terrains should be conducted in order to find and             results of erosion. A similar feature should be
resolve possible issues.                                      implemented using the triangle mesh.
                                                              The SPH simulation was planned to be migrated to a
                                                              GPGPU solution like OpenCL or CUDA. However,
                                                              the solver uses just a few percent points of the
                                                              processing time, so the expected benefits would be
                                                              negligible.      Possible    performance       related
                                                              optimizations (and potential porting to GPGPU)
                                                              should concentrate on the terrain representation and
                                                              collision detection instead.
                                                              The proposed method has also an interesting feature
                                                              compared to purely random algorithms – the
                                                              emergence and growth of the meanders is a
                                                              continuous process, which could be presented to the
                                                              end-users as a feature. “From Dust” is a game in
                                                              which the player assumes the role of a god and
                                                              achieves the mission objectives by shaping the
                                                              landscape using nature elements. One of these
                                                              elements is water, which erodes the terrain.
                                                              Adaptation of the proposed method for a direct use in
                                                              a game environment would require a significant
                                                              amount of work, but should be possible and could
Figure 5. Close-up of the river channel (terrain              enhance the gameplay.
surface is rendered using Gouraud shading with a
single light source at the camera’s position). The
bed is too wide and too flat.



WSCG 2012 Communication Proceedings                      85                                        http://www.wscg.eu
7. REFERENCES                                                        Wydzialu Elektroniki, Telekomunikacji i Informatyki
[Bel05] Belhadj F. and Audibert P.. Modeling Landscapes              Politechniki Gdanskiej, Proceedings of ICT Young
   with Ridges and Rivers. Proceedings of the ACM                    2011 vol. 1, 2011.
   Symposium on Virtual Reality           Software and            [Mul03] Müller M., Charypar D. and Gross M.. Particle-
   Technology 2005, 2005.                                            based fluid simulation for interactive applications. SCA
[Ben01] Benes B. and Forsbach R.. Layered Data                       '03     Proceedings      of      the     2003       ACM
   Representation for Visual Simulation of Terrain                   SIGGRAPH/Eurographics symposium on Computer
   Erosion. IEEE SCCG2001 Budmerice, Slovakia, 2001.                 animation, 2003.
[Ben06a] Benes B.. Hydraulic Erosion: A Survey. Invited           [Mul09] Müller M.. Fast and Robust Tracking of Fluid
   paper to SCCG 2006, ACM SIGGRAPH, 2006.                           Surfaces. Proceedings of ACM SIGGRAPH /
                                                                     EUROGRAPHICS         Symposium    on    Computer
[Ben06b] Benes B., Tešinsky V., Hornys J. and Bhatia                 Animation (SCA), 2009.
   S.K.. Hydraulic Erosion. Computer Animation and
   Virtual Worlds 17(2), 2006.                                    [Mus02] Ebert D.S., Musgrave F.K., Peachey D., Perlin K.
                                                                     and Worley S.. Texturing and Modeling, Third Edition:
[Ben09] Kristof P., Benes B., Krivanek J. and Stava O..              A Procedural Approach. The Morgan Kaufmann Series
   Hydraulic Erosion Using Smoothed Particle                         in Computer Graphics, 2002.
   Hydrodynamics. Proceedings of Eurographics 2009
   vol. 28 No.2, 2009.                                            [Nal97] Nalder G.. Aspects of Flow in Meandering
                                                                     Channels. Transactions of the Institution of
[Bez10] Bezin R., Peyrat A., Crespin B., Terraz O., Skapin           Professional Engineers New Zealand: General Section
   X. and Meseure P.. Interactive hydraulic erosion using            Volume 24 Issue 1, 1997.
   CUDA. Proceedings of the 2010 international
   conference on Computer vision and graphics: Part I,            [Nei05] Neidhold B., Wacker M. and Deussen O..
   2010.                                                             Interactive physically based Fluid and Erosion
                                                                     Simulation. Proceedings of the Eurographics Workshop
[Bro06] Brosz, J., Samavati, F. and Sousa, M.. Terrain               on Natural Phenomena, NPH 2005, 2005.
   synthesis by-example. Advances in Computer Graphics
   and Computer Vision International Conferences                  [Pru93] Prusinkiewicz P. and Hammel M..A Fractal Model
   VISAPP and GRAPP 2006, 2006.                                       of Mountains with Rivers. Proceedings of Graphics
                                                                      Interface’93, 1993.
[Cen09] Puig-Centelles A., Varley P.A.C. and Ripolles O..
   Automatic Terrain Generation with a Sketching                  [Ram99] Rameshwaran P., Spooner J., Shiono K., and
   Interface. Proceedings of the 17th International                  Chandle, J.H.. Flow Mechanisms in two-stage
   Conference in Central Europe on Computer Graphics,                meandering channel with mobile bed. Proceedings of
   Visualization and Computer Vision (WSCG ’09), 2009.               IAHR Congress in Graz, Austria, 1999.
[Chi05] Chiang, M.Y., Huang, J.Y., Tai, W.K., Liu, C.D.           [Sme09] Smelik R.M., Kraker K.J., Groenewegen S.A.,
   and Chiang, C.C.. Terrain synthesis: An interactive               Tutenel T. and Bidarra R.. A survey of Procedural
   approach. Proceedings of the International Workshop               Methods for Terrain Modelling., CASA Workshop on
   on Advanced Image Technology, 2005.                               3AMIGAS, 2009.
[Ein26] Einstein, A.. The cause of the formation of               [Sta08] Stava O., Benes B., Brisbinn M. and Krivanek J..
    meanders in the courses of rivers and of the so-called            Interactive Terrain Modeling Using Hydraulic Erosion.
    Baer's Law. Read before the Prussian Academy,                     Eurographics/SIGGRAPH Symposium on Computer
    January 7, 1926, published in Die Naturwissenschaften,            Animation, 2008.
    Vol. 14 (English translation in "Ideas and Opinions,"         [Teo08] Teoh T.S.. River and Coastal Action in Automatic
    by Albert Einstein, Modern Library, 1994), 1926.                 Terrain Generation. Proceedings of the International
[Gos10] Goswami P., Schlegel P., Solenthaler B. and                  Conference on Computer Graphics and Virtual
   Pajarola R.. Interactive SPH Simulation and Rendering             Reality’08, 2008.
   on       the       GPU.        Proceedings      ACM            [Teo09] Teoh T.S.. RiverLand: An Efficient Procedural
   SIGGRAPH/Eurographics Symposium on Computer                       Modeling System for Creating Realistic-Looking
   Animation, 2010.                                                  Terrains. ISVC ’09 Proceedings of the 5th International
[Kel88] Kelley A.D., Malin M.C. and Nielson G.M..                    Symposium on Advances in Visual Computing, 2009.
   Terrain simulation using a model of stream erosion.            [Zho07] Zhou, H., Sun, J., Turk, G. and Rehg, J.. Terrain
   Proceedings of SIGGRAPH '88, 1988.                                synthesis from digital elevation models. IEEE
[Kur11a] Kurowski M.. Modelowanie terenu na bazie                    Transactions on Visualization and Computer Graphics
   symulacji erozji z wykorzystaniem deformowalnych                  13,2007.
   wokseli. Zeszyty Naukowe Wydzialu Elektroniki,                 [Van11] Vanek J., Benes B., Herout A. and Stava O..
   Telekomunikacji i Informatyki Politechniki Gdanskiej,             Large-Scale Physics-Based Terrain Editing Using
   Proceedings of WGK 2011 vol. 1, 2011.                             Adaptive Tiles on the GPU. IEEE Computer Graphics
[Kur11b] Kurowski M.. Modelowanie terenu 3D z                        and Applications November/December 2011, Vol 31,
   jaskiniami inspirowane erozją. Zeszyty Naukowe                    No 6, 2011.




WSCG 2012 Communication Proceedings                          86                                          http://www.wscg.eu
 Parallel Treecut-Manipulation for Interactive Level of Detail
                         Selection
                          Daniel Schiffner                                                Detlef Krömker
                    Goethe Universität                                               Goethe Universität
                   Robert-Mayer-Str. 10                                             Robert-Mayer-Str. 10
              Germany, 60054, Frankfurt (Main)                                 Germany, 60054, Frankfurt (Main)
              dschiffner@gdv.cs.uni-frankfurt.de                               kroemker@gdv.cs.uni-frankfurt.de


                                                         ABSTRACT
We present a dynamic system that allows to alter the Level of Detail (LOD) of a treecut-based object. The adap-
tation and selection is made in a parallel process which avoids stalling or locks because of expensive calculations
and LOD changes. We present a method to control the exchange between the independent threads. Based on this
separation, we present multiple strategies to perform the LOD-selection for point-based representations.

Keywords
Level Of Detail, Parallel LOD-selection, LOD-strategies, Thread Management.

1    INTRODUCTION                                                        Following to this introduction, we will give an insight
                                                                         into related work and then present our framework. This
Level of Detail (LOD)-techniques are required in
                                                                         includes the synchronization of the evaluation as well as
today’s rendering environments to assure interactivity
                                                                         the LOD-strategies. We present performance and visual
because of the ever growing number of primitives
                                                                         results that were achieved with the proposed framework
used [Hol11]. The selection of a LOD-representation,
                                                                         and conclude with an outlook regarding future work.
for example, can be based on the current view or
object-related properties. However, these selections
may require expensive computations, and thus, discrete                   2   RELATED WORK
LODs are preferred over continuous methods. We                           Several methods to create a LOD-hierarchy exist. Typ-
address this issue and present a system to allow parallel                ically, these levels are created by the hand of a designer
LOD-selection.                                                           who may be supported by a reduction algorithm. These
This LOD-selection can be derived using different                        pregenerated LODs are then used during rendering and
strategies.   These range from a simple recursive                        are exchanged by some kind of metric.
algorithm up to a priority-based selection. Using a                      It is possible to avoid the generation of a hierarchy
perceptual metric in combination with a prioritization,                  or pregenerated LODs to reduce the amount of stored
the visual quality of an existing representation is                      information. Especially interesting in this context are
preserved with respect to the human visual system. As                    progressive representations. These produce intermedi-
the necessary calculations made by a perceptual metric                   ate solutions and are not restricted to fixed, i.e. discrete,
can be expensive, the LOD-selection is performed in a                    levels. For this reason, these methods are referred to
parallel thread. So, stalling of the rendering is reduced                as continuous LOD. Hoppe [Hop96] presented a mesh-
to a minimum.                                                            based reduction method, which was extended to point-
In this work, we describe our point-based rendering sys-                 based representations by [Wu05].
tem and show how to manage the individual threads.                       To create the individual levels, the reduction methods
Furthermore, we present multiple LOD-strategies that                     can exploit geometric information to increase details.
evolve an existing representation using only local oper-                 [Gar97], for example, apply an error metric to increase
ations.                                                                  the quality of the reduced versions. If such an algorithm
                                                                         is applied sequentially and the results are stored, a hier-
                                                                         archy is created. The current representation is defined
Permission to make digital or hard copies of all or part of              by a vertex front or a cut / treecut.
this work for personal or classroom use is granted without
fee provided that copies are not made or distributed for profit          Instead of generating details, the vertex front can be
or commercial advantage and that copies bear this notice and             altered to select the representation based on the cur-
the full citation on the first page. To copy otherwise, or re-           rent hierarchy. Schiffner and Krömker [Sch10] use a
publish, to post on servers or to redistribute to lists, requires        treecut to adapt the representation by prioritizing nodes
prior specific permission and/or a fee.                                  with high curvature. A similar approach is presented



WSCG 2012 Communication Proceedings                                 87                                           http://www.wscg.eu
                      Model
        Data                                                   Commands, etc.           Controller




                                                                                                                             Inv
                                                                                                Update




                                                                                                                               ok
                                                                                                                                 es
                                                                     Feedback Stage




                                                                                                                                     Re
                                                                                                                                      fre
                                                                                                                                          sh
                                                                                      TC Evaluation
      Static             TC            ...        TC
    Representation   Modify                   Modify

                     Representation           Representation             Model Info                                    Visual Info




                                      Scene                                                       Extract Data
                                                                                                                                          View
Figure 1: The design of our framework is based upon a Model-View-Controller (MVC) pattern. We introduce
a Feedback Stage, which consists of a LOD-strategies and generates the new representation of the TreeCut. This
component extracts information from both the Model and the View. Changes are relayed via the Controller.

by [Car11], where an octree cut is used to select data              Thread Management
for visualization of large data sets. Both methods avoid            We separate the Feedback Stage from the Controller,
a retraversal of the hierarchy and preserve the current             as it is an independent component. As it has access to
vertex front, similar to Progressive Meshes [Hop96]                 all information required, it will be executed in parallel
or Progressive Splats [Wu05]. Both cuts includes a                  to the default rendering. During evaluation, the ren-
method to alter the current distribution. This idea will            dering of the old representation is continued. Due to
be used as a so-called LOD-strategy within this work.               the parallelization of both processes, some kind of syn-
Multi-threaded or parallel applications for rendering               chronization needs to be included to avoid deadlocks or
often focus on splitting the current representation into            race-conditions.
multiple viewports. Applications here range from
Global Illumination [Hol11] to efficient large data set
visualization [Gos12]. Recently, Peng et al. [Pen11]                                 Evaluation                        Rendering
presented an approach to generate large crowds by par-
allel processing the individual models. Similar to our                                                      Data
                                                                          Synced




                                                                                       wait
work, a scene is optimized for interactive renderings.
They, however, use a fixed evaluation method.                                                                 Signal
                                                                                      process

3   SYSTEM DESIGN                                                                     Extract
As common in graphics applications [Shi10], our
                                                                          Parallel




framework is based upon a Model-View-Controller                                       Process                            Update
                                                                                                         if process




(MVC) pattern. The Controller invokes the changes
of the Model, i.e. the TreeCut [Sch10], which holds
the current LOD representation. The View uses this                                     Alter                               Draw
current representation to derive a visual output using a
graphics API.                                                                          wait                                Query
We extend this default pattern by introducing a
                                                                          Synced




Feedback Stage (see figure 1). It is similar to an                                   Exchange
observer component, but with the ability to influence
the Controller. Thus, the Feedback Stage can alter the
Model which results in a different View. The Feedback               Figure 2: The processing sequence used to control the
Stage utilizes a strategy to apply the necessary LOD-               evaluation and rendering threads. While the evaluation
operations. Therefore, the strategy may need to extract             is processing, the rendering thread displays the current
information from the rendered scene. This is visualized             LOD-version. Once the evaluation has completed, the
by the connection between the Feedback Stage and                    data is exchanged and the LOD is updated gracefully.
the View in figure 1. In the following, we will refer               The query from the rendering thread is made without
to the extraction of information and application of the             lock. This conditionally triggers the exchange of a new
strategy as an iteration.                                           representation, which must be performed synchronized.


WSCG 2012 Communication Proceedings                            88                                                       http://www.wscg.eu
We propose a synchronization-strategy based on two                4    LOD-STRATEGIES
states that are queried by the rendering thread: wait             Once a TreeCut has been established, only two core op-
and process. This allows to add the evaluation with               erations are applied (refine, coarse), which repre-
only small changes to the rendering code. The ren-                sent the changes in the detail. To alter the representa-
dering thread only has to query for a new representa-             tion in a global manner, we apply LOD-strategies that
tion, while the Feedback Stage will handle the complete           are based solely on the current cut.
strategy evaluation and LOD-selection. The performed
                                                                  We include a threshold value to control the application
steps are visualized in figure 2.
                                                                  of the individual operations. This counteracts repet-
We leverage the fact that the TreeCut is only repre-              itive refines or coarses of nodes. We, hereby,
sented with an index-list. The derivation of a new LOD            mean that a node is refined in an iteration while it
thus only requires to generate a new index-list, which            is coarsened in the next.
will be swapped or blended with the current one. This
                                                                  In the following, we will present three different types
minimizes stall and flicker once a new representation is
                                                                  of strategies: An optimization, a bucket-based approach
available. Only a pointer, or the VBO id, needs to be
                                                                  and a recursive traversal of the hierarchy. The latter dif-
replaced. No copying of this data is performed during
                                                                  fers from the first ones because it operates on the com-
synchronization.
                                                                  plete LOD-hierarchy instead. It is included to show the
On initialization of the evaluation thread, the state is          universal applicability of the proposed thread manage-
set to wait. In this state, all data can be accessed              ment.
safely from the rendering thread. Here, no locking the
data is required as no processing is performed. Only in           Optimization Strategy
this state the data will be exchanged. As stated before,          The optimization LOD-strategy evaluates the current
the evaluation will generate an index-list, which can be          cut and applies a partial sorting based on a priority
swapped with the current one used for display.                    value, similar to [Car11]. This strategy requires some
If a new representation is requested, because the scene           kind of limitation regarding the cut-size, e.g. a maximal
has changed or is considered invalid, the rendering pro-          node count. The priorities of the parent nodes should
cess issues an update request to the evaluation via a sig-        to be larger than their children to avoid artifacts. Oth-
nal. The evaluation thread is then set to the process-            erwise, a parent node, i.e. a coarser representation, is
state. The rendering continues displaying the old rep-            favoured over a more detailed one.
resentation as long as the evaluation is generating an            During the Extract in the evaluation thread (refer to fig-
updated version. After rendering a single frame, the              ure 2), the priorities are aquired. In our implementation,
evaluation is queried.                                            we use the curvature from the cut-nodes as priorities.
When starting to generate a new representation, the               The partial sorting is applied by iterating the complete
evaluation extracts the required data. This includes to           cut and storing only the nodes with highest and lowest
copy the current index-list used by the rendering thread.         priority.
As this is an read-only operation, no lock is required.
                                                                  The algorithm selects nodes with highest priority for
LOD-strategies may need to aquire additional data from
                                                                  refinement, while nodes with lowest priority are
the View or the Model which can also be copied without
a lock.
After completion of a single iteration, the evaluation                      Nodes
thread will change its state to wait. As it is possible                                       Optimize
that the current representation is optimal for the applied
strategy, a flag is used to indicate this case. This also                                     if space
allows to accelerate the LOD-strategies as they may ter-              Low           High            refine
                                                                                                                       repeat




minate prematurely.                                                                           if no-space
When the evaluation is in wait-stage again, the Ex-                                                coarse
change is executed. The Exchange does not cause a                                             if {} or no-gain
race-condition in both threads, because neither the ren-
dering nor the evaluation requires access to the crucial                                             abort
data at this time. Additionally, we only require to ex-           Figure 3: The optimization LOD-strategy for TreeCut-
change, i.e. swap, the used index-list if a new represen-         evaluation. Only the nodes with highest and lowest pri-
tation has been generated.                                        ority are processed. This accelerates the evaluation as
The Feedback Stage can be invoked again directly after            it reduces the theoretical time complexity [Car11]. The
the completion of the iteration. No additional updates            most important ones are refined, while the least im-
to the Feedback Stage are required, as it extracts the            portant ones are coarsened. This strategy requires a
current information in parallel.                                  maximal node count to be applied


WSCG 2012 Communication Proceedings                          89                                          http://www.wscg.eu
coarsened in the representation. A coarse frees                    have a positive value, the node would be expanded in
space for further refinement with more important nodes.            the next iteration. This would invalidate the operation
The operations performed by this strategy are visual-              and introduce a flicker into the represenation and the
ized in figure 3.                                                  coarse is not executed.
As the partial sorting size can be considered constant             As each node within the cut is evaluated, a linear time
during run-time, e.g. the size is not changed during an            complexity is given: O(n) with n being the cut-size.
iteration, a linear time-complexity is given: O(n log k)           For this LOD-strategy, the threshold is defined as the
with k being the constant partial sorting size and n the           minimal delta that is required to force a coarse. We
size of the current cut.                                           have achieved good results by a threshold of 0.
The threshold is defined as the minimal gain in prior-
ity required when altering the TreeCut. Therefore, the             Recursive Strategy
primitives in the sorted sequences (low and high) are              The last strategy evaluates the complete LOD-hierarchy
compared before a coarse is applied. A refine is                   instead of the current cut. This method is inspired by
always executed as long as space is available.                     the QSplat rendering system [Rus00]. Starting at the
                                                                   root node, the new cut is defined by the individual nodes
Bucket-based Strategy                                              when aborting the recursion. This abort is either due to
The second strategy assigns a target bucket to each node           culling, small splat area, or when no further refinement
within the cut. The strategy alters the TreeCut to match           is possible, i.e. a leaf node is reached.
a certain distribution as closely as possible. This strat-         For this method it is required to additionally store the
egy requires a method that determines the target bucket            complete hierarchy, which is not the case for the other
for a node.                                                        two LOD-strategies. During the Extract-step, this hier-
An example for application is the generation of a                  archy is mapped to be accessible. The evaluation then
stippling-like appearance of an object. The target                 starts the recursive traversal on a plain index-list.
bucket for each node is derived by using the illumi-               The worst time complexity of this algorithm is O(N)
nation at the current node’s location. The darker the              where N is the number of nodes within the tree. As
current location, the more nodes are used within this              the abort criterion includes culling, an acceleration is
region, i.e. the hierarchy is refined. The node is                 achieved, which results in an average logarithmic time
coarsened if a lighter representation is required.                 complexity for large objects.
In figure 4, an exemplary application of the LOD-                  As opposed to the other two methods, the recursive
strategy is shown. For each node, a target bucket is               strategy generates a new cut instead of manipulating an
calculated. If this bucket differs from the currently as-          existing one. Thus, the definition of the threshold is not
signed bucket, the delta is used to determine the accord-          applicable to this strategy.
ing cut-operation. In the figure, a + denotes a posi-
tive delta and a refine needs to be applied, a - is a              5   RESULTS
coarse. The 0 is the special case, that the node al-
                                                                   We tested the different evaluation strategies with our
ready has the correct bucket and no operation is neces-
                                                                   rendering system. We measured the rendering times
sary.
                                                                   and the overhead introduced by the usage of our sys-
After the buckets have been calculated for all siblings,           tem. The proposed LOD-strategies are compared to
the operations are validated. As in the optimization               each other and the evaluation times in dependency of
LOD-strategy, a refine has higher precendence than                 the original primitive count and the current count will
a coarse. For this reason, the left branch is expanded             be given as well.
in figure 4.
Special care has to be taken, if a coarse-operation
needs to be applied. The parent node needs be inspected
as well. The operation is only executed, if the bucket of                                0       ?
the parent does not invalidate it. A small example will
                                                                       +   0   -     -   -   +
illustrate this scenario.
In the right branch of the tree in figure 4, a coarse                      +             -

needs to be applied. Therefore, the parent node is in-             Figure 4: The bucket-based strategy for TreeCut-
spected (visualized by the question mark). In this case,           evaluation. Each node is assigned a target node. All sib-
the target bucket for the parent does not have a different         lings and the parent define the operation to be applied.
delta (it is 0), i.e. it does not invalidate the operation.        If a coarse-operation is requested, the parent node is
Thus, the coarse can be applied safely. The same ap-               inspected (indicated by the question mark). Only if the
plies, if the delta would be negative. If the parent would         operation is considered save, it is executed.


WSCG 2012 Communication Proceedings                           90                                         http://www.wscg.eu
              50
                                 Without Feedback (fit)
                                                                           LOD-version after an iteration has been completed.
                                         Feedback (fit)                    Note that this increase is only generated if the eval-
              40                                                           uation is in wait-stage and a new LOD-version was
                                                                           created. The overall time falls below the version with-
              30                                                           out the Feedback Stage (labeled Without Feedback) be-
  Time [ms]




                                                                           cause rendering is accelerated and less primitives are
              20                                                           required.
                                                                           The graphs in figure 6 show the performance of the pro-
              10
                                                                           posed strategies. We tested each strategy with multiple
              0
                                                                           objects that are drawn in a predefined scene. During
                   0   500,000             1,000,000      1,500,000        rendering, only one object and one light source is used.
                                 Surfels
                                                                           Both are rotated and moved to assure a large number
Figure 5: The performance impact when using the pro-                       of update request for the LOD-strategies. The objects
posed Feedback System. The new representation gen-                         are taken from the Standford 3d repository [3DScan].
erated is swapped from the evaluation thread to the ren-                   As the same scene is used for all objects and LOD-
dering. Note that the increase is not required every                       strategies, the aquired evaluation times are comparable.
frame, but only when an iteration has been completed.
                                                                           We omit information of the transfer of the data from the
In our prototype, no changes are made to the represen-
                                                                           evaluation thread to the rendering thread as the gener-
tation during rendering. The average overhead is the
                                                                           ated data is only swapped.
difference between the fitted lines.
                                                                           As expected, both TreeCut-based strategies perform
                                                                           with linear time complexity. The bucket-based (refer
A sequential comparison is not included using the pro-                     to figure 6a) LOD-strategy does not change the size
posed strategies, but can be derived easily by summing                     of the TreeCut as much as in the optimization LOD-
up the rendering and evaluation times.                                     strategy (refer to figure 6b). This is due to the fact that
Additionally, we present some visual outputs generated                     the bucket-based strategy is not limited by a maximal
by our renderer. For all renderings, we use a point-                       node count.
based rendering method that utilizes the Phong Splat-                      The optimization LOD-strategy allows to include any
ting technique presented by [Bot05].                                       priority into an existing hierarchy. The evaluation re-
As noted before, we use the curvature as the priority                      mains linear despite the performed sorting. In the
value in case of the optimization LOD-strategy. The                        shown case, 8k elements are sorted.
curvature identifies important regions on the surface of                   The recursive strategy applies the QSplat traversal pre-
the object, and detail is preserved in regions where the                   sented by [Rus00]. The abort criterion includes back-
surface changes. This was also presented in [Sch10]                        face culling, and for this reason, multiple nodes or sur-
and [Lee05]. We preprocessed the curvature and in-                         fels can be rejected early. This results in an overall ac-
cluded it within the LOD-hierarchy. Additionally, we                       celeration of the traversal. For simple objects, the gain
assure that parent nodes have a higher curvature value                     is not as large as with objects with higher geometric
than their children. During rendering of the scene, the                    complexity. However, the strategy does not allow fine-
maximal node count is set to be idendtical to the count                    tuning of a single representation. Only the recursive
given by the recursive LOD-strategy.                                       algorithm can be altered.
The bucket-based strategy uses the illumination infor-                     Visual Results
mation to derive a target bucket for each node. We use a
                                                                           Some results achieved with the proposed LOD-
fixed splat size for each node, and so create a stippling-
                                                                           strategies are shown in figure 7. A directional light
like appearance of an object.
                                                                           source is used for illumination.
Finally, the recursive method implements the QSplat hi-                    In figure 7a, we applied the bucket-based strategy along
erarchy and enables the basic QSplat method to be ren-                     with an illumination-based target bucket function. We
dered efficiently using the Phong Splatting technique                      determine the target bucket by weighting the current il-
[Bot05] without further adaptation.                                        lumination with respect to the depth of the node and
                                                                           maximal depth of the tree. This creates a stippling-like
Time Measurements                                                          appearance of the drawn object. We enhanced Phong
Our prototype is written in C++ and openGL. The tests                      Splatting technique for the bucket-based approach to
were made with a Intel i5 with 3.47 GHz, 8.0 GB                            generate both a closed surface and equal-sized surfels.
RAM and a nVidia GeForce 260 GTX with 896MB                                The visual quality of the rendered version depends
RAM. The graphics in figure 5 show the overhead in-                        mainly on the used hierarchy. We enhanced the gener-
troduced due to the exchange of the newly generated                        ation by using a node as parent instead of the average



WSCG 2012 Communication Proceedings                                   91                                          http://www.wscg.eu
             300                                                                        300                                                                          300
                                                 Bucket (fit)                                                         Optimization 8k (fit)                                                         Dragon
                                                                                                                                                                                                     Bunny
             250                                                                        250                                                                          250                               Lion
                                                                                                                                                                                                    Buddha

             200                                                                        200                                                                          200
 Time [ms]




                                                                            Time [ms]




                                                                                                                                                         Time [ms]
             150                                                                        150                                                                          150


             100                                                                        100                                                                          100


             50                                                                         50                                                                           50


              0                                                                          0                                                                            0
                   0   100,000   200,000    300,000     400,000   500,000                     0   100,000   200,000      300,000     400,000   500,000                     0   250,000         500,000        750,000
                                 TreeCut Size [Nodes]                                                       TreeCut Size [Nodes]                                                 TreeCut Size [Nodes]

(a) Bucket-based LOD-strategy performance (b) Optimization LOD-strategy performance (c) Recursive                                                                               LOD-strategy             performance
graph.                                    graph.                                    graph.
Figure 6: The performance graphs of the proposed LOD-strategies. The values are given for an average single
iteration. The LOD-strategies have been applied to mulitple objects with varying sizes. The first two offer linear
time complexity, but the priority strategy has a larger overhead due to the required partial sorting. The recursive
strategy is able to fast reject large portions of the hierarchy, which results in logarithmic time.

as proposed by [Rus00]. This suppresses motion of                                                                       from budget-based restrictions with perceptual opti-
surfels when changing the LOD. Also, the more leaf                                                                      mization up to a bucket-based selection where nodes
nodes are available, the better the dark regions can be                                                                 are assigned a specific level. Also, non-cut-based
displayed.                                                                                                              methods can benefit from the proposed system, which
A result generated with the optimization LOD-strategy                                                                   has been shown as well by including the QSplat
is shown in figure 7b. The surfels are prioritized by the                                                               algorithm.
local curvature and surfel-size. This preserves details at                                                              As the object and selection is made in parallel, no
regions where the object’s surface is changing. Larger                                                                  stalling of the actual rendering occurs. In addition, the
surfels are used in flat regions resulting in a reduction                                                               exchange between old and new representation can be
of the number of used surfels. In the shown image, only                                                                 made with blending to avoid flicker artifacts. The newly
45k surfels (original 183k) are used. The surfel-size has                                                               deduced LOD is not generated in advance, but created
been added to avoid generation of too large splats that                                                                 using information from the current representation. This
could mask detailed areas. This additional information                                                                  allows a finer grained adaptation to a given scenario.
is solely required for point-based representations.                                                                     Due to the design, the system can be included into ex-
Figure 7c shows the result created with the recursive                                                                   isting LOD-management systems as a data provider for
LOD-strategy. Obviously, there is no difference in the                                                                  new LOD-versions.
visual quality compared to the original QSplat algo-                                                                    All presented strategies have a low theoretical time
rithm if plain splatting is used. However, the paral-                                                                   complexity and show good performance in our proto-
lelization increases performance of the rendering. This                                                                 type. The parallel processing of the data preserves in-
is because the rendering can leverage VBOs and so                                                                       teractivity of the rendering without being restricted to
avoids repetitive transfer of rendering data.                                                                           a fixed LOD-set. The synchronization is achieved by
A higher visual quality is achieved by using the Phong                                                                  a simple query and thread-safe exchange is assured by
Splatting technique. This can be used without any adap-                                                                 design.
tation as the LOD-strategy is independent of the ren-                                                                   The LOD-strategies can be extended to account for in-
dering. With the original QSplat, the multiple render-                                                                  formation that is present in the scene. For example, the
passes of the Phong Splatting would require to traverse                                                                 optimization strategy can include perceptual informa-
the hierarchy more than once, which would massively                                                                     tion acquired from the current scene. This increases the
penalize the performance.                                                                                               quality of the representation, while no new data needs
A rendering with the maximal available detail of a sam-                                                                 to be generated. Especially interesting is the application
ple object (the Stanford lion) is depicted in figure 7d.                                                                of the TreeCut methods within the GPU to completely
It uses all 183408 leaf nodes and does not offer more                                                                   avoid transfer of data between CPU and GPU.
details than the reduced versions (shown in figures 7b
                                                                                                                        Yet, the system and the strategies are not optimal and
(45K) and 7c (87K)).
                                                                                                                        need to be refined. Similar to other approaches, we
                                                                                                                        plan to evaluate our system using many objects. The
6             CONCLUSION AND OUTLOOK                                                                                    question arises, whether a centralized thread or a agent-
Our approach increases the range of parallel pro-                                                                       based approach provides better results. Developers
cessing existing LOD-hierarchies.   The different                                                                       should be supported to decide which is the best for their
LOD-strategies account for many scenarios, ranging                                                                      scenario.



WSCG 2012 Communication Proceedings                                                                              92                                                                       http://www.wscg.eu
(a) Bucket-based strategy.     (b) Optimization strategy. The sur- (c) Recursive strategy. The surfels (d) High quality rendering without
The Stanford dragon is         fels for rendering are prioritized by are generated by the QSplat algo- the Feedback Stage (183k surfels).
altered in dependency of the   their curvature and surfel-size. This rithm (87k surfels).
illumination.                  version uses 45k surfels.
Figure 7: Results generated with proposed LOD-strategies. A stippling like appearance can be created by deter-
mining the target bucket based on the current illumination. The second figure has been generated with the curvature
and surfel-size as priority. The recursive splatting method presented by QSplat can be accelerated by employing a
recursive evaluation strategy. The figure on the right shows the high detailed version of the Stanford lion.

We plan to include a complete perception model in                            Massive Point Clouds using Multi-way kd-Trees.
the optimization LOD-strategy. In this case, a full 3d                       The Visual Computer 28. 2012.
model, like [Sch11] or [Lee05], seems most suitable,                    [Hil01] Hiller, S., Deussen, O., Keller, A. Tiled Blue
as the perception information is extracted directly from                     Noise Samples. Vision, modeling and visualiza-
the 3d data.                                                                 tion. pp.265–272. 2001.
The bucked-based method does currently not include                      [Hol11] Hollander, M., Ritschel, T., Eisemann, E.,
important properties like blue-noise [Hil01]. This infor-                    and Boubekeur, T. ManyLoDs: Parallel Many-
mation needs to be encoded within the hierarchy during                       View Level-of-Detail Selection for Real-Time
generation of the LOD and has to be ensured during                           Global Illumination. Computer Graphics Forum
selection of the individual nodes as well. Also, the tar-                    30. pp.1233–1240. 2011.
get bucket function needs to carefully select nodes for                 [Hop96] Hoppe, H. Progressive Meshes. SIGGRAPH
replacement.                                                                 96 conference proceedings. pp.99–108. 1996.
Finally, the system itself needs be enhanced, so that                   [Lee05] Lee, C. H., Varshney, A., Jacobs, D. W. Mesh
the performance and the selection quality increases.                         saliency, Proceedings of ACM SIGGRAPH 2005.
We plan to extend it with environmental information,                         pp.659–666. 2005.
e.g. processing power or battery. This allows to selec-                 [Pen11] Peng, C., Park, S., Cao, Y., and Tian, J. A
tively apply the LOD-selection on different hardware                         Real-Time System for Crowd Rendering: Par-
platforms, while being restricted to a universal, system-                    allel LOD and Texture-Preserving Approach on
independent criterion.                                                       GPU. Lecture notes in computer science vol.
                                                                             7060. pp.27–38. 2011.
7    REFERENCES                                                         [Rus00] Rusinkiewicz, S. and Levoy, M. QSplat: a
[3DScan] http://graphics.stanford.edu/                                       multiresolution point rendering system for large
     data/3Dscanrep/.                                                        meshes. SIGGRAPH 2000 conference proceed-
                                                                             ings. pp.343–352. 2000.
[Bot05] Botsch, M., Hornung, A., Zwicker, M.,
     Kobbelt, L. P. High-Quality Surface Splatting                      [Sch10] Schiffner, D., Krömker, D. Tree-Cut: Dy-
     on Today’s GPUs. Eurographics Symposium on                              namic Saliency Based Level of Detail for Point
     Point-Based Graphics. pp.17-24. 2005.                                   Based Rendering. Sensyble 2010. pp.37–43.
                                                                             2010.
[Car11] Carmona, R., Froehlich, B. Error-controlled
     real-time cut updates for multi-resolution volume                  [Sch11] Schiffner, D., Krömker, D. Three Dimensional
     rendering. Computers & Graphics. pp.934–944.                            Saliency Calculation Using Splatting. Sixth In-
     2011.                                                                   ternational Conference on Image and Graphics
                                                                             (ICIG). pp.835–840. 2011.
[Gar97] Garland, M., and Heckbert, P. S. Surface Sim-
     plification using quadric errormetrics. The art and                [Shi10] Shirley, P., Marschner, S. R., and Ashikhmin,
     interdisciplinary programs of SIGGRAPH 97.                              M. Fundamentals of computer graphics 3rd edi-
     pp.209–216. 1997.                                                       tion. 2010.
[Gos12] Goswami, P., Erol, F., Mukhi, R., Pajarola, R.,                 [Wu05] Wu, J., Zhang, Z., and Kobbelt, L. P. Pro-
     Gobbetti, E. An Efficient Multiresolution Frame-                        gressive Splatting. Eurographics Symposium on
     work for High Quality Interactive Rendering of                          Point-Based Graphics. pp.25–32. 2005.



WSCG 2012 Communication Proceedings                                93                                              http://www.wscg.eu
WSCG 2012 Communication Proceedings   94   http://www.wscg.eu
      Animation of Water Droplets on a Hydrophobic
                       Windshield
         Nobuyuki Nakata                          Masanori Kakimoto                         Tomoyuki Nishita
     The University of Tokyo               Tokyo University of Technology               The University of Tokyo
      5-1-5 Kashiwa-no-Ha                     1404-1 Katakura-machi                      5-1-5 Kashiwa-no-Ha
         Kashiwa, Chiba                           Hachioji, Tokyo                           Kashiwa, Chiba
        277-8561 Japan                            192-0982 Japan                           277-8561 Japan
nobnak@nis-lab.is.s.u-tokyo.ac.jp             kakimotoms@stf.teu.ac.jp                   nis@is.s.u-tokyo.ac.jp


                                                     ABSTRACT
   Animation of water drops on a windshield is used as a special effect in advanced driving games and simulators.
Existing water droplet animation methods trace the trajectories of the droplets on the glass taking into account
the hydrophilic or water-attracting nature of the glass material. Meanwhile, in the automobile industry, usage of
hydrophobic glass windshields has recently been a common solution for the drivers’ clear vision in addition to
cleaning the water with wipers. Water drops on a hydrophobic windshield behave differently from those on a
hydrophilic one. This paper proposes a real-time animation method for water droplets on a windshield taking
account of hydrophobicity. Our method assumes each relatively large droplet as a mass point and simulates its
movement using contact angle hysteresis accounting for dynamic hydrophobicity as well as other external forces
such as gravity and air resistance. All of a huge number of still, tiny droplets are treated together in a normal map
applied to the windshield. We also visualize the Lotus effect, a cleaning action by the moving droplets. Based on
the proposed simulation scheme, this paper demonstrates the motion of the virtual water droplets on the
windshield of a running vehicle model.


Keywords
Water droplets, hydrophobicity, windshield, driving simulator, contact angle hysteresis
                                                                    the glass. To clear the water, mechanical wipers
1. INTRODUCTION                                                     have been used since the beginning of the
Water flow on the window or windshield surfaces                     automobile history. In addition, as auxiliary
are commonly used as a rainy scene description in                   measures, coating the windshield with water
film works and other types of motion pictures. More                 repellent material became a solution a few decades
recently, computer generated animations of water                    ago. In the year 2000, the first water-repellent
flow on the windshields are realized for advanced                   finished windshield became commercially available.
video games and driving simulators. Since the glass                 Nowadays such hydrophobic windshield products
material has hydrophilic or water-attracting nature,                are widely used in the automobile market.
water droplets move along irregular trajectories                       A large amount of research literature on the
seeking for water-attracting places of the surface, as              behaviour of water on hydrophobic surfaces is
we often find on the windows in a rainy day. Most                   published in chemical and mechanical engineering
of the existing water droplet animation methods                     fields. To the authors’ knowledge, however, little
simulated these winding trajectories of the droplets.               work has been done on real-time simulation of
  In real driving situations, those water trajectories              water droplets sliding across hydrophobic
or water-film on the windshields due to the                         windshields. In this paper, we address this problem
hydrophilicity seriously affect the visibility through              and propose a solution consisting of several
                                                                    practical simulation models for use in games and
 Permission to make digital or hard copies of all or part of        driving simulators.
 this work for personal or classroom use is granted without            Water attracting or repelling feature of surface
 fee provided that copies are not made or distributed for
                                                                    material should be quantified differently in two
 profit or commercial advantage and that copies bear this
 notice and the full citation on the first page. To copy            situations, static and dynamic. The static repellency
 otherwise, or republish, to post on servers or to                  has been investigated for a long time and the
 redistribute to lists, requires prior specific permission          fundamentals have been established. For water
 and/or a fee.                                                      droplet animation, knowledge on the dynamic
                                                                    repellency is more important, which is true in



WSCG 2012 Communication Proceedings                            95                                     http://www.wscg.eu
engineering analysis of water-shedding phenomena              assume hydrophilicity. Also, they do not
on the windshield. While the dynamic water                    incorporate air resistance against the water drops or
repellency includes a number of unexplainable                 rolling resistance of the drops.
phenomena, there are a couple of major factors and              Several researchers have developed fluid
indicators characterizing the dynamic repellency.             dynamics based methods for the water droplet
Those include contact angle hysteresis, falling angle,        simulation. Wang et al. [Wan05a] took into account
falling velocity, and falling acceleration.                   surface tension, contact angle, and contact angle
  The relationship between the contact angle                  hysteresis. The surface tension is more dominant in
hysteresis and the slope angle has long been                  a water droplet than in regular large-scale fluid
investigated. In case of an ideal water droplet shape,        forms. Thürey et al. [Thu10a] introduced the mean
the contact angle hysteresis is known to be in                curvature flow, which is known as a motion
proportion to the falling angle.                              equation for surface boundaries, and evaluated the
  The falling velocity and acceleration vary by the           phenomena caused by the surface tension more
surface material even when the slope angle remains            appropriately than Wang et al.
constant. Although the standard methods for                      Zhang et al. [Zha11a] developed a faster
evaluating       and    measuring       the   falling         computation method for droplets using the mean
velocity/acceleration were not established until              curvature flow without other fluid simulations.
recently, it is known that the behaviour of a falling         They ignored the internal fluid flow of the droplets
water droplet on the hydrophobic surface is                   but used the surface tension and other external
explainable in terms of rolling and sliding.                  forces to give deformation, collision and division to
   In this paper, we take the knowledge on the                each droplet represented as a polygon mesh. They
dynamic repellency into account and propose a real-           achieved 10-50 fps in the experiment with 10K-50K
time animation method for water droplets on the               polygon mesh. However, due to the implicit method
hydrophobic windshield. As the water-repellent                for the mean curvature flow computation, the
coated windshields become standard in the                     stability of their solution depends highly on the
automobile market, our contribution is to provide             mesh quality and the time step, and the performance
video game and simulator developers with a means              optimization is limited.
of reproducing realistic and harmonious motions of               In order to tackle the problem of the droplet
the water droplet cluster traveling across the                motion on the hydrophobic surfaces, we need to
hydrophobic windshield.                                       understand dynamic repellency. The structure or the
   This paper is organized as follows. In the next            behaviour of the surface molecules are considered
section we introduce related work on both                     to be a source of the dynamic repellency. To figure
engineering analyses and animation techniques for             out the behaviour, Hirvi et al. [Hir08a] simulated a
water droplets. Then our proposed method is                   droplet consisting of thousands of water molecules
explained in a theoretical point of view in Section 3,        using a molecular dynamics calculation technique.
followed by more detailed descriptions on the                 Korlie [Kor93a] proposed a cluster model of quasi-
implementation and results in Section 4. Finally we           molecular particles on a horizontal plane and
give conclusions and future work in Section 5.                introduced its dynamical equations which lead to
                                                              the value of the contact angle of the cluster.
2. RELATED WORK                                                  Analyses of real water droplets have been done
                                                              by several research groups. For example, Sakai et al.
In the computer graphics field, several methods               [Sak06a] measured the velocity and the acceleration
have been introduced for animating water droplets.            of a droplet sliding across water-repellent surfaces.
Kaneda et al. [Kan93a] [Kan96a] proposed methods              Droplets are known to run down either rolling or
to describe the movement of the droplets by                   slipping on the incline depending on the degree of
defining each droplet as a particle and move it with          hydrophobicity [Ric99a] [Suz09a]. Hashimoto et al.
particle dynamics. Since the droplets travel seeking          [Has08a] measured the relationship between the
for water-attracting places, their trajectories on the        volume and the velocity of a windswept droplet.
glass surface form complex shapes. They also
simulated these motions by a random walk method                  We address the problem of dynamic water-
using random numbers [Kan99a]. Recently their                 repellency taking the contact angle hysteresis into
method was implemented as a real-time simulator               account. In addition, we use the knowledge of the
with a GPU computing technique [Tat06a].                      real water drop analyses to verify and compensate
Fournier et al. [Fou98a] depicted the trajectories of         our results. We avoided using the fluid dynamics
droplets using the mass spring model. None of the             simulation, the mean curvature flow, or any type of
above     methods     took     into     account    the        molecular forces since they are not suitable for real-
hydrophobicity of the inclined surface since they             time visualization. Due to the computing load and




WSCG 2012 Communication Proceedings                      96                                      http://www.wscg.eu
the time step limitations, those methods cannot               spherical geometry. Meanwhile, the contact angle of
handle sufficient number of droplets on a car                 the glass becomes 90°-100° when it is coated with
windshield.                                                   commercially available repellent material.
   In our method, each droplet is represented as a               Based on the above two observations, we assume
mass point or a particle. Thus, we are able to                that each rain droplet is rendered as a hemisphere.
incorporate additional forces into the real-time              In practice, the geometric shape is basically a disc-
simulation loop; air resistance against the water             like plane and the normal vectors for refraction are
droplets and viscous dissipation which acts as a              controlled to make it look hemisphere. Details are
rolling resistance of each drop. Although these               described in Section 4.3.
forces are crucial factors for the fast movement of
water drops, they have not been fulfilled in the              3.2 Contact Angle Hysteresis
previous methods [Wan05a] [Thu10a] [Zha11a].
                                                              When a thin pipe is inserted into water, the water
   Particle dynamics are common in the real-time
                                                              level in the pipe is raised by the capillary action.
simulation field. They are widely adopted in games
                                                              This is caused by a force called the capillary force
and interactive applications. Real-time physics
                                                              which operates along the triple boundary line
engines in the market are equipped with features of
                                                              among the water, the solid and the air. The capillary
particle dynamics and rigid body dynamics
                                                              force is determined by the Young-Laplace equation.
including collision detections as fundamental
functions. We implemented our method on top of a                                             Receding
game engine ‘Unity’ and added unique behaviours                      Proceeding            contact angle
of water droplets running slowly or quickly, or                       direction
staying on the hydrophobic surfaces.
                                                                                           θr
3. A PRACTICAL MODEL FOR                                                            θa           Drag due to the
   WATER DROPLETS ON                                                                              contact angle
                                                                                                   hysteresis
   HYDROPHOBIC WINDSHIELDS                                         Advancing
                                                                  contact angle      α Slope angle
3.1 Water Droplet Geometry
                                                              Figure 2. Advancing and receding contact angles of
When a droplet is on a solid surface, the contact             a water droplet.
angle is defined as the angle between the solid
surface and the droplet surface. The contact angle is
determined by the Young equation, which describes                With regard to a droplet which lies on a solid
the balance of three surface tensions, as shown in            plane, the capillary forces along the circular triple
Equation (1).                                                 boundary cancel each other out if the contact angle
              cos        ,
                                                              is constant along the circle. When some external
                                                  (1)
where,  is the contact angle,  is the surface
                                                              forces are put on the droplet and its shape is

tension of the water droplet,  is the surface
                                                              deformed, the contact angles vary while the droplet

tension of the solid,  is the boundary tension
                                                              stands still until the contact angle variance reaches
                                                              at a certain value.
between the water and the solid (Figure 1).                      The contact angle hysteresis is defined as the

                                                              contact angles ( and  , respectively). These two
                                                              difference between the advancing and receding
                          Water droplet
                                                              angles are defined as the largest and the smallest
                     γL                                       contact angles, respectively, at the moment that the
                                                              water droplet starts moving on the solid plane by
                          θ                                   the sufficient external force. The slope angle at this
                γS                    γSL                     moment is called the falling angle. Figure 2
                                                              illustrates the advancing and receding contact
Figure 1. Contact angle and tensions of a water               angles for an incline.
droplet.
                                                                 While the droplet is moving on the plane, a drag
                                                              operates on the droplet toward the reversed
  When the radius of the droplet on hydrophobic               direction against the proceeding direction. The
surfaces is less than the radius of capillary (2.8mm),        amount of drag is related to the contact angle
the surface tensions are the dominant factors of the          hysteresis. Assuming that the shape of the triple
water drop shape. Thus the droplet forms a near



WSCG 2012 Communication Proceedings                      97                                      http://www.wscg.eu
boundary is a circle, the drag  is approximated            compensated wind velocity for the droplet.
with the following equation [Car95a]
           cos            cos  ,
                                                              3.4 Viscous Dissipation
                
                                                   (2)
where,  represents the radius of the water droplet.
                                                               When a droplet is moving or rolling, another drag is
 and  are the receding and the advancing
                                                               caused by some in-bulk friction called viscous
                                                               dissipation [Bic05a]. The drag is in proportion to
contact angles, respectively.
                                                               the velocity of the droplet and represented as

3.3 Wind Drag                                                               /012  η456,                     (5)
                                                               where, 7 is the degree of viscosity of the water, 4 is
                                                               the radius of the droplet, 5 is the velocity of the
Automobile windshields meet with air resistance, or

                                                               droplet. 6 is a factor dependent on the contact
wind drag, according to the velocity of the running
vehicle. The wind drag is defined as follows:

                 !  ,
                           
                                                               angle.
                           
                                                   (3)

where,  is the density of the air,  is the
                                                               3.5 Wind Speed and the Droplet
                                                                   Acceleration
the droplet, and ! is the velocity relative to the air.
coefficient of resistance, is the projected size of
                                                               In the surface finishing engineering discipline,
   In Equation (3), the droplet is assumed to be               Hashimoto et al. [Has08a] introduced an experiment
floating in the air. Since all droplets in our model           to measure the acceleration of various volumes of
are placed on a solid windshield, the equation needs           water droplets placed on an angled hydrophobic
to be modified. We assume that the wind is                     plane in a wind tunnel. Figure 3 quotes from the
weakened at places very close to the solid plane. It           literature and shows the result of the measured
is known that in such near-boundary layer, the wind            descending or ascending acceleration of the droplets.
velocity changes in a complicated manner.                      The contact angle, the slope angle, and the falling
                                                               angle are 105° , 35° and 10°, respectively.
  We employed a simplest compensation to
decrease the velocity in the near-boundary layer                  In the range where the wind velocity is relatively
using an exponential law as shown in the following             low, moderate but more falling accelerations are
formula.                                                       observed as the droplet size becomes greater. When
                               '
                                                               the wind velocity is raised beyond a certain value
                       !$ &        * + δ
                          (
              !"  #
                                                               (7m/s in Figure 3), the droplet stays still within
                         %
                       !           * - δ,
                                                   (4)         some range of wind velocities. When the velocity is
                                                               further raised beyond a higher value (11m/s), rapid
where, ! is the wind velocity out of the boundary
                                                               ascending accelerations are observed, which are
layer (relative to the solid plane), * is the height of
                                                               greater as the droplet becomes larger.
the droplet, . is a parameter representing the
thickness of the boundary layer, and !" is the
                                                                 On the other hand, we simulated the sliding
                                                               accelerations of a droplet taking the following five
                                                               forces into account (Figure 4).
                                                                   Gravity (vertical) 8
                                                                   Wind drag (horizontal) 
                                                                   Perpendicular force (normal to windshield)




 Figure 3. A measured relationship between the                  Figure 4. External forces added to a droplet and
 wind velocity and the acceleration of droplets,                the resultant acceleration. In this example, the
 using a varying droplet size as a parameter                    gravity is more dominant than the wind drag and
 (excerpt from [Has08a]).                                       thus the droplet slides down.



WSCG 2012 Communication Proceedings                       98                                      http://www.wscg.eu
    windshield) /012
   Viscous dissipation drag (tangential to


    windshield) 
   Contact angle hysteresis drag (tangential to

   The wind drag  has been described in
Section 3.3. The contact angle hysteresis drag
behaves as a resistance force parallel to the

force normal to the windshield. The force 
windshield, in the same way as the perpendicular

represented in Equation (2) defines the maximum
limit of the hysteresis drag.                                  Figure 6. Droplet trajectories caused by the Lotus
  In our implementation, the maximum limit is                  effect (image captured from a live-action movie of a
                                                               windshield).
Ω ≡ cos     cos  . Since the relationship between
specified   by a       dimensionless     coefficient


simulate, we approximate the Ω value as a function
the wind velocity and the contact angles is hard to            We implemented this process and it is invoked on
of the wind velocity !. When the velocity is small,
                                                               droplet collision detection.
we force the Ω value to keep a minimum constant
Ω< which is typically 0.5.
                                                               3.7 Distribution of Raindrop Radii and

   Ω!  max @Ω< , 2 B1      exp $       &H I,
                                        F
                                                                   the Lotus Effect
                                        G
                                                    (6)
where, σ is a constant parameter which controls the
                                                               Lotus effect is a phenomenon which occurs when a

saturation rate of Ω!. When the wind is extremely
                                                               water droplet moves across a hydrophobic surface.
                                                               Lots of very small droplets and contamination

extreme as  → 180° ,  → 0°, and thus Ω∞ →
strong, the contact angles are assumed to be also as           spread on the surface are removed by the moving

2. This is well accounted for by Equation (6).
                                                               droplet along the trajectory. The same phenomenon
                                                               is observed on a windshield as demonstrated in the
   Figure 5 shows a simulated result of the                    snapshot of Figure 6.
accelerations for the varying droplet sizes. The                  Figure 7, an excerpt from [Fur02a], is a rain
range of wind velocities in which the droplet stays            droplet radius distribution under 1mm/h rainfall.
still is reproduced, and the range is very similar to          The graph is with the raindrop diameters as the
the measured result in Figure 3.                               horizontal axis and the number of raindrops for each
                                                               diameter as the vertical logarithmic axis. The line
                                                               indicated as ‘MP’ is an exponential distribution
                                                               model called the Marshall-Palmer distribution
                                                               [Mar48]. Each graph legend is the place name of the
                                                               observing site. Some legends contain observing
                                                               periods in months.




 Figure 5. Simulation results of the droplet
 accelerations.


3.6 Collision between Droplets
The surface tension of the water droplet causes a
pressure difference in the droplet. This is known as
the Laplace pressure and is greater as the droplet             Figure 7. Distribution of the number of raindrops for
radius is smaller. Therefore, when two water                   each diameter (drop size distribution). Each graph
droplets of different sizes collide with each other,           legend indicates the name of the observing site
the small droplet gets absorbed by the larger one.             (excerpt from [Fur02a]).




WSCG 2012 Communication Proceedings                       99                                     http://www.wscg.eu
                                                               /012 is in proportion to the droplet velocity. The
   According to the model, the smaller the raindrop            important point is that the viscous dissipation drag
diameter is, the greater the number of raindrops
becomes. Especially, tiny raindrops of below 1mm               above constant value can be used to control the
are contained with an exponentially large numbers.             maximum droplet speed.
Therefore, it is impractical to simulate the motion of            While the droplets are moved by the external
every droplet. Fortunately, those tiny raindrops do            forces, we obtain each collision point with its u-v
not move at all with our simulation model as shown             coordinates and the normal vectors of the colliders
in Figure 5. Thus we apply a single large normal               from the collision detector of the physics engine.
map onto the windshield. The map contains the                  For a droplet being regarded as to be on the
normal vectors which represents all the small                  windshield, the windshield point corresponding to
droplets standing still on the windshield.                     the droplet is calculated and the refraction map
                                                               image for the Lotus effect is updated.
4. IMPLEMENTATION AND                                             In case that a droplet collides with another
   RESULTS                                                     droplet, the Laplace pressure effect is applied. The
This section describes implementation of our                   system compares the masses of the two droplets. If
method proposed in the previous section and                    the difference is greater than the pre-defined
demonstrates some results.                                     threshold, these two will fuse together into one
                                                               droplet.
4.1 Implementation Overview
                                                               4.3 Rendering Large, Movable
We implemented the system on top of Unity, a                       Droplets
popular game engine. Although our method regards
each water droplet as a particle, we implemented               Each large water droplet (with over 1mm diameter)
each droplet as a small rigid body which does not              is rendered as a disc-shape polygon mesh when it is
rotate. Regarding the rigid body physics engine, we            staying still on the windshield. The normal vectors
used NVIDIA PHYSX embedded in the Unity                        on the disc surface are controlled so that the
system.                                                        refracted environment appears to be mapped on a
   The flow of the whole process is outlined as                hemisphere.
follows.                                                          While the droplet is moving across the
                                                               windshield, its shape is deformed to be longer along
   Initialization
                                                               the moving direction. The normal vectors are
   Main loop                                                  controlled so that the lengthened transparent droplet
       Droplet generations                                    looks like a drug capsule sectioned by a screen-
       Physics simulation                                     parallel plane. The deformation is controlled so that
       Collision detection                                    the assumed volume of the droplet is preserved.
             Droplet mergers                                  Using its normal vectors, the pixel shader calculates
             Droplet deletions                                the refraction directions and maps the background
                                                               texture image as the environment. Figure 8 is a
       Updates of large droplet shapes
                                                               close-up rendering image of a pseudo-hemisphere
       Update of windshield alpha map (Lotus
                                                               water droplet and a deformed pseudo-hemisphere.
          effect over small droplets)
                                                                 Those large droplets are generated with various
       Rendering

4.2 Physics Simulation of Droplets
In each time step of the simulation, our system
calculates the external forces imposing on the water
droplets as illustrated in Figure 4.
  Regarding the gravity, we added some random
noise to the force component parallel to the
windshield in order to realize natural motions of the
droplets caused by some assumed fluctuation of the
running vehicle.
  The implementation of viscous dissipation
                                                               Figure 8. Droplets rendered as a pseudo-
6 in Equation (5) is not determined. We used a
(Section 3.4) is a heuristic matter since the factor
                                                               hemisphere (left) and a deformed pseudo-
constant value η6  0.5 in the equation. The                hemisphere (right).



WSCG 2012 Communication Proceedings                      100                                     http://www.wscg.eu
sizes according to the Marshall-Palmar distribution
shown in Figure 7. The number of large droplets
generated per frame is set to be five typically. They
are accumulated but eventually moved away out of
the windshield or collided and fused with others. As
a result, a couple of hundred to one thousand large
droplets reside in the steady-state situation.

4.4 Rendering Small and Still Droplets
Small droplets (with less than 1mm diameter) are
represented as perturbation in a normal map image             Figure 10. A result with low wind velocity

                                                              with Ω<  0.5.
for the windshield, as described in Section 3.7. The          (11.3m/s) and a large contact angle hysteresis
diameters of the generated small droplets vary also
according to the Marshall-Palmar distribution. The
number of small droplets in our implementation
amounts to approximately 800K/QR .
  The outside scene image is refracted according to
the normal map. The trajectories of large droplets
(pseudo-hemispheres) are stored as an image
component which is used to suppress the normal
map. They are composed in the shader program and
the Lotus effect on the windshield surface is
rendered (Figure 9).

                                                              Figure 11: A result with low wind velocity

                                                              with Ω<  0.05.
                                                              (11.3m/s) and a small contact angle hysteresis




Figure 9. The Lotus effect. Small and still
droplets are rendered as a normal map on the
windshield. Large and moving droplets are
rendered as pseudo-hemispheres.

                                                              Figure 12. A result with high wind velocity

                                                              Ω<  0.5.
                                                              (15m/s) and a large contact angle hysteresis with
4.5 Performance
All results referred to in this section are captured
snapshots of real-time animations rendered from the
driver’s point of view toward the automobile                  all. In Figure 11, the adherence is smaller and the
proceeding direction viewing the outside through              droplets move along the windshield curve.
the windshield. The source of the outside image is a             Figure 12 is a result with stronger wind and the
motion picture shot with a video camera placed                large droplets climb straight up the windshield.
between the two front seats of a running car when             Since the adherence is strong and the boundary
no rain is falling. The pre-recorded image is                 layer is set to be thick, the small droplets are made
mapped as a video texture onto a billboard model              still.
placed in front of the windshield model.                        The frame rates for Figures 10, 11 and 12 are
  Figures 10 and 11 are the examples with a small             134-153fps, 80-100fps, and 70-100fps, respectively.
wind velocity. In Figure 10, a relatively large               The scene contains a windshield, large droplets and
contact angle hysteresis is specified and thus the            the video texture billboard shapes, which total
adherence is strong that the droplets do not move at          approximately 17K vertices.




WSCG 2012 Communication Proceedings                     101                                     http://www.wscg.eu
4.6 Rendering Conditions                                      [Fou98a] Fournier, P., Habibi, A., Poulin, P.,
                                                                 Simulating the flow of liquid droplets. Graphics
For the rendering results, we used an Intel Core2                Interface, pp.133-142, 1998.
Extreme X9600 (3GHz), NVIDIA GeForce
GTX480 Graphics and 8GB main memory. The                      [Fur02a] Furutsu, T., Shimomai, T., Reddy, K.K.,
horizontal field of view was 45° and the distance                Mori, S., Jain, A. R., Ong, J.T., Wilson, C.L.
between the viewpoint and the windshield was                     Comparison of the characteristics of the drop
approximately 0.5m. The horizontal curvature                     size distributions in the tropical zone (In
radius of the windshield geometry was 5m constant                Japanese). Open Workshop 2002 on Coupling
and the vertical curvature was 0 (flat). The slope of            Processes in the Equatorial Atmosphere, 2002.
the windshield was inclined at a 45° angle.                   [Has08a] Hashimoto, A., Sakai, M., SONG, J.-H.,
                                                                 Yoshida, N., Suzuki, S., Kameshima Y., and
5. CONCLUSION AND FUTURE                                         Nakajima, A. Direct observation of water
                                                                 droplet motion on a hydrophobic self-assembled
   WORK                                                          monolayer surface under airflow. Journal of
We proposed a real-time animation method which                   Surface Finishing Society of Japan, Vol.59,
reproduces the behaviour of a group of water                     No.12, pp.907-912, 2008.
droplets on a hydrophobic windshield. We modeled              [Hir08a] Hirvi, J.T., and Pakkanen, T.A.
each of large droplets as a mass point and took into             Nanodroplet impact and sliding on structured
account dynamic hydrophobicity by employing the                  polymer surfaces. Surface Science. No.602,
contact angle hysteresis which causes appropriate                pp.1810–1818, 2008.
adherence for each droplet.
                                                              [Kan93a] Kaneda, K., Kagawa, T. and Yamashita,
   We also compared the accelerations of simulated               H. Animation of water droplets on a glass plate.
droplets with those of measured real water droplets              Proc. Computer Animation ‘93, pp.177–189,
from literature of surface finishing engineering                 1993.
analysis. By introducing a near boundary layer
where the wind is reasonably weakened, our result             [Kan96a] Kaneda, K., Zuyama, Y., Yamashita, H.
matched the measured one and reproduced realistic                and Nishita, T. Animation of water droplet flow
behaviours of the droplets.                                      on curved surfaces. Proc. Pacific Graphics ’96,
                                                                 pp.50–65, 1996.
  For a huge number of tiny water droplets which
do not move in our model, we introduced a normal              [Kan99a] Kaneda, K., Ikeda, S., and Yamashita, H.
map applied to the windshield. By using the image-               Animation of water droplets moving down a
based droplets, the Lotus effect was effectively                 surface. Journal of Visualization and Computer
reproduced.                                                      Animation, Vol.10, No.1, pp.15-16, 1999.
   For practical number of large droplets, our                [Kor97a] Korlie, M. S., Particle modeling of liquid
method runs in real-time and can be easily adopted               drop formation on a solid surface in 3-D.
as an effect for video games and vehicle simulators.             Computers & Math. with Applications, Vol.33,
The performance is degraded when the large                       No.9, pp.97-114, 1997.
droplets are not blown off and accumulated on the             [Mar48a] Marshall, J.S., and Palmar, W.McK. The
windshield because the motion simulation is done                distribution of raindrops with size. Journal of
on a per large droplet basis.                                   Meteorology, Vol.5, pp.165-166, 1948.
   Future work includes the performance                       [Ric99a] Richard, D., and Quere, D. Viscous drops
improvement for larger number of droplets, more                  rolling on a tilted non-wettable solid. Europhys.
realistic deformation of the droplets, and handling              Lett., Vol.48, No.3, pp.286-291, 1999.
of uneven wind velocity distributions.                        [Sak06a] M. Sakai, J-H Song, N. Yoshida, S.
                                                                 Suzuki, Y. Kameshima and A. Nakajima,
REFERENCES                                                       Relationship between sliding acceleration of
[Bic05a] Bico, J., Basselievre, F., and Fermigier, M.            water droplets and dynamic contact angles on
   Windswept droplets. Bulletin of the American                  hydrophobic surfaces. Surface Science, 600 (16),
   Physical Society 2005, 58th Annual Meeting of                 204-208, 2006.
   the Division of Fluid Dynamics, 2005.                      [Suz09a] Suzuki, S., Nakajima, A., Sakurada, Y.,
[Car95a] Carre, A., and Shanahan, M.E.R. Drop                    Sakai, M., Yoshida, N., Hashimoto, A.,
   motion on an inclined plane and evaluation of                 Kameshima, Y., Okada, K. Mass Dependence of
   hydrophobia treatments to glass. Journal of                   rolling/slipping ratio in sliding acceleration of
   Adhesion, Vol.49, No.3-4, pp.177-185, 1995.                   water droplets on a smooth fluoroalkylsilance




WSCG 2012 Communication Proceedings                     102                                     http://www.wscg.eu
   coating. Europhys. Lett., Vol.48, No.3, pp.286-         [Wan05a] Wang, H., Mucha, P.J., and Turk, G.
   291, 2009.                                                Water drops on surfaces. ACM TOG, Vol.24,
[Tat06a] Tatarchuk, N. Artist-directable real-time           No.3 (Proc. SIGGRAPH 2005), pp.921-929,
   rain rendering in city environments. Course               2005.
   Note #26, SIGGRAPH 2006.                                [Zha11a] Zhang, Y., Wang, H., Wang, S., Tong Y.,
[Thu10a] Thürey, N., Wojtan, C., Gross, M., and               and Zhou, K. A deformable surface model for
   Turk, G. A multiscale approach to mesh-based               real-time water drop animation. IEEE TVCG,
   surface tension flows. ACM TOG, Vol.29, No.4               PrePrint, August 2011.
   (Proc. SIGGRAPH 2010), 48, 2010.




WSCG 2012 Communication Proceedings                  103                                  http://www.wscg.eu
WSCG 2012 Communication Proceedings   104   http://www.wscg.eu
    Visualization of Very Large 3D Volumes on Mobile Devices
                            and WebGL
                                    José M. Noguera, Juan-Roberto Jiménez
                            Graphics and Geomatics Group of Jaén. University of Jaén.
                             Campus Las Lagunillas, Edificio A3, 23071 Jaén, Spain.
                                         {jnoguera,rjimenez}@ujaen.es


                                                         ABSTRACT
Platforms based on OpenGL ES 2.0 such as mobile devices and WebGL have recently being used to render 3D
volumetric models. However, the texture storage limitations of these platforms cause that only low-resolution
models can be visualized. This paper describes a novel technique that overcomes these limitations and allows us to
render detailed high resolution volumes on these platforms. Additionally, we propose a software architecture that
permits existing volume rendering techniques to be adapted to mobile devices and WebGL. A set of experiments
has been carried out to assess the performance of the proposed architecture on these platforms with different
volumes of increasing resolution. Results prove that our proposal is feasible, robust and achieves visualization of
very large volumes on constrained platforms.
Keywords:        Volume visualization, OpenGL ES, mobile devices, WebGL, large volumetric models, software
architecture.

1    INTRODUCTION                                                         In addition, recent advances in display technologies al-
Nowadays mobile devices are extensively used as a                         low today’s mobile devices to feature large and high
worthy tool in many different scenarios of our life.                      resolution screens, which require large volumetric mod-
Their hardware and software capabilities are constantly                   els in order to achieve a minimum of quality. For exam-
being enhanced, and recent research has demonstrated                      ple, newer tablets such as the iPad3 feature screen reso-
their validity to compute complex computer graphics                       lutions that surpass the Full-HD standard used in most
algorithms. In fact, it has been proved that the vol-                     monitors and TV screens. Nevertheless, their GPU and
ume visualization field can benefit from the proper-                      memory capacities are still limited and do not support
ties of mobile devices in many interesting applications                   large models directly.
[18, 1, 5, 7, 9, 11, 13, 22].                                             In this paper, we deeply study this problem in the con-
However, it is a common misunderstanding to assume                        text of direct volume rendering. We present a pro-
that the same results can be achieved by a literal trans-                 posal to render very large volumetric models in order to
lation to mobile devices of classic algorithms originally                 meet the user expectations in quality and performance
developed for standard PCs or workstations. There are                     by overcoming the referred limitations of handheld de-
two important factors that must be taken into account:                    vices, see Figure 1. Moreover, we describe a software
                                                                          architecture that allows us to adapt existing volume ren-
• The standard graphics specification of this kind of                     dering techniques based on 3D textures to platforms
  devices is OpenGL ES 2.0 [12], which differs from                       that only support 2D textures.
  the desktop PC counterpart in several aspects, e.g.,                    The ideas described in this paper are also applied to We-
  the lack of 3D texture support.                                         bGL1 , the standard for accelerated graphics on the Web.
                                                                          As this standard is based on the same specification used
• These devices must rely on batteries, so their
                                                                          by mobile devices, i.e. OpenGL ES 2.0, it suffers from
  hardware and software architectures are designed to
                                                                          the same limitations, including the lack of 3D textures.
  favour power-efficiency instead of pure computing
  power.                                                                  Finally, we have implemented a mobile and a WebGL
                                                                          based prototypes and conducted a set of experiments
                                                                          to test performance of these platforms under the condi-
Permission to make digital or hard copies of all or part of               tions of maximum storage requirements.
this work for personal or classroom use is granted without                The rest of the paper is organized as follows. Section 2
fee provided that copies are not made or distributed for profit
                                                                          presents current research in volume visualization tech-
or commercial advantage and that copies bear this notice and
                                                                          niques for mobile devices and WebGL. In Section 3,
the full citation on the first page. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee.                              1    https://www.khronos.org/registry/webgl/



WSCG 2012 Communication Proceedings                                 105                                             http://www.wscg.eu
        64×64×256                128×128×256                256×256×256                 512×512×256
           Figure 1: Volumes of increasing resolution. Images rendered using the proposed technique.

our novel technique and architecture for volume render-              approach. While the user is interacting the number of
ing are presented. Section 4 presents our performance                slices is drastically reduced. At the end of an inter-
evaluation and discusses the results. Finally, Section 5             action a new image is rendered with the whole set of
concludes the paper.                                                 slices. This rendering step is carried out in the mobile
                                                                     device itself, or in a remote server in case of complex
2   PREVIOUS WORK                                                    or large models. Focused on the visualization of bones,
                                                                     Campoalegre [18] also proposed a client-server scheme
In the context of scientific visualization and volume
                                                                     where the model is compressed in the server side by the
rendering, a volume is usually represented as a set of
                                                                     Haar Wavelet function and reconstructed in the client
images/slices that are parallel and evenly distributed
                                                                     device. On the other hand, Congote et al. [1] imple-
across the volume. Equation 1 expresses the compu-
                                                                     mented a ray-based technique using the WebGL stan-
tation of the final color of a given pixel by composing
                                                                     dard.
the colors and opacities of the samples along a given
line across the volume, for a certain wavelength λ [10]:             All the aforementioned techniques share the same lim-
                                                                     itation: the lack of 3D textures on OpenGL ES 2.0 and
                                                                     WebGL severely restricts the size and resolution of the
             n                        n
 Cλ (x) =                                  (1 − α(x + rl ))          volumetric models that can be rendered. The problem is
            ∑ cλ (x + rk )α(x + rk ) ∏                               aggravated on WebGL, as these applications are usually
            k=0                    l=k+1
                                                         (1)         run on desktop computers equipped with large moni-
where Cλ (x) is the final color at a given position x, c(x+          tors.
rk ) is the color of the kth sample at position x+rk inside          The following proposals deal with the problem of very
the volume and a(x + rk ) is its corresponding opacity.              large volumetric models but in the context of PC or
Volume visualization algorithms have not been applied                workstations whose features differ from the intrinsic
to mobile devices until recently. First attempts over-               peculiarities of mobile devices and WebGL. A straight-
came the mobile devices limitations by employing a                   forward method to deal with a large volume is the brick-
server-based rendering approach. This approach relies                ing technique [2]. This technique subdivides the vol-
on a dedicated rendering server that carries out the ren-            ume into several smaller blocks in such a way that a sin-
dering of the volume and streams the resulting images                gle block fits into texture memory. Gunthe and Straßer
to the mobile client over a network [7, 9]. Also follow-             [3] used a wavelet based volume compression in order
ing a server-client scheme, Zhou et al. [22] employed a              to render large volume data at interactive frame rates
remote server to precompute a compressed iso-surface,                in a standard PC. Tomandl et al. [17] combined lo-
which is sent to the mobile device allowing a faster ren-            cal and remote 3D visualization (standard PC + high-
dering. Moser and Weiskopf [11] introduced an interac-               end graphics workstation) achieving low-cost but high-
tive technique for volume rendering on mobile devices                quality 3D visualization of volumetric data. Schneider
that adopts the 2D texture slicing approach. Noguera                 and Westermann [15] also overcame the problem of the
et al. [13] proposed an algorithm that overcomes the                 limited texture memory by compressing large scale vol-
3D texture limitation of mobile devices and achieves                 umetric data sets. Their solution takes advantage of
interactive frame rates by caching the geometry of the               temporal coherence on animated environments. Thelen
slices in a vertex buffer object (VBO). ImageVis3D [5]               et al. [16] introduced a dynamic subdivision scheme in-
is an iOS application that uses the 2D texture slicing               corporating multi-resolution wavelet representation to



WSCG 2012 Communication Proceedings                            106                                         http://www.wscg.eu
visualize data sets with several gigabytes of voxel data
interactively on distributed rendering clusters. Finally,
Xie et al. [21] subdivided the volume dataset into a
set of uniform sized blocks and combined early ray ter-
mination, empty-space skipping and visibility culling
techniques to accelerate the rendering process.

3   METHODOLOGY
This section details the algorithm, the software archi-
tecture and the implementation details that we propose
to render large volumetric models on handheld devices
and WebGL. Volumes are usually stored as a set of
slices, each one containing a 2D image that represents
the intersection of the volume with the slice. Common
volume rendering approaches [19] store these slices in
a 3D texture. However, neither mobile devices nor We-
bGL support 3D textures. This limitation can be over-
                                                                   Figure 2: Four mosaics stored in an RGBA texture.
come by storing the slices in a single 2D texture follow-
ing a mosaic configuration [1, 13]. Nonetheless, with-
out recurring to external servers, this technique limits
the size of the volume that can be stored because 2D
textures are considerably smaller than 3D textures.                                 Direct Volume Rendering
We extend this mosaic configuration solution to exploit                                     Technique
                                                                   Shader


the maximum texture capacity of the GPU in order to
deal with larger volumetric models. Our idea is based
on maximizing the multi-texture storage capacity of the
device by using all the available texture units and color                        Texture Coords.
channels. Usually, current handheld devices are able                                Adapter
to store up to 8 RGBA textures of 20482 texels each.
These numbers give us a maximum volume size of 5123
voxels when using our technique, which is considerably
                                                                   Textures




larger than the models rendered until now on mobile
platforms.                                                                               ...                Transfer
Our technique stores the 3D volume by placing each                                                          Function
slice one next to the other in a given color channel of a                           3D Model
2D texture. If the texture dimensions are exceeded, we
continue storing slices in the next color channel. This                 Figure 3: Software architecture of our proposal.
way, data-level parallelism is optimized [6, 20]. When
all the channels of the texture are completed, the re-
maining slices are stored in consecutive texture units            by volume rendering techniques to assign a color to
following the same pattern. Figure 2 shows an exam-               each voxel [19]. Thereby, different parts of the model
ple of a 2D texture where each color channel stores a             (bones, muscles, etc.) can selectively be emphasized
subset of slices in a mosaic configuration. Thus, each            by interactively modifying the transfer function.
RGBA color represents the values inside four different            In the shader section there are two modules: the texture
non-consecutive slices of the volume.                             coordinate adapter (TCA) and the selected direct vol-
Our storage configuration technique can be utilized               ume rendering (DVR) technique. The DVR technique
with any standard volume rendering approach based                 sends a 3D coordinate to the TCA and receives back an
on 3D textures. Figure 3 illustrates our proposal for a           interpolated grey-tone. This tone is then converted into
volume rendering architecture designed for OpenGL                 an RGBA value according to the given transfer func-
ES 2.0. This architecture is divided into two main                tion and used to compute the color of the corresponding
parts: the texture memory and the shader. The texture             fragment. It is important to remark that the DVR tech-
memory is used to store both the 3D volume and the                nique is unaware of the underlying 3D model storage
transfer function. The volume is stored using multi-              method. In fact, this architecture provides an straight-
textures as previously described. On the other hand, the          forward mechanism to adapt existing DVR techniques
transfer function refers to the texture normally required         to the platforms we are interested in this paper.



WSCG 2012 Communication Proceedings                         107                                          http://www.wscg.eu
               aux1     =    f loor(S ∗ z)                                          Stex   = min(Mx My , S)
                                            
                                      aux1                                         aux3    = f loor(zres ∗ Stex )
               aux2     =    f loor
                                      Mx My                                        aux4    = mod(aux3 + 1, Stex )
                                                                                              aux3     x
                                              
                                        Mx My
              depth =        min 1.0,                                                x1    =        +
                                           S                                                   Mx     Mx
                                                                                                          
                 zini   =    aux2 ∗ depth                                                     f loor aux
                                                                                                       My
                                                                                                         3
                                                                                                                  y
                             z − zini                                                y1    =                 +
                 zres   =                                                                          My             My
                              depth                                                                        
                                                                                                    aux4         x
                                       aux2                                          x2    = f ract            +
                  u1    =    f loor                                                                    Mx          Mx
                                      Maxch                                                               
                 ch1    =    mod(aux2 , Maxch )                                               f loor aux
                                                                                                       My
                                                                                                         4
                                                                                                                  y
                                                                                     y2    =                 +
                                                                                                   My             My
Listing 1: Computation of the texture unit u1 , the color                          next    = c + step(aux3 , aux4 )
channel ch1 and the corresponding (x, y, zres ) from the                             u2    = t + step(Maxch , next)
3D texture coordinate (x, y, z).                                                    ch2    = mod(next, Maxch )
                                                                                     v1    = tex2D( T [u1 ], (x1 , y1 ) )[ch1 ]
The TCA module transforms the 3D texture coordinate                                  v2    = tex2D( T [u2 ], (x2 , y2 ) )[ch2 ]
provided by the DVR technique to a format suitable for                               V     = mix(v1 , v2 , zres ∗ Stex − aux3 )
the volume representation and computes the grey-tone.
The returned value includes the trilinear interpolation
with the one-voxel distance neighbourhood. The pro-                         Listing 2: Computation of V , the value of the grey-tone
cess performed by this module can be decomposed into                        at position (x, y, z).
two differentiate tasks.
The first task consists of deriving the texture unit u1 , the
                                                                            The shaders represented by Listings 1 and 2 have care-
RGBA channel ch1 and the new local 3D texture coordi-
                                                                            fully been designed in order to avoid flow control op-
nates (x, y, zres ) from the original 3D texture coordinates
                                                                            erators, when possible, by promoting the use of built-in
(x, y, z). Listing 1 shows how to compute these values.
                                                                            GLSL functions like step. Observe that the use of flow
S is the number of slices in the volume, Mx × My is the
                                                                            control operators have a cost in the GPU of mobile de-
maximum number of slices in a mosaic stored in a sin-
                                                                            vices.
gle color channel of a texture, see Figure 2, and Maxch
is the number of channels per texture. As the slices
are stored in a consecutive manner along the channels
and texture units, each mosaic stores an interval of the
z-component of the full volume. The value zres is the                       4   RESULTS
residual z defined as the original z minus the z coordi-
nate of the voxel stored in the first texel of the selected                 In order to measure the effectiveness and performance
mosaic.                                                                     of our technique two prototypes have been imple-
                                                                            mented. The first one is a mobile application using
The second task is devoted to compute a pair of 4-                          OpenGL ES 2.0 and the second one is a desktop
tuples (u1 , ch1 , x1 , y1 ), (u2 , ch2 , x2 , y2 ) that define two         solution using WebGL. The selected technique for the
texels from the set of 2D textures, where ui , chi refers to                DVR module is a ray-based technique implemented in
the texture unit and the color channel, respectively, and                   the GPU [4, 8]. This technique basically consists of a
xi , yi are the 2D texture coordinates of the desired texel                 loop of n steps that traverses the volume accumulating
in the corresponding mosaic stored in the texture unit ui .                 color and opacities along a given ray-direction.
These two texels are neighbours along the z direction,
and are placed in the same or in consecutive mosaics.                       Recall that our architecture is independent of the visu-
The grey-tones of these texels are merged to simulate                       alization technique, and a faster texture-based approach
trilinear interpolation. Note that bilinear interpolation                   could have been used instead [13]. However, our goal
is automatically obtained by the texture interpolator of                    was not to measure the performance of the DVR tech-
the GPU. Listing 2 shows how to perform these opera-                        nique, but to assess the performance and scalability
tions. Here, T is an array of 2D samplers that contains                     when the resolution of the 3D model increases and mul-
the volumetric model.                                                       tiple textures are used.



WSCG 2012 Communication Proceedings                                   108                                          http://www.wscg.eu
    In our experiments, we used the CT human male dataset
    provided by the Visible Human Project2 . This dataset
    has a total resolution of 5122 × 1877 voxels.

    4.1    Results on Mobile Devices
    The experiments were conducted on an iPad2
    tablet. This device features a dual core PowerVR
    SGX543MP2 GPU and the iOS 5 operating system.
    The test application was developed as a native iOS
    application, using C++ and GLSL ES 2.0. According
    to Apple’s technical specifications, this device supports
    a maximum 2D texture size of 20482 texels, and up to
    8 texture units.
    Our experiments intended to cover all the range of
    model resolutions provided by our technique. We used
    a subset of the CT human dataset, shown in Figure 4,
    with different resolutions. For each experiment, a 100
    frame animation of the camera rotating around the
    CT human model was generated, and the mean times
    needed to render each frame were taken. Due to the                Figure 4: The CT human model on an iOS mobile de-
    tile-based deferred rendering architecture [14] used by           vice. Resolution 2563 using 4 textures and 80 steps-
    the GPU, OpenGL ES calls can be deferred until the                raytracing.
    scene is presented. In order to perform exact timings,
    we forced the frame rendering to finish by means of a
    glFinish call. Figure 5 shows graphically the results              1400
    obtained in milliseconds. The results for the following            1200
    experiments are included:                                          1000
                                                                        800
    • 1283 A: stored using one single-channel texture.                  600
                                                                        400
    • 1283 B: the same model as the previous experiment.                200
                                                                          0
    • 2563 A: stored using one RGBA texture.                                  128^3A
                                                                              128 3A    128^3B
                                                                                       128 3B    256^3A
                                                                                                 256 3A   256^3B
                                                                                                          256 3B 512^2x384
                                                                                                                 512 2 x 384


    • 2563 B: stored using four single-channel textures.
                                                                      Figure 5: Rendering time (ms) for an iPad2 mobile de-
    •   5122 × 384:   stored using six RGBA textures.                 vice. Screen resolution: 320×480 pixels. Raytracing:
                                                                      80 steps.
    The experiment 1283 A utilized a simplified version of
    the TCA module that only handles one mosaic, similar              the same TCA module. This suggests that the resolution
    to the proposal of Congote et al. [1], while the experi-          of the volumetric model does not have a significative
    ments 1283 B, 2563 A, 2563 B and 5122 × 384 used our              impact on the performance of the rendering process, as
    proposed TCA module that can deal with high resolu-               long as the model fits in the device’s memory. The per-
    tion 3D models.                                                   formance of the volumetric rendering depends on the
    Note that 5122 × 384 is the maximum resolution that               screen resolution and on the number of steps performed
    can be achieved by our technique on this device, be-              during the raycasting rather than on the model resolu-
    cause one texture unit is used by the transfer function           tion, as it was studied in [1, 13].
    and another one is required by the rendering technique.
                                                                      As stated above, the experiments 1283 A and 1283 B
    In all cases, uncompressed    20482
                                      textures were used.             were performed using the same dataset but different
    The ray-based DVR technique performed 80 steps in all             TCA modules. The experiment 1283 B used our
    the experiments. The screen resolution was 480 × 320              technique to compute the texture coordinates whereas
    pixels.                                                           1283 A used a simplified one. It was possible to use
    Results in Figure 5 show that the rendering times are al-         this simplified version because the model is small
    most constant among all the tested datasets when using            enough to fit in one mosaic. As shown in Figure 5, the
                                                                      rendering time of the second experiment doubles the
2   http://www.nlm.nih.gov/research/visible/                          time achieved by the first one. Our proposed technique
    visible_human.html                                                neither increases the number of texture accesses nor



    WSCG 2012 Communication Proceedings                         109                                        http://www.wscg.eu
adds additional conditional branches in the shader.               We found that Opera stopped working every time we
Nevertheless, it increases its longitude by about a dozen         tried to load more than one 81922 texture. The reason
of straightforward code lines to handle additional color          was that the NVIDIA driver exceeded the Windows im-
channels and texture units, see Listings 1 and 2. Albeit          posed rendering time limit (TDR) of two seconds. This
these lines only perform simple computations, they are            limited our experimentation to a model of 5122 × 1024
repeated once per step.                                           voxels, which is the maximum size that can be encoded
The experiments 2563 A and 2563 B employed a dataset              using all color channels of a single 81922 RGBA tex-
that is too large for one mosaic of 20482 pixels. Two             ture.
strategies can be used to store it: we can use either one         Interestingly, we did not run into these problems when
RGBA texture (2563 A) or four single-channel textures             we were experimenting with mobile devices, in spite of
(2563 B). Our experiments show that there are no signi-           the fact that both platforms share the OpenGL ES 2.0
ficative differences between both strategies in terms of          specification. In our opinion, today’s WebGL imple-
rendering times, and as a consequence, we can use the             mentations are still relatively immature and the tested
best suited for our particular application.                       mobile device proved to be a more predictable and sta-
Finally, we also prove that our technique allows us to            ble platform. As opposed to WebGL under Opera, the
exploit all the available texture resources of the mobile         iPad2 was able to correctly handle all our experiments,
device in order to render a very large dataset. We man-           including those using the maximum texture size on all
age to render a model of up to 5122 × 384 voxels while            the available texture units.
keeping the same rendering speed.                                 Nevertheless, comparing Figures 5 and 7 we can easily
                                                                  observe that WebGL is more than one order of magni-
4.2    Results on WebGL                                           tude faster than the selected mobile device when render-
                                                                  ing a similar volumetric model, even with a commodity
Following, we conducted a similar set of experiments
                                                                  desktop PC.
to test the performance of the WebGL implementation.
The tests were carried out using an Intel Core2 Quad              The experiments 5122 × 256A and 5122 × 256B (see
CPU Q6600, 4 GB of RAM, a GeForce 8800GT and                      Figure 7) used a model that can be stored in a single
Windows 7 SP1 64 bits. As web browser, we tested                  mosaic. Therefore, our proposed TCA module was not
Opera 11.50 labs (build 24661).                                   strictly needed for such a small model. As stated above,
                                                                  both experiments differed in the TCA module. The dif-
The selected GPU supports 2D texture sizes of up to
                                                                  ference in time shows the cost of including our module
81922 texels and provides 16 texture units. Given that
                                                                  to deal with large models. We can clearly see that the
the texture size is considerably larger than the provided
                                                                  GPU handles the additional operations without a no-
by the iPad2, we used a larger subset of the CT human
                                                                  ticeable increment of time.
dataset for our experiments, as shown in Figure 6.
                                                                  The last experiment (5122 × 1024) used a large model
In order to measure the rendering times, we forced the
                                                                  that cannot be encoded on one mosaic in a conventional
WebGL canvas to redrawn continuously and counted
                                                                  way. Therefore, our TCA module is mandatory to ren-
the number of frames rendered during an animation.
                                                                  der it. In this case, the four color channels of a texture
This animation consisted of the camera rotating around
                                                                  were used, and thus, the model was four times larger
the model during 5 seconds. Figure 7 shows graphi-
                                                                  than the one used in the previous experiments. This ex-
cally the mean times needed to render each frame in
                                                                  periment showed that the rendering time is greater than
milliseconds. The results for the following experiments
                                                                  in the previous experiments. This result is somewhat a
are included:
                                                                  surprise, since the texture dimensions, operations and
                                                                  texture fetches are the same. In fact, the only difference
• 5122 × 256A: stored using one single-channel tex-
                                                                  is the number of color channels.
  ture.

• 5122 × 256B: the same model as the previous exper-              5   CONCLUSIONS
  iment.                                                          Due to the today’s mobile GPU limitations, it was not
•   5122 × 1024:   stored using one RGBA texture.                 possible to render volumetric models larger than 1283
                                                                  voxels on devices such as the Apple’s smartphones and
The experiment 5122 × 256A utilized the simplified                tablets. However, in this paper we have proposed a
version of the TCA module described in Section 4.1,               novel technique that enables mobile devices to render
while the experiments 5122 × 256B and 5122 × 1024                 very large volumes by using multi-texturing to encode
used our proposed TCA module. In this case, we used               volumetric models on a set of RGBA 2D textures.
uncompressed 81922 textures and 128 steps for the ray-            We have also proposed a simple and easy to implement
casting. The WebGL canvas resolution was 8002 pixels.             architecture that can be used to adapt any existing di-



WSCG 2012 Communication Proceedings                         110                                         http://www.wscg.eu
              Figure 6: Visible human dataset rendered with WebGL, resolution: 5122 × 1024 voxels.

 140                                                             and the European Union (via ERDF funds) through the
 120                                                             research project P07-TIC-02773; and by the University
 100                                                             of Jaén through the project PID441012.
  80
  60
  40                                                             6     REFERENCES
  20
                                                                 [1]   J. Congote, A. Segura, L. Kabongo, A. Moreno,
   0
        512 2 x 256 A
         512^2x256A     5122 x 256 B
                        512^2x256B      512 2 x 1024
                                        512^2x1024                     J. Posada, and O. Ruiz. Interactive visualization
                                                                       of volumetric data with WebGL in real-time. In
Figure 7: Rendering time (ms) for a desktop PC with a                  Proceedings of the 16th International Conference
nVidia Geforce 8800GS. Screen resolution: 8002 pix-                    on 3D Web Technology, Web3D ’11, pages 137–
els. Raytracing: 128 steps.                                            146, New York, NY, USA, 2011. ACM.
                                                                 [2]   K. Engel, M. Hadwiger, J. M. Kniss, A. E. Lefohn,
rect volume rendering technique based on 3D textures                   C. R. Salama, and D. Weiskopf. Real-time vol-
to mobile devices and WebGL.                                           ume graphics. In ACM SIGGRAPH 2004 Course
Our experiments have proved that we can render vol-                    Notes, SIGGRAPH ’04, New York, NY, USA,
umes of up to 5123 × 384 voxels on a mobile device                     2004. ACM.
without decreasing the rendering speed. The proposed             [3]   S. Guthe and W. Straßer. Real-time decompres-
technique is also very akin to WebGL, because this                     sion and visualization of animated volume data.
standard shares the same limitations that mobile de-                   Proceedings of the IEEE Visualization Confer-
vices, mainly the lack of 3D texture support.                          ence, pages 349–356, 2001.
Regarding future works, we plan to study texture com-            [4]   M. Hadwiger, P. Ljung, C. R. Salama, and
pression in order to reduce cache issues and improve ef-               T. Ropinski. Advanced illumination techniques
ficiency. Furthermore, we plan to test the performance                 for GPU-based volume raycasting. In ACM SIG-
problems when transmitting large volumes across the                    GRAPH 2009 Courses, SIGGRAPH ’09, pages
Internet on WebGL. We want to explore multiresolution                  2:1–2:166, New York, NY, USA, 2009. ACM.
techniques in order to optimize network bandwidth.               [5]   ImageVis3D. ImageVis3D: A real-time volume
Progressive refinement techniques can probably be used                 rendering tool for large data. scientific computing
in this context to improve the user interaction experi-                and imaging institute (sci), 2011. [accessed 29
ence with the volume.                                                  September 2011].
                                                                 [6]   F. Ino, S. Yoshida, and K. Hagihara. RGBA pack-
ACKNOWLEDGEMENTS                                                       ing for fast cone beam reconstruction on the GPU.
This work has been partially supported by the Span-                    In , Proceedings of the SPIE Medical Imaging,
ish “Ministerio de Ciencia e Innovación” and the Eu-                   2009.
ropean Union (via ERDF funds) through the research               [7]   S. Jeong and A. E. Kaufman. Interactive wire-
project TIN2011-25259; by the “Consejería de Inno-                     less virtual colonoscopy. The Visual Computer,
vación, Ciencia y Empresa” of the “Junta de Andalucía”                 23(8):545–557, 2007.



WSCG 2012 Communication Proceedings                        111                                         http://www.wscg.eu
[8]    J. Kruger and R. Westermann. Acceleration tech-              [21] K. Xie, J. Yang, and Y. , Zhu. Real-time visu-
       niques for GPU-based volume rendering. In Pro-                    alization of large volume datasets on standard pc
       ceedings of the 14th IEEE Visualization 2003                      hardware. Computer Methods and Programs in
       (VIS’03), VIS ’03, pages 38–, Washington, DC,                     Biomedicine, 90(2):117–123, 2008.
       USA, 2003. IEEE Computer Society.                            [22] H. Zhou, H. Qu, Y. Wu, and M. yuen Chan. Vol-
[9]    F. Lamberti and A. Sanna. A solution for display-                 ume visualization on mobile devices. In 14th
       ing medical data models on mobile devices. In                     Pacific Conference on Computer Graphics and
       SEPADS’05, pages 1–7, Stevens Point, Wiscon-                      Applications, pages 76–84, 2006.
       sin, USA, 2005. World Scientific and Engineering
       Academy and Society (WSEAS).
[10]   M. Levoy. Display of surfaces from volume data.
       IEEE Comput. Graph. Appl., 8:29–37, May 1988.
[11]   M. Moser and D. Weiskopf. Interactive Volume
       Rendering on Mobile Devices. In Workshop on
       Vision, Modelling, and Visualization VMV ’08,
       pages 217–226, 2008.
[12]   A. Munshi, D. Ginsburg, and D. Shreiner.
       OpenGL(R) ES 2.0 Programming Guide.
       Addison-Wesley Professional, 1 edition, 2008.
[13]   J. Noguera, J. Jiménez, C. Ogáyar, and R. Segura.
       Volume rendering strategies on mobile devices. In
       International Conference on Computer Graphics
       Theory and Applications (GRAPP 2012). Rome
       (Italy), pages 447–452, 2012.
[14]   Power VR. PowerVR Series5 Graphics SGX ar-
       chitecture guide for developers, 2011.
[15]   J. Schneider and R. Westermann. Compression
       domain volume rendering. Proceedings of the
       IEEE Visualization Conference, pages 293–300,
       2003.
[16]   S. Thelen, J. Meyer, A. Ebert, and H. Hagen.
       Giga-scale multiresolution volume rendering on
       distributed display clusters. Lecture Notes in
       Computer Science (including subseries Lecture
       Notes in Artificial Intelligence and Lecture Notes
       in Bioinformatics), 6431 LNCS:142–162, 2011.
[17]   B. Tomandl, P. Hastreiter, C. Rezk-Salama, K. En-
       gel, T. Ertl, W. Huk, R. Naraghi, O. Ganslandt,
       C. Nimsky, and K. Eberhardt. Local and remote
       visualization techniques for interactive direct vol-
       ume rendering in neuroradiology. Radiographics,
       21(6):1561–1572, 2001.
[18]   L. C. Vera. Volumetric medical images visualiza-
       tion on mobile devices. Master’s thesis, Polytech-
       nic University of Catalonia, 2010.
[19]   D. Weiskopf. GPU-based interactive visualiza-
       tion techniques. Mathematics and visualization.
       Springer, 2007.
[20]   C. Wooley. GPU Gems 2: Programming Tech-
       niques for High-Performance Graphics and
       General-Purpose Computation (Gpu Gems),
       chapter 35, pages 557–571. Addison-Wesley Pro-
       fessional, 2005.




WSCG 2012 Communication Proceedings                           112                                       http://www.wscg.eu
                                 Voxel-Space Shape Grammars
            Zacharia Crumley                           Patrick Marais                                 James Gain
         University of Cape Town                   University of Cape Town                      University of Cape Town
              South Africa                               South Africa                                 South Africa
        zacharia.crumley@gmail.com                  patrick@cs.uct.ac.za                          jgain@cs.uct.ac.za


                                                         ABSTRACT
We present a novel extension to shape grammars, in which the generated shapes are voxelized. This allows easy
Boolean geometry operations on the shapes, and detailing of generated models at a sub-shape level, both of which
are extremely difficult to do in conventional shape grammar implementations. We outline a four step algorithm
for using these extensions, discuss a number of optional enhancements and optimizations, and test our extension’s
performance and range of output. The results show that our unoptimized algorithm is slower than conventional
shape grammar implementations, with a running time that is O(N 3 ) for a N 3 voxel grid, but is able to produce a
broad range of detailed outputs.

Keywords:
procedural generation, shape grammars, voxels

1    INTRODUCTION                                                         eration software, such as SpeedTree1 , Terragen2 , and
For video games, virtual environments, and cinema spe-                    CityEngine3 . There are many procedural generation al-
cial effects, cost-effective content creation is an increas-              gorithms [14] but our research focuses specifically on
ing concern. The amount of models, animations, tex-                       shape grammars [18]. These are a type of formal gram-
tures, and sounds needed for these applications has                       mar, consisting of an axiom (the initial item to begin
been steadily growing with the increase in computa-                       with) and a set of production rules which modify, add,
tional power and the quality of graphics. It is now at                    or replace items.
a point where hundreds of modellers, animators, and                       Shape grammars are distinguished from conventional
artists will work for months or years to create the con-                  grammars, in that their rules operate directly on ge-
tent necessary for a single mainstream video game or                      ometric shapes instead of symbols from an alphabet.
blockbuster film. The large size of these teams means                     Their production rules include geometric operations on
the costs involved are significant, and in spite of the                   shapes (such as rotation, scaling, etc.), in addition to
number of people working on the project, long develop-                    shape replacement (as grammars do with symbols). An
ment times are still the norm. For this reason, content                   example of a basic shape grammar is shown in figure 1.
creators have begun turning to procedural generation,                     Shape grammars were originally developed in architec-
as a way of decreasing costs and shortening develop-                      ture as a tool for formalizing architectural design. Al-
ment times.                                                               though still used for that purpose, they are also find-
Procedural generation refers to methods designed to                       ing use in computer science as a method for procedu-
algorithmically generate content, instead of having it                    rally generating models of buildings and other struc-
hand-crafted. Minimal human interaction is required –                     tures. This is the application our research focuses on.
generally limited to setting the initial parameters of the                Conventional shape grammars operate on mesh repre-
algorithm, or providing example inputs.                                   sentations of shapes. These are moved, rotated, subdi-
Today procedural generation is increasingly used to                       vided and otherwise operated on until a final collection
generate large amounts of high quality content, par-                      of shape meshes is produced: the output of the shape
ticularly plants, landscapes, and textures. This is ev-                   grammar.
idenced by the growth of commercial procedural gen-                       However, there are two major problems with conven-
                                                                          tional mesh-based shape grammars, as used in proce-
                                                                          dural generation and both are difficult to solve.
Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without                Firstly, it is difficult to robustly apply constructive solid
fee provided that copies are not made or distributed for profit           geometry (CSG) or Boolean geometric operations on
or commercial advantage and that copies bear this notice and
                                                                     1 http://www.speedtree.com/
the full citation on the first page. To copy otherwise, or re-
                                                                     2 http://www.planetside.co.uk/
publish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee.                              3 http://www.procedural.com/




WSCG 2012 Communication Proceedings                                 113                                           http://www.wscg.eu
 Axiom           Rule 1                     Rule 2
                                                                      stages, and produces a mesh model, suitable for use in
                                                                      real-time, or offline, 3D graphics. We also discuss opti-
                                                                      mizations and optional steps for the algorithm, as well
                                                                      as testing its performance and range of output.



                                                                      2   RELATED WORK
Figure 1: A simple shape grammar that produces an
infinite series of 2D stairs, and its first three iterations.         Stiny and Gips [18] first developed shape grammars in
                                                                      an attempt to formalize architectural design. Although
meshes: overlapping edges and vertices must be identi-                still used in architecture, they have also been adapted
fied and trimmed or removed, whichever is appropriate.                for use in the procedural generation of structures for
This can lead to complications around numeric stabil-                 other applications [19, 12].
ity, slivers, degenerate triangles, and other issues. Al-             Early shape grammars were simplistic with a limited
ternatively, this redundant geometry can be left hidden,              range of output, but over time several ideas from pro-
but this is inefficient. It may also be necessary to cre-             cedural generation, most notably from L-systems, were
ate new geometry. This process is complex, and there                  incorporated into shape grammar implementations.
are edge cases that remain problematic. Even with such                Most notably, environmental sensitivity and stochastic
an algorithm, the overlap between different shapes can                rules [3]. With environmental sensitivity, rules can
lead to texture seams. This is visually unappealing and               query the current set of shapes and adjust their output
difficult to overcome.                                                based on the information they get. Stochastic rules
The second limiting problem of mesh shape grammars                    introduce randomness, by randomly choosing different
is that texturing is done at a per-shape level. The faces             outputs and parameters to introduce variation in the
of each shape, or pre-made piece of geometry, have                    shapes generated.
fixed texture coordinates and associated 2D textures
                                                                      Shape grammars were later extended to building gen-
that are projected onto the faces. This means it is diffi-
                                                                      eration using split grammars [19], which focus on sub-
cult to have texture details that span multiple shapes, or
                                                                      division of shapes. For example, a building’s wall is
control the textures at a sub-shape level.
                                                                      divided first into floors, then the floors into different
For example, it would be difficult to create racing strips            rooms, and finally the walls of the rooms into different
running along a car model produced by a shape gram-                   windows. Split grammars are particularly well suited to
mar, since the different sections of the car are made up              façade generation [19] due to the regular, grid-like lay-
of different polygons created by different rules.                     out of building windows. They work by recursive sub-
Modifying shape grammars to operate in a voxel-space                  division of a shape, guided by following productions
solves these limitations. CSG operations on voxels are                from a rule set. The final set of shapes that arise from
trivial, solving the first deficiency. For the second, tex-           the repeated subdivision form a model of the desired
tures can be assigned on a per-voxel basis, which allows              building wall. However, split grammars are less suc-
details to more easily span shapes and removes the tex-               cessful at creating internal structure. Early approaches
ture’s association with a specific shape.                             to this problem tended to be simplistic, such as using
Using voxels presents some challenges, such as the                    n-sided prisms as the split grammar axioms [4]. Subse-
need for large amounts of memory and storage, and                     quently, better methods were developed, such as start-
the discrete nature of the underlying grid, which can                 ing from the building’s footprint obtained from aerial
introduce aliasing artifacts and other sampling issues.               imagery [7], and creating the building’s structure with
However, these problems can be dealt with, or worked                  shape grammar rules [12].
around, using tree data structures for efficiently manag-             Along with the other extensions mentioned, the features
ing space, which also allow us to use high resolution                 of split grammars have since been incorporated into
voxel grids, reducing the impact of aliasing artifacts.               current grammar implementations, unifying all features
This paper presents our preliminary research into inter-              under one grammar, for greater ease-of-use [12].
preting shape grammars in a voxel space, in contrast to               The range of output possible from modern shape gram-
the traditional mesh geometry approach. Our goal is to                mar implementations is extremely broad [12] and as
extend the expressive range of shape grammars with the                a result, shape grammars are considered the industry
ability to easily and robustly apply Boolean geometry                 standard for procedurally generating architecture. They
operations.                                                           are able to create whole cities with realistic buildings
Our major, and novel, contribution is an algorithm for                (using additional techniques [15] to create the road net-
this process. The algorithm is made up of four main                   works and block layout). The best example of this in



WSCG 2012 Communication Proceedings                             114                                         http://www.wscg.eu
    practice is the CGA grammar of CityEngine4 , which                  Grammar Speciﬁction        Shape Grammar Interpretation
    procedurally creates cities and buildings.                                                        tagged shapes
    Example-based shape grammars can be used to gen-                                                    Shape Voxelization
    erate different models in the same style as the given-
                                                                                                      voxel grid with tags
    example. This can be done from an image of a build-
    ing’s façade [13], or from existing models [2].                       Detailing Rule Set              Voxel Detailing

    The problem of easily, and visually, editing shape gram-                                          detailed voxel grid
    mars has also been addressed by Lipp et al. [9] in their                                             Mesh Generation
    work on interactively editing shape grammars for archi-
                                                                                                      mesh of voxel grid
    tecture. Their approach is primarily concerned with op-
    erating on the grid-like façades of buildings, but does                                            Mesh Post-processing
    feature methods for assisting in the overall building
    structure creation.                                                Figure 2: A flow chart of our algorithm. The cyan boxes
                                                                       are user inputs; the grey boxes are the four stages of the
    Recent work [1, 5] has also developed methods for au-              algorithm. Arrow labels show the output of each stage.
    tomatically creating a structural skeleton for models              Mesh post-processing is optional.
    generated by shape grammars. These skeletons can
    then be used to create animations or run structural simu-
    lations on the generated models using a physics engine.            Shape Grammar Specification: This is the set of pro-
                                                                         ductions and associated parameters for the shape
    However, shape grammars do have limitations. In split                grammar itself.
    grammars, shapes that span multiple subdivisions, and              Detailing Rule Set: The collection of rules that are
    shape intersections, are difficult to handle gracefully.             used to assign visual detailing information to the
    In addition, particularly unusual building designs with              voxel grid. This information is used when display-
    complex elements, such as tunnels and interior hollows,              ing the created model.
    are very hard to generate.
    Voxels have seen previous use in procedural genera-                The five stages of the algorithm are:
    tion, predominantly in games 5 and terrain represen-
    tation. However, 3D texture synthesis methods have
                                                                       1. Shape Grammar Interpretation: The input gram-
    been extended to create 3D models. Merell’s algorithm
                                                                          mar is run to produce a collection of shapes.
    [11] works by assigning a cuboid section of geome-
                                                                       2. Shape Voxelization: The shapes from the previous
    try to each voxel type, and then keeping a record of
                                                                          step are voxelized into a voxel grid.
    how these cubes of geometry can be placed adjacent to
                                                                       3. Voxel Detailing: Surface voxels in the grid are as-
    each other while keeping the resulting model consis-
                                                                          signed visual detailing information.
    tent. Texture synthesis methods are employed to create
                                                                       4. Mesh Generation: A mesh representation of the
    a, potentially infinite, voxel grid that corresponds to a
                                                                          voxel shape is produced, using the marching cubes
    consistent model. One downside is that this requires the
                                                                          algorithm.
    manually created cuboid sections of geometry.
                                                                       5. Mesh Post-processing (optional) The generated
    Another modelling approach that allows Boolean                        mesh is smoothed and refined.
    geometry operations while avoiding the issues around
    mesh-based CSG was proposed by Leblanc et al.
                                                                       Below, we cover each of the five stages in detail, ex-
    [8]. Their approach allows modeling of shapes using
                                                                       plain their operation, what inputs they use, and what
    Boolean geometry operations, but is substantially more
                                                                       outputs they generate. Figure 2 shows an overview of
    complex than a voxel-based approach, and does not
                                                                       the process.
    solve the issue of creating surface detail at a sub-shape,
    or trans-shape, level.
                                                                       3.1    Shape Grammar Interpretation
    3   FRAMEWORK                                                      The first stage of the algorithm requires that we specify
    The process of interpreting a shape grammar to produce             a shape grammar and then iterate it to produce the out-
    a model in our framework consists of five stages (one              put set of shapes. This step is very similar to applying
    of which is optional), and requires two inputs from the            a conventional shape grammar, with some minor dif-
    user. The final output is a textured mesh, suitable for            ferences. A set of shape grammar rules, and an axiom
    use in modern graphics applications. The two user in-              shape, are provided by the user. This rule set is then
    puts are:                                                          run on the axiom, producing new shapes and modify-
                                                                       ing existing shapes on each iteration of the rules. This
4   http://www.procedural.com/                                         continually-updated set of shapes (the current shape set)
5   http://www.minecraft.net/                                          converges to the final output of the shape grammar. An



    WSCG 2012 Communication Proceedings                          115                                           http://www.wscg.eu
  Axiom: building_base
  rule 0: building_base -> roof (shape: triangle) | tower (shape: rectangle; symmetry:   rule 2: tower -> tower_peak (shape: triangle) |   rule 4: door -> door_arch (shape: circle)
  reﬂective) | window (shape: square; symmetry: reﬂective) | door (shape: square)        tower_window (shape: rectangle)
  rule 1: roof -> chimney (shape: rectangle)                                             rule 3: window -> window_arch (shape: triangle)




Figure 3: A simple shape grammar being interpreted in parallel to form a basic building. Grey shapes are additive,
and red ones are subtractive. Position and size information in the example grammar are not shown, for the sake of
simplicity. Note that the final step involves the symmetric copies being created, as the grammar only requires two
iterations to complete.

Algorithm 1 Shape Grammar Interpretation                                                       mation on these extensions, we refer readers to the lit-
  currShapeSet ← {Axiom}                                                                       erature [16, 12, 3]. We also have a collection of stan-
  iterations ← 0                                                                               dard utility functions for common operations, such as
  while       iterations < MaxIterations      and                                              scaling, translation, rotating, and hollowing shapes. All
  currShapeSet.hasNonTerminals() do                                                            that is required from this stage of the generation process
     if parallelExecution = TRUE then                                                          is a specification of the final set of shapes.
        for all i ∈ currShapeSet do                                                            There are two grammar extensions that we found very
           i ← doRuleDerivation(i)                                                             useful for generating models in our experiments.
        end for
     else                                                                                      First is the tagging of shapes with metadata. One of the
        a ← getFirstNonTerminal(currShapeSet)                                                  operations that the shape grammar rules can perform is
        a ← doRuleDerivation(a)                                                                to add metadata tags to shapes in the current shape set.
     end if                                                                                    We implemented these as arbitrary strings. These serve
     iterations ← iterations + 1                                                               to preserve additional information about the shapes that
  end while                                                                                    can be used in later stages of the generation algorithm.
  for all i ∈ currShapeSet do                                                                  For example, in generating a castle, a cylinder (and its
     if i.hasSymmetry() then                                                                   children if recursive tagging is used) could be tagged
        s ← i.createSymmetricCopies()                                                          with “type:tower” and “material:stone”. These tags in-
        currShapeSet.insert(s, i − 1)                                                          dicate additional properties of the shape that enhance
     end if                                                                                    later detailing and texturing.
  end for                                                                                      Secondly, we support the specification of symmetry in
                                                                                               the grammar rules. Our grammar implementation has
                                                                                               special operators for indicating that a shape, or group
example of this stage is shown in figure 3. Pseudocode                                         of shapes, (and any child shapes that derive from them)
for this process is shown in algorithm 1.                                                      should be cloned to create symmetrical versions.
This process will terminate under one of two condi-                                            We support two types of symmetry: rotational and re-
tions: Either after a user-defined maximum number of                                           flective. In rotational symmetry, three arguments are
iterations, or once all shapes are terminal and none of                                        provided to the operator, from which positioning infor-
the rules can be performed on the set of shapes.                                               mation for the symmetrical copies can be derived: Prot
Rules can be interpreted in parallel (as done in L-                                            - the center point around which the symmetric branches
systems [16]), or in series (as done in traditional formal                                     are rotated; Vrot - a vector normal to the plane of rota-
grammars [12]). These are appropriate in different situ-                                       tion, and Nrot - the number of rotational copies to create.
ations, depending on the type of model being generated                                         For reflective symmetry, only two arguments are re-
by the shape grammar. Serial rule derivation is suitable                                       quired to fully construct the mirror copies of the shape,
for most situations, except for models that have fractal                                       or group of shapes, to be reflectively copied: Pre f - a
qualities, where parallel rule derivation is advised.                                          point on the plane of reflection, and Vre f - a normal to
Any of the many enhancements and extensions to gram-                                           the plane of reflection.
mar generation methods can be used here: environ-                                              Symmetry information is specified when the grammar
mental sensitivity, stochastic rules, a derivation tree for                                    is run, but symmetric copies are only added once the
querying earlier shape set states, and more.                                                   grammar rules terminate. This is done as a post-process
Our implementation includes these three extensions, as                                         because further shapes could be added to the set of
well as split grammar operators [12]. For more infor-                                          shapes undergoing symmetry, in iterations after the



WSCG 2012 Communication Proceedings                                                      116                                                              http://www.wscg.eu
symmetry is specified. Rather than tracking the sym-
metric copies and updating each of them for every
change in shape, we simply flag the set of shapes for
symmetry and wait until the rule derivation completes,
before creating the symmetric copies.
Once the shape grammar has finished, a full specifica-
tion of the final output set of shapes is passed to the
next stage. This includes positions, dimensions, orien-
tations, tags, and any other relevant information.
                                                                   Figure 4: The output of the simple building shape gram-
3.2    Shape Voxelization                                          mar from figure 3 after being voxelized. The colours of
In this phase, the shapes output from the shape gram-              the voxels correspond to the tags they inherited from the
mar are voxelized into a voxel grid. This is analogous             shapes. Grey indicates ‘material:wall’, brown indicates
to rasterizing vector graphics into a pixel format. An             ‘material:roof’, and the dark grey ‘material:chimney’.
example of this process is shown in figure 4, and pseu-
docode in algorithm 2.                                             grammar may generate unintended results, depending
                                                                   on the order in which the shapes are voxelized.
Algorithm 2 Shape Voxelization                                     To resolve this ambiguity, the shape grammar can as-
  shapes ← getShapeGrammarOutput()                                 sign a priority to the shapes. This is an integer that de-
  shapes ← sortByPriority(shapes)                                  termines when the shape will be voxelized. Before vox-
  shapes_bbox ← getBoundingBox(shapes)                             elization, the shapes are sorted by priority, and added in
  gridResolution ← getVoxelGridResolution()                        sorted order. This allows a user to control when shapes
  voxelGrid ← initializeEmptyGrid(gridResolution)                  are added, and resolve order-dependency issues.
  for all i ∈ shapes do                                            Finally, we can manually edit the voxel grid once the
    i ← scaleShape(i, gridResolution, shapes_bbox)                 shapes have been voxelized. This could be done to al-
  end for                                                          low hand-crafted modifications to the output of a gram-
  for all i ∈ shapes do                                            mar, or because the user is dissatisfied with some aspect
    voxelGrid.voxelizeShape(i)                                     of the output that is difficult to correct in the grammar.
  end for
  return voxelGrid                                                 Manual editing is important for artists and modellers,
                                                                   and our shape grammar extensions do not restrict it at
                                                                   all, although it requires voxel editing software.
Due to the large memory requirements of storing voxel
grids naïvely, it is infeasible to store the grid as a 3D          The final output of this stage is a 3D voxel grid, where
array. Our implementation uses an octree, to efficiently           each voxel is either solid or empty, and may have meta-
manage space [17].                                                 data tags associated with it.
It is possible to use other tree data structures for stor-         3.3    Voxel Detailing
ing the voxel grid, such as point region octrees (PR-
octrees), kd-trees, or R-trees. In the general case, where         In this stage of the algorithm, voxels are assigned an
no assumptions can be made about the data sets to be               appearance in the final model. This can include, but is
stored, and no special look-ups are required, the best             not limited to, texturing information, bumps maps, dis-
option is an octree [17]. This is because the other tree           placement maps, lighting information, and materials.
types all require re-balancing (an expensive operation).           This is done on a per-voxel basis, by a user-created rule
                                                                   set which operates on each voxel individually. These
Tags associated with the shapes to be voxelized are                rules may iterate over the voxels multiple times, al-
assigned to the relevant voxels. In the case of over-              lowing the creation of complex multi-pass detail. For
lapping shapes, it is possible that a voxel may inherit            example, cellular automata patterns could be created,
tags from multiple shapes. This is not problematic at              since they map very well onto the discrete, gridded na-
this stage, but may cause ambiguities during detail-               ture of voxels. The scope of these rules is extremely
ing, which could have unintended consequences. Users               broad, and features such as context-sensitivity and ran-
should bear this in mind when designing grammars.                  domness can easily be included. Everything from as-
The order in which shapes are added is also important,             signing a simple texture based on position, to random-
because shapes may be additive or subtractive (additive            ized complex multi-pass procedural methods are possi-
for creating solid structures, or subtractive for carving          ble. A simple example of a voxel grid after undergoing
empty spaces out of solids). Adding and subtracting                detailing is shown in figure 5, and pseudocode of the
geometry in this manner is not commutative. Hence a                detailing process is found in algorithm 3.



WSCG 2012 Communication Proceedings                          117                                         http://www.wscg.eu
                                 Detailing Rules
rule_0: if "material:roof" in tags then texture = red_tile
rule_1: if "material:chimney" in tags then texture = black_stone
rule_2: if "material:stone" in tags then texture = random_selection(grey_brick,
dark_brick)
rule_3: if voxel.borders_enclosed_space() == true then texture = blue_paint




                                                                                        Figure 6: Our Enterprise model with two different de-
                                                                                        tailing rule sets applied to it. Above, with its original
                                                                                        detailing; below, with a camouflage pattern. This shows
                                                                                        how detailing rule sets that are not reliant on shape tags,
                                                                                        such as camouflage, can be applied to any voxel grid.
Figure 5: The voxel grid from figure 4 after detailing.
Each voxel has been assigned a texture in accordance                                    any voxel grid, and does not need to concern itself with
with the detailing rule set supplied. Non-surface voxels                                how that data set was produced.
are ignored, and are not displayed in the diagram.                                      It is possible to have a detailing rule set that is com-
Algorithm 3 Voxel Detailing                                                             pletely independent of metadata tags. For example, de-
  detailingRuleSet ← getDetailingRuleSet()                                              tailing that creates a consistent pattern across the entire
  maxIterations ← getMaxDetailingIteration()                                            model without using the context information from the
  sur f aceVoxels ← voxelGrid.getSurfaceVoxels()                                        tags. These detailing rules will work on any model pro-
  for i = 1 to maxIterations do                                                         vided to them, regardless of tags. An example of such a
    for all j ∈ sur f aceVoxels do                                                      detailing rule set being applied to a model intended for
        n = voxelGrid.getNeighbouringVoxels( j)                                         a different rule set is shown in figure 6.
         j.detailTags ← detailingRules.runRules( j, n)                                  However, in most practical situations, we expect that
    end for                                                                             rules from the detailing rule set will be dependent on
  end for                                                                               metadata tags in the voxel data set. For example, de-
                                                                                        tailing a house’s walls with brick textures and the doors
                                                                                        with wooden ones requires that the two parts of the
Detailing is done on a per-voxel level as opposed to                                    model be distinguished. Hence the user should en-
the per-shape level of conventional shape grammars be-                                  sure that the shapes in the shape grammar are properly
cause this allows more complex procedural detailing                                     tagged for the detailing stage.
of generated models, and it is much easier for detail
                                                                                        Running snippets of code for each voxel in a grid can
features to span shapes and work on sub-shape scales.
                                                                                        be extremely slow, especially so if the grid is large, or
This also circumvents the problem of texture seams be-
                                                                                        the rule’s code is complex. For this reason, we only run
tween adjacent shapes prevalent in conventional gram-
                                                                                        the rule set on surface voxels in the grid.
mars. The disadvantage to this freedom is more com-
plexity for the user. This complexity could be reduced                                  We define a surface voxel to be any solid voxel in the
in two ways. Firstly, by creating a visual rule editor to                               grid which is 26-connected to at least one empty voxel.
use, as opposed to text-based programming. Secondly,                                    Because the marching cubes algorithm does not gener-
by designing an interface that allowed rapid prototyping                                ate triangles for completely empty or solid space, only
of rules on small examples, to quickly detect problems.                                 voxels on the border between solid and empty space
However, we did not implement these, and leave them                                     will affect the final model. All others can be ignored.
to future work.                                                                         Using the hierarchical structure of the octree, surface
Relevant details about the voxel are passed to the rule                                 voxels can be quickly identified. If a node of the octree
set. In our implementation these details are: tags as-                                  does not have any children, then only the voxels around
sociated with the voxel; normal of the voxel; the max-                                  the edge of that node need be checked further. For all
imum resolution of the octree; the coordinates of the                                   reasonable models, this dramatically reduces the num-
current voxel; the count of the current iteration of the                                ber of surface voxel candidates that need to be checked,
rule set, and the above details for all neighbouring vox-                               improving speed by an order of magnitude or more.
els, within a user-specified radius.                                                    The final output of this step is a voxel grid where all
It should also be noted that this stage is independent of                               surface voxels have been assigned detailing informa-
previous steps. A detailing rule set can be applied to                                  tion. This information must unambiguously provide all



WSCG 2012 Communication Proceedings                                               118                                          http://www.wscg.eu
information required for rendering, either as is, or when          node, and the shape to be added. If the area covered by
converted to a mesh.                                               a child node is entirely within the shape, then that voxel
                                                                   is set with the shape’s information, if there is a partial
3.4    Mesh Generation and Post-processing                         intersection between the child node and the shape, then
There are many methods for rendering voxel grids.                  the algorithm is recursively called on that node.
These are often based on ray tracing, or point rendering.
                                                                   While faster, this is more complex to implement, and
In some situations it may be suitable to render the out-
                                                                   requires an exact collision detection algorithm. We
put of the shape grammar with these methods, but most
                                                                   suggest that future implementations make use of this
graphics applications today work with triangle meshes,
                                                                   method to greatly reduce run times.
not voxel data sets. For this reason, we need to convert
our voxel data set into a mesh that can be used in con-            The implementation of marching cubes can also be sub-
ventional raster graphics applications, such as modern             stantially accelerated by only marching over the surface
3D game engines.                                                   voxels of the voxel grid. Since solid or empty regions
The marching cubes algorithm [10] is a well-                       will not produce triangles, the voxels of those regions
established solution to the problem of extracting a                need not be processed. The list of surface voxels from
mesh representation of a voxel grid or isosurface.                 the detailing step can be re-used here.
We make use of it to produce a mesh version of our
generated model.                                                   4     TESTING AND EXPERIMENTATION
The algorithm outputs a list of triangles, each of which
                                                                   In order to evaluate our voxel-space extensions to
can be associated with a voxel in the input grid. Each
                                                                   shape grammars we undertook three experimental
triangle is then assigned textures, materials, and other
                                                                   tasks: performance testing, where we analyzed the
detailing information from the surface voxel.
                                                                   time and memory required; variation testing, where
Hence we end up with a fully textured and detailed                 we produce multiple similar models from a single
mesh representation of the voxel grid that the original            grammar; and output range testing, where we examine
shape grammar produced.                                            the range of outputs our algorithm can produce, and its
The mesh produced by the marching cubes algorithm is               ability to generate models of well-known structures.
suitable for direct use in graphics applications, but its
visual quality could be improved by post-processing.               4.1    Performance
One of the problems with the marching cubes algorithm
is that the output mesh has visual artifacts caused by the         We analyzed our algorithm’s performance across a va-
discrete nature of the voxels. Curves in particular, are           riety of voxel grid sizes and user inputs. The two main
not fully captured during the meshing process, and will            results of interest are the time taken to generate a model,
instead appear ‘bumpy’, although increasing the reso-              and the peak memory usage of the process.
lution of the voxel grid can reduce this.                          We decomposed timing into the four stages of the al-
The severity of this problem can be reduced by an ap-              gorithm to get an idea of their relative durations (post-
propriate mesh smoothing algorithm, which will signif-             processing was excluded as it is an optional step).
icantly decrease the impact of such artifacts [6].                 Testing was performed on a PC with an Intel Core 2
It should be noted though, that naïve smoothing algo-              Duo clocked at 2.4Ghz and 3 gigabytes of RAM.
rithms can lose details that are not artifacts, and should         Performance testing was conducted with a selection of
be retained, such as sharp corners. For this reason,               36 shape grammar and detailing rule-set combinations,
we recommend the use of one of the more advanced                   at 4 different voxel grid resolutions. The selection of
smoothing algorithms, which will retain these features.            grammars and rule sets was specifically chosen to en-
There are a number of such algorithms, but in general              compass a wide range of complexity. figure 7 shows
these advanced methods of smoothing come at the cost               the timing results across all of the 36 models.
of more complexity and a longer running time.
                                                                   Before analyzing the results it should be noted that
3.5    Optimizations                                               our implementation was strictly intended as a single-
There are several optimizations to our algorithm that we           threaded proof-of-concept. Hence, performance was
did not implement due to time constraints. These have              not a priority, and there is large scope for improvements
the potential to dramatically reduce running times, and            in this area (as mentioned in section 3.5.) Nonethe-
hence we discuss them here.                                        less, we include our results as we believe they provide
Voxelization of shapes can be performed extremely ef-              a baseline for comparison to future implementations of
ficiently by exploiting the hierarchical nature of the oc-         our work.
tree. Beginning at the root of the tree, query the inter-          The first thing to note is that the shape grammar inter-
section between each of the eight children of the octree           pretation is orders of magnitude faster than the other



WSCG 2012 Communication Proceedings                          119                                          http://www.wscg.eu
                                                                                               theme and style, from a single shape grammar and de-
                              900
                              800
                                                                                               tailing rule set, by randomizing the parameters of the
  Running Time (in seconds)


                              700                                                              shape grammar and detailing rules.
                              600
                              500                                                              To test the power of our shape grammar extensions, we
                                                                    Mesh Generation
                              400                                   Voxel Detailing            created shape grammars and detailing rule sets repre-
                              300                                   Shape Voxelization
                              200
                                                                                               sentative of a broad variety of models, including imita-
                              100                                                              tions of well-known existing structures. A selection of
                                0                                                              these generated models are shown in figure 9.
                                    64       128       256    512
                                         Size of Voxel Grid
                                                                                               5   LIMITATIONS
Figure 7: A cumulative graph of the average times taken                                        There are two limitations to our voxel-space shape
for our algorithm to run on 36 different inputs, covering                                      grammar algorithm that could restrict its potential uses.
a range of complexities. Shape grammar interpretation
is not shown as it was negligible compared to the other                                        Firstly, in order to obtain a high quality model from
three stages.                                                                                  a voxel data set, the set must be at a high resolution,
                                                                                               so as to remove “blocky” visual artifacts caused by the
                                                                                               discrete nature of a voxel grid. Mesh smoothing as a
stages, due to its independence from the voxel grid res-
                                                                                               post-process helps, but it is not sufficient on its own.
olution. The average interpretation time was 50 mil-
liseconds. Due to the minuscule relative time, grammar                                         However, the higher the resolution, the slower the voxel
interpretation is not shown in figure 7.                                                       detailing process is. This is because each voxel in the
                                                                                               model must be detailed, and the number of voxels is
As expected the running times of the other stages of
                                                                                               cubic in the dimensions of the voxel grid.
the algorithm is approximately cubic in the size of the
voxel grid. This is expected, as their running time is                                         Secondly, texturing at a per-voxel level may be insuf-
directly proportional to the number of voxels to operate                                       ficient in certain cases, such as for curved surface de-
on, which scales cubically with the size of the grid.                                          tails, where the discrete nature of the underlying voxel
                                                                                               data can cause visual artifacts. For example, an elabo-
The biggest cause of variation in running times is the
                                                                                               rate spiral design with fine curved detail on the side of
number of iterations in the surface detailing. Because
                                                                                               a spaceship would almost certainly run into sampling
each voxel must be processed for every iteration, the
                                                                                               issues if created with a detailing rule set.
number of iterations makes a large difference in the
amount of processing to be done, especially for higher                                         A possible solution to this problem would be allowing
voxel grid resolutions.                                                                        the addition of decal textures to the final version of the
                                                                                               mesh. These decals would replace the existing details
Peak memory usage followed the same pattern of be-
                                                                                               in certain locations and display detail that could not be
ing cubic in the resolution of the voxel grid. The min-
                                                                                               created within the detailing rule set framework.
imum and maximum amount of memory used, across
all testing inputs, were approximately 150 and 1400                                            It must be noted though, that neither of these limitations
megabytes, respectively.                                                                       are critical, and none of them should be problematic in
                                                                                               the majority of cases.
These running times are significant for larger resolu-
tion grids. However, in practice, users can prototype
their grammars and rule sets on lower resolution mod-                                          6   CONCLUSION
els and, once satisfied with them, then do off-line gen-                                       We have presented a novel extension to conventional
eration of a high resolution model for actual use. This                                        shape grammars, where the shape output of the gram-
means the long running times for large models will not                                         mar is voxelized, allowing more robust Boolean geom-
significantly disrupt work-flow.                                                               etry operations and a new per-voxel approach to detail-
                                                                                               ing the surface of generated models.
4.2                            Variation and Range Testing                                     These extensions address two shortcomings in current
Variation testing involved randomizing the parameters                                          shape grammar implementations: the support of com-
in several of our shape grammars, and producing mul-                                           plex shapes through CSG, and sub-, or trans-, shape de-
tiple models from them. The objective is to ensure that                                        tailing at per-voxel level, for more elaborate and con-
our algorithm is capable of producing many different                                           trolled texturing.
models that share a similar style, from a single shape                                         Our algorithm is slower and more memory intensive
grammar. A selection of the models produced in this                                            than conventional shape grammar implementations, but
manner are shown in figure 8.                                                                  not outside acceptable limits. Additionally, our exten-
As can be seen from the images, our algorithm is capa-                                         sions allow the generation of a wide range of models,
ble of producing a variety of models, sharing a common                                         including variations from a single shape grammar.



WSCG 2012 Communication Proceedings                                                      120                                         http://www.wscg.eu
Figure 8: A selection of tanks and space stations produced by two of our shape grammars, using randomized
parameters in their rules. This shows how a single grammar can produce multiple models in the same style. These
models were all generated from cubic voxel grids of resolution 256.




Figure 9: A broad selection of the models produced by our algorithm, using a range of detailing rule sets and
shape grammar extensions, including cellular automata patterns, symmetry and multiple-pass textures. All of
these models were generated using a cubic voxel grid of size 256.

Our extensions add new functionality to shape gram-               and tag voxels to create detail corresponding to damage,
mars, without losing existing capabilities, and signifi-          wear and tear over time, growth of mold, and more.
cantly increase the range of achievable content.
                                                                  The voxel detailing rules could be extended to operate
6.1   Future Work                                                 at multiple resolutions of the voxel grid. Octree nodes
The per-voxel detailing stage could be expanded to ad-            could easily be coalesced to form a lower-resolution
dress the interior of generated models. In our work, we           version of the model, to which the rule set could then
have only performed detailing on the surface voxels of            be applied. This would allow the creation of large scale
the model, but the method could be extended to detail             detailing initially, working down to finer details as the
the interior voxels too. This would allow the creation of         rules are run at higher resolutions.
details such as rooms inside generated buildings.                 Finally, a solution to the constraint of texturing being
Post-processing could also be done on the voxel grid              limited to a per-voxel level is the use of decal textures
before it is detailed. This could be used to add, remove,         on the generated mesh. Detailing could be extended to



WSCG 2012 Communication Proceedings                         121                                         http://www.wscg.eu
allow arbitrary textures to be projected onto the gener-                applications, SM ’03, pages 346–351, New York,
ated mesh, complimenting the textures assigned in the                   NY, USA, 2003. ACM.
detailing step. This would allow texturing beyond the            [8]    Luc Leblanc, Jocelyn Houle, and Pierre Poulin.
per-voxel level our system is currently limited to.                     Modeling with blocks. The Visual Computer
                                                                        (Proc. Computer Graphics International 2011),
ACKNOWLEDGEMENTS                                                        27(6-8):555–563, June 2011.
The financial assistance of the National Research Foun-          [9]    Markus Lipp, Peter Wonka, and Michael Wim-
dation (NRF) towards this research is hereby acknowl-                   mer. Interactive visual editing of grammars for
edged. Opinions expressed and conclusions arrived at,                   procedural architecture. In SIGGRAPH ’08: ACM
are those of the authors and are not necessarily to be                  SIGGRAPH 2008 papers, pages 1–10, New York,
attributed to the NRF.                                                  NY, USA, 2008. ACM.
Funding assistance for this research was also provided           [10]   William E. Lorensen and Harvey E. Cline. March-
by the University of Cape Town.                                         ing cubes: A high resolution 3d surface construc-
                                                                        tion algorithm. SIGGRAPH Comput. Graph.,
7     REFERENCES                                                        21:163–169, August 1987.
[1]   Richard Baxter, Zacharia Crumley, Rudolph                  [11]   Paul Merrell. Example-based model synthesis.
      Neeser, and James Gain. Automatic addition                        In Proceedings of the 2007 symposium on Inter-
      of physics components to procedural content. In                   active 3D graphics and games, I3D ’07, pages
      Proceedings of the 7th International Conference                   105–112, New York, NY, USA, 2007. ACM.
      on Computer Graphics, Virtual Reality, Visual-             [12]   Pascal Müller, Peter Wonka, Simon Haegler, An-
      isation and Interaction in Africa, AFRIGRAPH                      dreas Ulmer, and Luc Van Gool. Procedural mod-
      ’10, pages 101–110, New York, NY, USA, 2010.                      eling of buildings. In SIGGRAPH ’06: ACM
      ACM.                                                              SIGGRAPH 2006 Papers, pages 614–623, New
[2]   Martin Bokeloh, Michael Wand, and Hans-Peter                      York, NY, USA, 2006. ACM.
      Seidel. A connection between partial symmetry              [13]   Pascal Müller, Gang Zeng, Peter Wonka, and Luc
      and inverse procedural modeling. In ACM SIG-                      Van Gool. Image-based procedural modeling of
      GRAPH 2010 papers, SIGGRAPH ’10, pages                            facades. In SIGGRAPH ’07: ACM SIGGRAPH
      104:1–104:10, New York, NY, USA, 2010. ACM.                       2007 papers, page 85, New York, NY, USA, 2007.
[3]   Peter Eichhorst and Walter J. Savitch. Growth                     ACM.
      functions of stochastic lindenmayer systems.               [14]   F. Kenton Musgrave, Darwyn Peachey, Ken Per-
      Information and Control, 45(3):217–228, June                      lin, and Steven Worley. Texturing and modeling:
      1980.                                                             a procedural approach. Academic Press Profes-
[4]   Stefan Greuter, Jeremy Parker, Nigel Stewart, and                 sional, Inc., San Diego, CA, USA, 1994.
      Geoff Leach. Real-time procedural generation               [15]   Yoav I. H. Parish and Pascal Müller. Procedu-
      of ‘pseudo infinite’ cities. In Proceedings of the                ral modeling of cities. In Proceedings of the 28th
      1st international conference on Computer graph-                   annual conference on Computer graphics and
      ics and interactive techniques in Australasia and                 interactive techniques, SIGGRAPH ’01, pages
      South East Asia, GRAPHITE ’03, pages 87–ff,                       301–308, New York, NY, USA, 2001. ACM.
      New York, NY, USA, 2003. ACM.                              [16]   P. Prusinkiewicz and Aristid Lindenmayer. The
[5]   Martin Ilčík, Stefan Fiedler, Werner Purgath-                    algorithmic beauty of plants. Springer-Verlag
      ofer, and Michael Wimmer. Procedural skeletons:                   New York, Inc., New York, NY, USA, 1990.
      kinematic extensions to cga-shape grammars. In             [17]   Hanan Samet. The design and analysis of spa-
      Proceedings of the 26th Spring Conference on                      tial data structures. Addison-Wesley Longman
      Computer Graphics, SCCG ’10, pages 157–164,                       Publishing Co., Inc., Boston, MA, USA, 1990.
      New York, NY, USA, 2010. ACM.                              [18]   G. Stiny and J. Gips. Shape grammars and the
[6]   Thouis R. Jones, Frédo Durand, and Mathieu                        generative specification of painting and sculpture.
      Desbrun. Non-iterative, feature-preserving mesh                   Information processing, 71:1460–1465, 1972.
      smoothing. In ACM SIGGRAPH 2003 Papers,                    [19]   Peter Wonka, Michael Wimmer, François Sil-
      SIGGRAPH ’03, pages 943–949, New York, NY,                        lion, and William Ribarsky. Instant architecture.
      USA, 2003. ACM.                                                   In SIGGRAPH ’03: ACM SIGGRAPH 2003 Pa-
[7]   R. G. Laycock and A. M. Day. Automatically                        pers, pages 669–677, New York, NY, USA, 2003.
      generating large urban environments based on the                  ACM.
      footprint data of buildings. In Proceedings of the
      eighth ACM symposium on Solid modeling and



WSCG 2012 Communication Proceedings                        122                                          http://www.wscg.eu
     Interactively Simulating Fluid based on SPH and
                                                  CUDA
              Yige Tang                           Zhongke Wu                           Mingquan Zhou
      Beijing Normal University            Beijing Normal University               Beijing Normal University
       No. 19 XinJieKouWai St              No. 19 XinJieKouWai St                  No. 19 XinJieKouWai St
            Haidian District                     Haidian District                        Haidian District
       100875, Beijing, China               100875, Beijing, China                  100875, Beijing, China
      solidsnake1905@gmail.co                  zwu@bnu.edu.cn                        mqzhou@bnu.edu.cn
                   m


                                                 ABSTRACT
          In this paper, we propose a novel method of interactive fluid simulating based on SPH, and
     implement it on CUDA (Compute Unified Device Architecture). Firstly we use SPH (Smoothed
     Particle Hydrodynamics) theory to simulate the motion of fluids. Secondly we propose an interactive
     method between fluid and rigid objects. We treat the rigid objects as two different types, static one and
     dynamic one. We deal with the two types in separately suitable ways in order to enforce their motion
     similar to the real world. By taking advantages of CUDA which are greatly effective for large scale
     numeric computation in parallel, our simulation achieves real time with low cost.


     Keywords
     Simulation, Fluid, SPH, CUDA, Interactive


                                                              real time and visual effects more than precision in
1.   Introduction
                                                              fluid computation.
     As a common phenomenon in nature, simulation
                                                                  The earliest approach of fluid simulation is
of fluid including water and smoke is an important
                                                              simple particle system, which is proposed by Reeves
part in visual reality. After Reeves’ proposal of
                                                              in simulating fire and flake [Ree83a]. After that,
particle system in 1983, which is used to present
                                                              Shinya   and    Stam    improved    particle   system
non-solid objects such as water, fire and smokes,
                                                              respectively in their work by introducing random
researchers have done a lot of work for simulating
                                                              turbulences [Shi92a] [Sta93a]. Fluid simulation based
fluid vividly and effectively. Fluid simulation is
                                                              on Navier-Stokes Equations is implemented in 2D
usually applied in medical image visual reality and
                                                              space firstly, both Gamito and Yaeger et al. have
video game. Simulating fluid in computer is a tough
                                                              made contributions to it [Gam95a] [Yae86a]. In 1997,
issue. Although the theory of computational fluid
                                                              Stam et al. proposed an approach based on grid to
dynamics has existed for many years, some properties
                                                              simulating smoke [Sta99a], which is the first
of fluid which contains convection, turbulence and
                                                              interactive method on fluid simulation.
surface tension are difficult to be expressed through
                                                                  There are two approaches of simulating fluid
simple modeling. However, due to the fact that the
                                                              based on Navier-Stokes equations, the Eulerian
real time simulation is more important than
                                                              viewpoint and the Lagrangian viewpoint.
computational precision in computer graphics, the
                                                                  In the Eulerian approach, which is based on
mathematical model and implementation focus on

WSCG 2012 Communication Proceedings                     123                                       http://www.wscg.eu
position, the fluid properties on some fixed points are                       2005 [Kol05a]. This approach emerges earlier than
computed. The absolute locations of fixed points                              CUDA.
never change, and the properties need to be computed                               CUDA is a general-purpose GPU programming
are velocity, pressure, density, etc. The Eulerian                            toolkit released by nVIDIA in 2007, which makes it
method is appropriate for simulating gases, while is                          possible to use C language program on GPU. With
not able to present liquid well in wave and foam. The                         the help of CUDA, the processors of GPU can run
Lagrangian approach is different from Eulerian one                            parallelly by independently executing the same
as it is based on particles. Desbrun et al. and                               groups of operations on different sets of data. The
Tonnesen use particles to present soft objects [Ses96a]                       above features are well suited for SPH method,
[Ton98a]. Witkin et al. use particles to control                              because the same groups of operations such as force
implicit surface [Wit94a]. Dan et al. simulate lava by                        computation,        speed       and       position         update     are
using particles [Sto99a]. Comparing with Eulerian                             completely same and have to be executed for each
approach, Lagrangian approach has several following                           particle.
advantages. First, it’s not grid based, so fluid can
                                                                              2. Fundamentals
move in the whole scene and interact with other
                                                                              2.1 Smoothed Particle Hydrodynamics
object. Second, it can present more details of fluid,
                                                                                   Essentially, SPH is a computational model to
such as the mergence and dispersing of water drop.
                                                                              compute the interactive result of each particle in the
      Recently,       the      most     common         Lagrangian
                                                                              fluid system. It defines a way to compute properties
approach        is     based      on        smoothed         particle
                                                                              of a fix point impacted by other particles in the
hydrodynamics (SPH), which is proposed by Lucy in
                                                                              continuous space. A distance related weighted
1977           [Luc77a]              firstly         used          in
                                                                              function             which is called kernel function is the
astronomical phenomenon. Muller et al. introduced
                                                                              key of SPH method. is the distance between some
SPH theory to compute fluid simulation in 2003
                                                                              position        and particle            position      . Another form
[Mul03a]. Their approach simplifies solution method
                                                                              of kernel function is               |         | . Kernel function
of Navier-Stokes equations, while the amount of
                                                                              satisfies the following equation ∫                     |        |
calculation is so great that it’s hard to implementing
                                                                               .We use poly6 kernel as our kernel function, its
animation in real time when the quantity of particles
                                                                              form is
is huge.
      Development of programmable GPU technique
makes large-scale numeric computation be solved                                                       {                                       (1)
effectively under low cost. Due to the feature, SPH
method        can    be     solved     in      parallel.    Through
                                                                              After affirming kernel function, smoothed fields
programming on GPU, real-time simulation for
                                                                                        of arbitrary attributes             of the particle as
large-scale-particles fluid becomes possible. The
GPU based radix sorting approach designed by Satish
et al. combining with spatial uniformed-grid, reduces                                     ∑               |       |                           (2)
the    cost     in    finding        neighbors       of     particles
[Sat08a].Then          the       method           improves        the
                                                                              After replacing the kernel in formula (2) by the
computational        speed.      In 2004 Amada               et   al.
                                                                              gradient of the kernel, we easily get the gradients of
implemented          forces’     computation         of     particles
                                                                              the field as following.
[Ama04a], while the neighbor finding task is still
done by CPU. The method completely implemented
on GPU is firstly proposed by Kolb and Cunts in                                            ∑                  |         |                     (3)



WSCG 2012 Communication Proceedings                                     124                                                      http://www.wscg.eu
       In SPH simulation, we only need consider                          2.1.3       External Forces
pressure, viscous force and external force. Pressure                            External forces include gravity, collision forces
and viscous force can be calculated by above                             and interaction forces with environment. They are
formulations.                                                            applied directly to the particles without the use of
                                                                         SPH.
2.1.1        Pressure
Formula (2) is used on computing pressure yields                         2.1.4       Computing Procedure
                                                                                The SPH model is executed under the following
                         ∑        |              |           (4)         steps:
                                                                                1)   Find neighbors of each particle
If there are only two particles, the pressure force                             2)   Calculate the particle density
calculated by formula (4) will not be symmetric                                 3)   Calculate forces on particles
because the pressures at the locations of the two                               4)   Update position and velocity of particles in
particles are not equal. Following equation is a                                     next time step
simple, stable and fast solution.                                               After finishing these steps, the particles move
                                                                         obeying SPH rules can be rendered shown in Figure
                         ∑                |              |   (5)         1.



Since particles only carry the three quantities mass,
position and velocity, the pressure at particle
locations has to be evaluated firstly.
We compute pressure by using equation (6)


                                                             (6)


Here      is a gas constant and               is the
environmental pressure.
2.1.2        Viscosity
       We use following equation to calculate
viscosity.
                                                                              Figure 1. A group of SPH particles. Their
                                                                              motions are computed by above formulas.
                         ∑            |              |       (7)
                                                                         2.2 Neighbors Search
Since viscosity forces are dependent on velocity                                SPH is computationally heavy. The first step is
differences and not on absolute velocities, there is a                   finding neighbors of each particle. In worst case, each
natural way to symmetrize the viscosity forces by                        particle should be compared with all of others, whose
using velocity differences:                                              complexity is           .To avoid this, using uniform
                                                                         grid reduces most of cost. We use the algorithm
                                                                         presented in [Gre08a], which can be summarized as
                ∑                 |                  |       (8)
                                                                         follows:




WSCG 2012 Communication Proceedings                                125                                         http://www.wscg.eu
     1) Divide the simulation domain into a
                                                                                     1       3         6         8
          uniform grid.
     2) Use the spatial position of each particle to                      CellEnd
          find the cell it belongs to.
                                                                                     2       5         7         8
     3) Use the particle cell position as input to a
          hash function                                                                          (c)
     4) Sort the particle according to their spatial
                                                                      Figure 2. Data structure used by radix sort. (a)
          hash.
                                                                      presents particles’ position in the cells. (b)is
     5) Reorder the particles in a linear buffer
                                                                      status of arrays before sort. (c) shows the
          according to their hash value.
                                                                      arrays needed after sort. The particles in same
                                                                      cell are consecutive in ParticleIndex array. We
     After those steps, it satisfies that particles in the
                                                                      use OldIndex array to get the particle index
same cell will lie consecutively in the linear buffer,
                                                                      before sort in order to access the velocity and
which    makes        finding       neighbors   much    more
                                                                      position of particles.
effectively.
     Radix sort’s implementation on GPU                   is
                                                                     3.     Interaction with Rigid Objects
proposed by Satish et al. [5]
                                                                           We classify rigid objects into two types. One
                                                                     type of them is static object such as glass with water.
                                                                     However the water acts, the glass keeps still. Another
                                                                     type is dynamic object such as a block on the water.
                                                                     We use different methods to simulate them
                                                                     respectively.


                                                                     3.1 Interaction with Static Objects
                                                                            It is easy to compute the interaction between
                                                                     particles and static rigid objects. Since the object
                                                                     can’t move, there is no need to compute a force from
                                                                     the particles on the object. We only compute the
                                (a)
                                                                     penalty force, which forces the particle back into the
Before sort:
                                                                     fluid region in the opposite direction. Deformation
  ParticleIndex
                                                                     will not happen on that static object.
              1   2     3       4     5   6     7   8
                                                                            When a particle collides with a static object, a
  ParticleCell
                                                                     penalty force is applied. Moore and Wilhelms
              0   3     0       1     1   2     1   2
                                                                     provide a comprehensive introduction to the penalty
                                (b)
                                                                     force method [Mat88a]. Here we use the penalty
After sort:
                                                                     force applied to a fluid particle can be calculated by
  ParticleIndex
                                                                     equation (9).
              1   2     3       4     5   6     7   8

  OldIndex
                                                                                                                       (9)
              1   3     4       5     7   6     8   2

  ParticleCell
                                                                     In equation (9),       is the distance by which the
              0   0     1       1     1   2     2   3
                                                                     particle has interpenetrated the static object,         is a
  CellStart



WSCG 2012 Communication Proceedings                            126                                         http://www.wscg.eu
spring constant,        is a damping constant,       is the
normal vector at the collision location, and         is the
                                                                             ∑                                          (13)
relative velocity of the particle to the static object.


3.2 Interaction with Dynamic Rigid                                         N is the amount of particles, and         is velocity
Bodies                                                              of a single particle .The angular velocity of the rigid
    Computing interaction with dynamic objects is                   body         is approximated by equation (14).
more complicated.
    We treat dynamic rigid bodies as a portion of
                                                                            ∑                                           (14)
fluid whose initial density is greater than the initial
density of the fluid. The only difference is in
computing the pressure impaction between fluid and                       Here       is the location of a single particle
particles of rigid bodies. The rigid particles push the             relative to its corresponding rigid body center of
fluid away from the rigid bodies. Likewise, the fluid               mass.
particles apply a pressure that results in a pure                        By using those above equations, we can get a
translation or rotation of the rigid bodies.                        rigid object’s velocity and angular velocity. Then we
    The pressure applied on a rigid body particle is                can simulate the motion of that rigid object.
given by equation (10), and the pressure at a fluid
particle is given by equation (11).
                                                                    4.     CUDA Computation
                                                                         In our implementation, we use three texture
            (             )
     {                                             (10)             arrays to store positions, velocities and densities of
                                                                    particles in last computing procedure, and we write
                                                                    new values to global memory of GPU respectively.
            (            )
     {                                             (11)             The following Table 1 outlines the steps of our SPH
                                                                    algorithm with CUDA.

    Here,           is the rigid body’s initial density,             SPHCompute() //for each frame
while           is the fluid’s.                                      {
    The rigid objects construct of rigid particles,                         Copy particles’ properties from CPU to GPU
whose motion is restricted to translation and rotation                      Set physical parameters of environment
without deformation. Pressure is applied individually                       Hash particles by their spatial position
to each rigid particle, causing them to move                                Sort particles using radix-sort
independently. After that, we need modify the                               /*--ComputeDensity--*/
position of those particles to keep the rigid body’s                        Launch CUDA kernel function for each
shape.                                                               thread
     The mass         of whole object formed by rigid                       Each thread calculate one particle
particles is calculated by formula (12).                                    Compute new densities using other particles
                                                                     in 27 neighbor grids by using SPH kernel function
     ∑                                             (12)                     __syncthreads()

Here,       is the single particle’s mass. Assuming all
of the particles are the same, then the velocity of the
center of mass can be computed by formula (13).



WSCG 2012 Communication Proceedings                           127                                         http://www.wscg.eu
                                                                 world with wave and foam. Additionally, the method
    /*--Compute Force--*/
                                                                 which we propose in this paper also makes the fluid
    Launch CUDA kernel function for each thread
                                                                 interacting with rigid objects well. The fluid in the
    Each thread calculate one particle
                                                                 box can move with the rotation of the box. The wood
    Calculate forces using densities computed above,
                                                                 block in the box is floated under the force from the
    including pressure and viscosity
                                                                 water. Figure 4 shows the interactive results of our
    Handle external forces such as gravity
                                                                 implementation.
    }


    Table 1.The procedure of SPH computing in our
                     implementation



5.      Rendering
        By using density we have computed in above
work, the Marching Cubes method is applied in our
work. An image spaced method is applied to simulate
the refractive effect. The rendering method not only
obtains a good visualization effect, but also bring
little calculation burden. The rendering results are
                                                                   (a) Result of interacting with static rigid objects.
shown in Figure 3.




                                                                 (b) Result of interacting with dynamic rigid objects.

                                                                              Figure 4. Results of interacting with
        Figure 3. Water rendered by using marching                                       rigid objects
        cube. The refraction effects are generated by
                            CGSL                                     Through using CUDA, the method achieves real
                                                                 time simulation, since we take the advantage of the
.
                                                                 capacity of parallel computation afforded by GPU.
6.      Conclusions                                              Our implementation runs on below platform:
        In this paper, we presented an interactive fluid             Windows7 OS 64-Bit， Intel(R) Core(TM) i7
simulating method based on Smoothed Particle                     CPU @3.07GHZ, 6GB RAM and GeForce GTX 570
Hydrodynamics. Our implementation can simulate                   with 1280MB video memory.
fluid in real time and vividly with the help of CUDA                 The results of frame rates are shown in table 2.
and GLSL. The fluid we simulate looks like in real



WSCG 2012 Communication Proceedings                        128                                        http://www.wscg.eu
     number of Particles          frame rate (FPS)             Proceedings of the 15th Annual Conference on
           65,536                       98.1                   Computer Graphics and Interactive Techniques,
          131,072                       42.2                   ACM Press: p. 289-298, 1988.
          262,144                       18.8                   [Mul03a] M. Muller, D. Charypar, M. Gross.
        (a) results without free surface rendering             Particle-Based Fluid Simulation for Interactive
                                                               Applications.Eurographics/SIGGRAPH Symposium
     number of Particles          frame rate (FPS)             on Computer Animation, 2003
           65,536                       50.9                   [Ree83a] W. T. Reeves. Particle systems: a technique
          131,072                       22.8                   for modeling a class of fuzzy objects. ACM

          262,144                       10.4                   Transactions on Graphics 2(2), pages 91-108, 1983.

         (b) results with free surface rendering               [Sat08a] N. Satish, M. Harris, M. Garland. Designing
                                                               efficient sorting algorithms for many core gpus.
Table 2. Runtime result of our implementation on
                                                               NVIDIA Technical Report NVR-2008-001, NVIDIA
                    above platform
                                                               Corporation, Sept, 2008.

      The large performance gap between the results in         [Ses96a] M. Desbrun and M. P. Cani. Smoothed

Table 2(a) and the ones in Table 2(b) is due to the            particles: A new paradigm for animating highly

surface rendering. The surface rendering takes extra           deformable bodies. In Computer Animation and

cost on memory and time. The above table presents              Simulation ’96 (Proceedings of EG Workshop on

that our implementation performs very well.                    Animation and Simulation), pages 61-76. Springer -
                                                               Verlag, Aug 1996.
                                                               [Shi92a] M. Shinya and A. Fourier Stochastic Motion:
7.     References                                              Motion Under the Influence of Wind. In Proceedings
 [Ama04a] T. Amada, M. Imura, Y. Yasumuro, Y.                  of Eurographics’92, pages 119-128, September 1992.
Manabe, K. Chihara. Particle - based fluid simulation          [Sta93a] J. Stam and E. Fiume. Turbulent Wind
on the GPU. Proc. ACM Workshop on General -                    Fields for Gaseous Phenomena. In Proceedings of
purpose Computing on Graphics Processors, 2004.                SIGGRAPH ’93, pages 369–376. Addison-Wesley
[Gam95a] M. N. Gamito, P. F. Lopes, and M. R.                  Publishing Company, August 1993.
Gomes. Two dimensional Simulation of Gaseous                   [Sta99a] J. Stam. Stable fluids. In Proceedings of the
Phenomena Using Vortex Particles. In Proceedings of            26th annual conference on Computer graphics and
the 6th Eurographics Workshop on Computer                      interactive techniques, pages 121–128. ACM
Animation and Simulation, pages 3–15. Springer -               Press/Addison-Wesley Publishing Co., 1999.
Verlag, 1995.                                                  [Sto99a] D. Stora, P. Agliati, M. Cani, F. Neyret, J.
[Gre08a] S. Green. Cuda Particles. Technicle report,           Gascuel. Animating lava flows. In Graphics Interface,
NVIDIA.                                                        pages 203-210, 1999.
[Kol05a] A. Kolb, N. Cuntz. Dynamic particle                   [Ton98a] D. Tonnesen. Dynamically Coupled
coupling for GPU-based fluid simulation. Proc. 18th            Particle Systems for Geometric Modeling,
Symposium on Simulation Technique, 722-727,                    Reconstruction, and Animation. PhD thesis,
2005.                                                          University of Toronto, November 1998.
[Luc77a] L. B. Lucy. A numerical approach to the               [Wit94a] A. Witkin and P. Heckbert. Using particles
testing of the fission hypothesis. The Astronomical            to sample and control implicit surfaces. In Computer
Journal, 82: 1013-1024, 1977.                                  Graphics (Proc. SIGGRAPH ’94), volume 28, 1994.
[Mat88a] M. Matthew, J. Wilhelms, Collision                    [Yae86a] L. Yaeger and C. Upson. Combining
Detection and Response for Computer Animation.                 Physical and Visual Simulation. Creation of the



WSCG 2012 Communication Proceedings                      129                                        http://www.wscg.eu
Planet Jupiter for the Film 2010. ACM Computer
Graphics (SIGGRAPH ’86), 20(4):85–93, August
1986.


Acknowledgements
The work is partially supported by National Natural
Science Foundation of China (No: 61170170) and the
Fundamental Research Funds for the Central
Universities (No: 2009SD-11)
Corresponding author, Email: zwu@bnu.edu.cn




WSCG 2012 Communication Proceedings                   130   http://www.wscg.eu
    Artificial jellyfish: evolutionary optimization of swimming
                                     V. Lazunin                                      V. Savchenko
                                    Hosei University                                  Hosei University
                                  lazunin@gmail.com                                vsavchen@hosei.ac.jp


                                                              ABSTRACT
Jellyfish, also known as "medusae", move by rhythmically contracting and expanding their bell-shaped bodies and are the
earliest known animals to achieve locomotion through the muscle power. Development of a generalized dynamical model of
medusan swimming is of interest to biologists as well as engineers. In this paper we present a new approach to modeling the
swimming behavior of a jellyfish. Due to the axial symmetry of the creature we used a 2D cross-section for the calculation with
the surface of the bell represented by two hemi-ellipsoidal curves. A simplified approach based on non-linear deformations of
a geometric object is used to model the bell contraction-expansion cycle. We used a particle-gridless hybrid method for the
analysis of incompressible flows, with averaging velocities field by the Shepard’s method (partition of unity). To the best of
our knowledge this is the first work where the optimal contraction and expansion parameters for the jellyfish movement were
found by solving the optimization problem of maximizing the speed while minimizing the energy loss.


Keywords
Fluid dynamics, jellyfish, vortex, elasticity, optimization

1    INTRODUCTION                                                            The physics of jellyfish swimming is not well under-
                                                                          stood. Existing animation techniques use combinations
Jellyfish are the earliest known animals to use muscle                    of sinusoidal curves to specify the deformations. How-
power for swimming [DCC07]. They swim by contract-                        ever, it is important for animation to achieve a real-
ing and expanding their mesogleal bells. The swim-                        istic movement depending on the size and shape of a
ming muscles contract to expel a portion of water rear-                   bell. We assume that "realistic" also means "optimal",
ward out of the subumbrellar cavity, thus generating a                    as the movements of the real jellyfish were "optimized"
thrust force to move the animal forward. The bell is                      by the process of natural evolution, and we, therefore,
refilled when it restores its shape after deformation it                  would be able to find realistic movements for an artifi-
received during the thrust phase. The bell consists of a                  cial 3D model of jellyfish by means of artificial evolu-
fiber-reinforced composite material called "mesoglea".                    tion. Other applications, such as computational biology,
The elastic characteristics of the mesogleal tissue were                  soft robotics and development of new propulsion tech-
studied, for example, by Megill et al. [MGB05]                            niques can benefit from development of a generalized
   The contractile muscle fibers of the medusae are only                  model of jellyfish swimming.
one cell layer thick, so the forces that they can pro-
duce do not scale favorably with the increasing medusa                       In this paper we present a system for finding opti-
size. For a medusa with the bell of diameter D, the                       mal swimming parameters for jellyfish models, based
mass of water that needs to be expelled from within the                   on our previous work where we studied vortex simula-
bell scales as D3 , while the muscle force only scales as                 tion for jellyfish [LS10]. The system consists of two
D1 . Therefore the force required for jet propulsion in-                  main parts: simulated swimming and motion optimiza-
creases with the animal size more rapidly than the avail-                 tion. We introduce a simple technique based on radial
able physiological force [DCC07]. Thus, the swimming                      basis functions (RBF) to model deformations of the jel-
performance may change dramatically with the increase                     lyfish bell and a particle-gridless hybrid method for the
of the medusan body size, and it is impossible to predict                 analysis of incompressible flows. We modeled the in-
the optimal swimming parameters based on the geomet-                      teraction between the fluid particles and the surface of
ric and kinematic similarity.                                             the bell in a form of elastic collision and reflection of
                                                                          the fluid particles off the boundary surface. The swim-
                                                                          ming efficiency was estimated for the bell and its par-
                                                                          ticular movement specified by a set of control points.
Permission to make digital or hard copies of all or part of               Genetic algorithms were used to find the optimal swim-
this work for personal or classroom use is granted without                ming pattern. To the best of our knowledge, this is the
fee provided that copies are not made or distributed for profit           first work where the optimal swimming parameters for
or commercial advantage and that copies bear this notice and
the full citation on the first page. To copy otherwise, or re-            the jellyfish movement were found by solving the op-
publish, to post on servers or to redistribute to lists, requires         timization problem. Throughout the paper we refer to
prior specific permission and/or a fee.                                   two other paper concerning computational simulation



WSCG 2012 Communication Proceedings                                 131                                         http://www.wscg.eu
of jellyfish ([LM09] and [RM09]), but neither of those              In their method they model the exchange of momen-
employs any numerical optimization.                                 tum between Lagrangian particle-based fluid model and
   The remainder of the paper is divided into 7 sections.           solids represented by polygonal meshes with virtual
In section 2 we discuss related work. We describe our               boundary particles to model the solid-fluid interaction.
approach in sections 3 to 5 and outline the algorithm in               Lipinski and Mohseni [LM09] used digitized mo-
section 6. In section 7 we outline the specifics of our             tions of two real hydromedusae to computationally sim-
prototype implementation and report of the experimen-               ulate the flows. They used a new arbitrary Lagrangian-
tal results, and in section 8 we conclude and describe              Eulerian method with mesh following the boundary be-
directions of future work.                                          tween the fluid and the jellyfish body.
                                                                       Yoon et al. presented a particle-gridless hybrid
2     RELATED WORK                                                  method for the analysis of incompressible flows
                                                                    [YKO99]. Their numerical scheme included La-
2.1    Studies of real life jellyfish                               grangian and Eulerian phases. The moving-particle
                                                                    semi-implicit method (MPS) was used for the La-
Experimental studies, including dye injection, filming
                                                                    grangian phase, and a convection scheme based on
and analyzing the resulting flow, indicate that smaller
                                                                    a flow directional local grid was developed for the
prolate medusae create strong jets during their bell con-
                                                                    Eulerian phase.
traction stage. Bigger oblate medusae, however, pro-
                                                                       Chentanez et al. presented a method for simulating
duce substantially less distinct jets and broad vortices at
                                                                    the two-way interaction between fluids and deformable
the bell margins. A hypothesis proposed by Colin and
                                                                    solids [CGFO06]. The fluids were simulated using
Costello [CC02] [DCC07] [DCC03] [DCCG05] is that
                                                                    an incompressible Eulerian formulation where a linear
oblate species are using their bell’s margins as "pad-
                                                                    pressure projection on the fluid velocities enforces mass
dles", thus utilizing a paddling, or rowing, mode of
                                                                    conservation, whereas elastic solids were simulated us-
swimming. According to the model presented by Dabiri
                                                                    ing a semi-implicit integrator implemented as a linear
et al. [DCC07], big oblate medusae are not capable
                                                                    operator applied to the forces acting on the nodes in
of swimming via jet propulsion. There is, however, a
                                                                    Lagrangian formulation.
study of McHenry and Jed [MJ03] which suggests that
the jetting model still provides more accurate approxi-                Hirato et al. proposed a method for generating ani-
mation of swimming in oblate jellyfish.                             mations of jellyfish with tentacles [HK03]. They used
                                                                    a simplified computational model based on the MPS
   The flow generated by oblate medusa’s pulsatile jets
                                                                    method to simulate the fluid. Their work is mainly fo-
consists mostly of radially symmetric rotating currents
                                                                    cused on visually plausible modeling of tentacles.
called vortex rings. To better understand the vortex for-
mation and their effect on swimming performance, nu-                   Rudolf and Mould created a system for physically-
merous experimental studies of real live jellyfish were             based animation of jellyfish [RM09]. Their approach
performed [CC02] [DCC07] [MJ03] [DG03] [DCC03]                      may look very similar to ours, as they also exploited the
[DCCG05]. Researches using mechanical jet genera-                   radial symmetry, simulating only a 2D cross-section,
tors demonstrate that there is a physical limit – called            and then creating a 3D bell for the visualization. The
the "vortex formation number" – for the maximum size                main difference between the approach proposed in
of the vortex rings. Once this number is reached, no                [RM09] and the one discussed in this paper is that
bigger vortex formation is possible, and the extra wa-              Rudolf and Mould did not employ any optimization,
ter creates a trailing current behind the vortex. The en-           instead assigning a visually plausible set of parameters
ergy cost for generating this current is higher than that           manually, by trial and error. They used a spring-mass
of creating the vortex ring, so it is optimal to generate           system to represent the body of a jellyfish and a
the largest possible vortex without any trailing current            grid-based immersed boundary method for fluid-solid
[DCC03]. Both thrust and efficiency increase in direct              coupling. As they note in their work, there is still
proportion with vortex ring volume [DCCG05]. Lip-                   very little knowledge about physical properties of real
inski and Mohseni [LM09] used digitized motions of                  jellyfish. Thus, we didn’t feel necessary to employ
two real hydromedusae to computationally simulate the               something as complex as a spring-mass system, since
flows. Their results confirm the hypothesis proposed by             the actual physical accuracy of the model would still
Colin and Costello and demonstrate that distinct type               be uncertain. Moreover, modeling a multi-layered
of jellyfish ("jetting" and "paddling") produce substan-            structure of the jellyfish bell with only one layer
tially different kinds of vortices.                                 of springs attached directly to the opposite sides of
                                                                    the bell does not look realistic. Some fugures from
                                                                    [RM09] demonstrate drastic change of both area and
2.2    Fluid-solid interaction
                                                                    linear size of the umbrella cross-section during the
Müller et al. proposed a particle-based method for in-              contraction, something we failed to observe in real
teraction of fluids with deformable solids [MST+ 04].               species, such as presented in experiments of Colin



WSCG 2012 Communication Proceedings                           132                                         http://www.wscg.eu
and Costello [DCCG05]. Instead of a spring-mass                    approximation with the initial number of nodes equal
system, we use a simpler approach, with the umbrella               40. A space mapping technique based on RBFs (see
of the jellyfish represented in 2D as two spline curves,           [SS01], and references therein) was used for non-linear
deformed by RBFs. Instead of a grid-based method,                  approximation of shape deformations in numerous ap-
for fluid simulation we used a particle-based method               plications. Space mapping in Rn defines a relation-
[YKO99] with elastic collision and reflection of the               ship between each pair of points in the original model
fluid particles off the boundary surface to prevent fluid          and the model after geometric modification. Let an
leaking across the boundary. Finally, [RM09] employs               n-dimensional region Ω ⊂ Rn of an arbitrary configu-
a very primitive visualization technique, an issue we              ration be given, and let Ω contain a set of arbitrary
were trying to address with a GPU-based parallel ray               control points {qi = (qi1 , qi2 , ..., qin ) : i = 1, 2, ..., N}for the non-
tracer, capable of representing transparency, reflectivity         deformed object, and {di = (d1i , d2i , ..., dni ) : i = 1, 2, ..., N}for
and venous structure.                                              the deformed object. By assumption, the points qi and
                                                                   di are distinct and given on or near the surface of each
2.3    Optimization                                                of two objects. The goal of the construction of the
The problem was studied by many researchers from the               deformed object is to find a smooth mapping function
computer graphics and animation community, but we                  that approximately describes the spatial transformation.
have no room for the comprehensive referencing, so we              The inverse mapping function can be given in the form
will mention only a few we found most relevant to out
                                                                                          qi = f (di ) + di ,                             (1)
work.
   Sims was one of the pioneers of artificial evolution.           where the components of the vector f (di ) are volume
In his work [Sim91] he used genetic algorithms to                  splines interpolating displacements of initial points qi
create evolving images, textures, animations and                   (see Appendix for the details).
plants, represented by procedural geometry, with hu-
man aesthetical selection instead of a fitness function.
                                                                   4     FLUID-SOLID COUPLING
In [Sim94] he used similar approach to artificially
evolve both morphology and behavior of articulated                 Using a grid-based approach for jellyfish is possible,
(e. g. composed of rigid parts and connecting joints)              but poses a number of problems. Using a regular grid,
creatures, which were evolved and trained to perform               as in [TGTL11] for an elastic body with varying thick-
specific tasks, like walking, jumping, following a light           ness will result either in a huge computational overkill
source, competing for a ball with other creatures etc.             (if the grid is dense enough to accomodate the thin
   Terzopoulos et al. [TTG94] modeled artificial fish              edges), or in a poor accuracy of the computation (if
as NURBS and spring-mass systems, using simulated                  the grid is more sparse). Using an irregular grid, as
annealing to find efficient moving patterns. Based on              in [LM09] requires solving a mesh warping/re-meshing
simulated sensory input, their fish could learn complex            problem. Solving Navier-Stokes equations with mov-
group behaviour, such as schooling, mating etc.                    ing boundary is a hard problem. For simplicity, we
   Tan et al. [TGTL11] used covariance matrix adap-                chose a particle-based method. Particle-based meth-
tation to find optimal swimming motion for fish, frog,             ods became a de-facto standard for a class of prob-
turtle and even some fictional creatures, represented as           lems where high precision is not required. For mod-
articulated bodies; however, they stated that their simu-          eling we used almost the same scheme as proposed by
lation method is unsuitable for soft body creatures, such          Yoon, Koshizuka and Oka [YKO99]. They proposed a
as jellyfish.                                                      particle-gridless hybrid method for the analysis of in-
   The works, discussed in this section, inspired our at-          compressible flows, where tracing of virtual moving
tempt to create a combined approach suitable for mod-              particles is used instead of solving nonlinear equations
eling and optimizing jellyfish. We emphasize that our              of velocity field. A particle interacts with other particles
work unites two themes of different research history:              according to a weight function w(r), where r is the dis-
generation of time-dependent shapes and estimation of              tance between two particles. The weight function used
dynamical characteristics of the generated models.                 by Koshizuka et al. is
                                                                            
                                                                             −(2r/re )2 + 2          (0 ≤ r < 0.5re )
3     BELL SIMULATION                                               w(r) =       (2r/re − 2)2         (0.5re ≤ r < re )     (2)
To simulate the bell contraction-expansion cycle we                         
                                                                                 0                    (re ≤ r)
used a simplified approach based on non-linear defor-
mations of a geometric object. Because the model                     Density for a particle is calculated as the sum of
of jellyfish has radial symmetry, we used a 2D model               weights of its interactions with the other particles (all
(cross-section) with the surface of the bell represented           interaction happens only within the radius re ):
by two hemi-ellipsoidal curves – the upper and the
                                                                                      hnii = ∑ w(|r j − ri |).                            (3)
lower. For our model we used a piece-wise linear                                                 i



WSCG 2012 Communication Proceedings                          133                                                     http://www.wscg.eu
Note, that, unlike in the MPS method, the particle num-                     Boundary conditions are perhaps the most important
ber density here is not required to be constant. A gra-                  factor influencing the accuracy of the flow computation.
dient vector between two particles i and j possessing                    The manner in which the boundary conditions are im-
scalar quantities φi and φ j at coordinates ri and r j is                posed influences the convergent properties of the solu-
equal to (φ j − φi )(r j − ri )/|r j − ri |2 . The gradient vec-         tion. Usually in particle-based methods boundary par-
tor at the particle i is given as the weighted average of                ticles are used to approximate the no-penetration con-
these gradient vectors:                                                  dition [MST+ 04] [PTB+ 03]. Repulsion and adhesion
                                                                         forces between the particles are used to simulate the
                                                        
            d         φ j − φi
  h∇φ ii = 0 ∑                  (r j − ri )w(|r j − ri |) , (4)          no-penetration, no-slip and actio = reactio conditions
            n j6=i |r j − ri |2
                                                                         on the boundary of the solid.
where d is the number of space dimensions and n0 is                         In our work contour points represent the geometry of
the particle number density.                                             the model and also define fluid boundaries. That is, the
   Diffusion is modeled by distribution of a quantity                    solution points are defined by the fluid particles and the
from a particle to its neighbors using the weight func-                  particles located on the boundary of the bell. For each
tion:                                                                    boundary particle we can calculate the boundary normal
                                                                         vector, pointing outwards, into the flow domain. For
                 2d
      h∇2 φ ii =           [(φ j − φi )w(r j − ri )], (5)                the no-slip condition, only the normal speed compo-
                 λ n0 ∑
                      j6=i                                               nent of any boundary particle is used, while the tangen-
                                                                         tial speed component is discarded. The no-penetration
where λ for a two-dimensional case with Equation (2)                     condition is modeled in a form of elastic collision and
                                    31 2
as the weight function is equal to 140 re . This model is                reflection of the fluid particles off the boundary surface.
conservative, because the quantity lost by the particle i                The motion of the bell was computed using only trans-
is obtained by the particle j.                                           lational parts in y direction. One component of the force
   The continuity equation for incompressible fluid can                  F on a rigid body is a derivative of linear momentum
be written as follows:                                                   mv of the gravitational center. It is assumed that jelly-
               Dρ                                                        fish body density is equal to the density of the water.
                   = −ρ(∇ · u) = 0.                  (6)
               Dt                                                        Thus m is a volume occupied by the jellyfish.
  The velocity divergence at the particle i is given by:                    The force F also invokes fluid and rigid body inter-
            d     (u j − ui ) · (r j − ri )                              action. Points on the curve used to represent the bell
  h∇ · ui = 0 ∑                             w(|r j − ri |). (7)          can be considered as rigid particles. When the bell is
           n j6=i       |r j − ri |2
                                                                         deformed, distances between boundary particles may
                                                                         change, so we put a new set of boundary particles af-
  Then the pressure is calculated as:
                                                                         ter each deformation, by evenly subdividing the curves.
            u∗∗
             i − ui
                   ∗      1
                     = − h∇Pn+1 ii ,                        (8)          The strategy of using rigid particles we followed was
               ∆t         ρ                                              first proposed in [CMT04]. The forces on rigid parti-
                           ρ                                             cles are computed by assuming the rigid body as a fluid.
            h∇2 Pn+1 ii = h∇ · u∗ ii ,                      (9)
                          ∆t                                             Therefore, for a particle i with the pressure pi , mass
where u∗ is the temporal velocity obtained from the ex-                  density ρi and speed vi , the force from the fluid acting
plicit calculation and u∗∗                                               on the node particle fif luid = fipress + fivis is calculated
                         i is the new-time velocity. The
left side of (9) is calculated using the Laplacian model                 by using the physical values of the neighbor particles as
(5). The right side is the velocity divergence, calcu-                   follows [DC96]:
lated by (7). We use variable re to avoid cases where                                fipress = − ∑ (pi + p j )/2ρ¯j ∇i wihj             (10)
some particles near the boundary will have very few                                                i6= j
neighbours to interact with. It gives a system of lin-                                       fivis = − ∑ Πi j ∇i wihj                   (11)
ear equations represented by an unsymmetric matrix,                                                        i6= j
which is solved by an unsymmetric-pattern multifrontal
method [Dav04]. Solving (9) may seem computation-                        where
                                                                                           −(cµi j + 2µi2j )/ρ¯j
                                                                                       
ally expensive, but with jellyfish, the most important                                                                  µi j < 0
                                                                              Πi j =                                                    (12)
fluid-solid interaction often happens near the very thin                                   0                            µi j ≥ 0
edges of the bell, so calculating accurate pressure field
is necessary.                                                            µi j = er vi j ri j /(ri2j + 0.01e2r ),        ρ¯j = 0.5(ρi + ρ j ),
   Instead of using a higher-order gridless convection                   ri j = ri − r j , vi j = vi − v j
scheme as it was proposed by Yoon et al. [YKO99] to
approximate flow directions, we applied averaging of                     5     OPTIMIZATION
the velocities field by a simple scheme, based on Shep-                  In techniques based on the error functional minimiza-
ard’s method (partition of unity) [She68].                               tion it may become necessary to solve highly non-linear



WSCG 2012 Communication Proceedings                                134                                                  http://www.wscg.eu
problems. Minimization by standard techniques re-                            the bending energy of its vertical y-component, mod-
quires high computational effort. Minimization of a                          eled similarly as a "vertical" plate.
simplified functional, for example a quadratic one, is                          In our simulation we used the "economy" principle:
reduced to solving a simple system of linear equations.                      the jellyfish is striving to reach maximum speed with
However, it leads to iterative minimization that depends                     minimum deformation of the bell. Thus, one of the fit-
on a sufficiently good initial guess. It seems to us that                    ness function component is the bending energy E b . In
an attractive way of attacking this problem is to use op-                    the numerical analysis we also measured two quanti-
timization techniques based on genetic algorithms, pro-                      ties characterizing jellyfish locomotion, i. e. distance
posed by Mahfoud and Goldberg in [MG92]. In this                             D passed by a body which is defined by swimming
work we used an algorithm with simulated annealing                           speed ν and energy loss E l . Energy loss is assumed
type selection.                                                              to be equal to surrounding water energy. Following the
   The application of the genetic algorithm starts with                      "economy" principle, we define the fitness function as
initially selecting a set of M variable control points                       follows:
{di = (d1i , d2i , d3i , ...) : i = 1, 2, ..., M} for the definition                 Fitness = wv · D − wb · E b − we · E l        (13)
of the space transformation generating the deformed
object. Actually, the user defines points qi on the ini-                     where wb is the weight for RBF energy, we is the weight
tial image of the bell in its rest state with correspond-                    for kinetic energy of a particle, wd is the weight for the
ing points di on the model of fully contracted bell (Fig.                    object velocity. These parameters are set by the user to
1). The collection of coordinates di and the contraction                     choose a mode of movement.
time tcont define a creature. The algorithm begins by
randomly distorting the initial creature and generates s                     6   ALGORITHM
creatures, which form the initial population. Now, the                       Initially, the 2D contour of the bell cross-section is
genetic algorithm with sequential simulated annealing                        specified as an array of points. Deformations are as-
is applied to this initial population to minimize the fit-                   signed to the bell margin points. Particles are placed at
ness function.                                                               a regular interval (on a regular grid) inside the bounding
                                                                             box, except the inner area of the bell. Then, the follow-
                                                                             ing steps are performed iteratively for each step of the
                                                                             contraction/expansion cycle:
                                                                             1. The averaged density is calculated for every parti-
                                                                                cle. A ball is generated for every particle, and the
                                                                                density is defined as the volume of the ball divided
                                                                                by the number of particles inside the ball. The ball
                                                                                diameter is not constant, and is adjusted so that all
                                                                                balls contain roughly similar number of particles.
                                                                             2. The bell margin points are moved by a step along the
Figure 1: An example of a creature: the right half of the                       deformation vectors. For the rest of the points their
bell cross-section in two states (initial and deformed)                         displacement vectors are calculated using RBFs.
and the deformation vector
                                                                             3. A cardinal spline is fit through the displaced bound-
   The spline f (P), determined by the set of N vari-                           ary points. Because some segments may become
able control points di which constitute a creature, used                        too long, we discard the boundary points and insert
for global space mapping, provides a minimization of                            them again by subdividing the spline curve evenly,
quantity ht A−1 h, that is called "bending energy". 8                           so that all the distances between neighboring points
points belonging to the border of the bounding box and                          are mostly equal.
two additional points in the center of the bell (x = 0) on
the upper and lower curve are used as anchor points. k                       4. A Poisson equation in a matrix form (unsymmetric,
destination points define general deformation of the jel-                       about 10000 linear equations) is solved, giving new
lyfish bell. A−1 is the bending energy matrix, which is                         values of pressure for each particle.
the inverse N × N upper left submatrix of T , and hi are                     5. Gradient vectors are calculated applying equation
so-called heights and N = 10 + k. Space transformation                          (4). For every particle, the speed vector is calcu-
hi is the difference between the coordinates of the ini-                        lated, and the particle is then moved along the vector
tial and destination point placements as shown in Fig. 1.                       by the time step ∆t.
The bending energy of a general transformation is the
sum xt A−1 x + yt A−1 y of the bending energy of its hori-                   6. New pressure values for the displaced particles are
zontal x-components, modeled as a "vertical" plate, and                         interpolated back to the nodes of the regular grid.



WSCG 2012 Communication Proceedings                                    135                                           http://www.wscg.eu
                                                                    to those observed for real jellyfish, showing that appli-
    Number of sub-steps Time (sec) Path (m)                         cation of the no-slip condition looks reasonable.
                      6           7      0.02                         A GPU-based parallel ray tracing system was devel-
                     10       13.14     0.032                       oped for the visualization (Fig. 2). We used recursive
                     14          18     0.031                       ray tracing to visualize the bell as a transparent object
                     18       23.99     0.033                       with refraction and reflection. Each primary ray hitting
                                                                    the bell was spawning several secondary rays, so the an-
                     20        26.6     0.038
                                                                    imation performance varied depending on the number
                     40       25.39     0.039
                                                                    of primary rays hitting the bell, and, thus, on the dis-
           Table 1: Performance results
                                                                    tance from the camera to the jellyfish and on the view-
                                                                    ing angle. Our ray tracer produced 20-30 frames per
7. Distance passed and energy lost at this step are cal-            second on a GeForce GT 540M GPU. See the supple-
   culated for the creature.                                        mentary video for an example of animation. We must
                                                                    add, that experience in areas such as 3D art and texture
   At the end of a swimming cycle, we have a fitness for
                                                                    painting would add significantly to the observed real-
the creature according to (13). A population of 10 such
                                                                    ism of the animation, but that is beyond the scope of
creatures is evolved until convergence within 10%. The
                                                                    this work.
best creature is then selected as "optimal".
                                                                      To validate our results, we compared them with ex-
   For animation, we created a 3D model out of the 2D
                                                                    perimental data received by Colin and Costello [CC02]
contour, and then visualized by ray tracing. At first,
                                                                    [DCCG05]. The distance passed by our model during
the mesh was created by rotational extrusion and tes-
                                                                    one full contraction-expansion cycle (which is roughly
sellation of the original 2D contour. Finally, we used
                                                                    equal to the bell radius) and the resulting water flows
Blender 3D modeling suite to create a roughly simi-
                                                                    seem to be in agreement with their data.
lar 3D model with some embellishments, such as inner
"veins", inward-oriented "velum" and several tentacles.             8   LIMITATIONS AND FUTURE WORK
Rotation of the original 2D contour was still used to put
anchor points at some interval around the bell. The an-             We employed our simulation software to find the opti-
chor points were used as a "skeleton" for RBF-based                 mal movement for a very simple 2D model, swimming
deformations of the entire bell model in 3D, including              straight ahead. A more thorough validation of our tech-
the veins, at each animation step. Some random per-                 nique, using a variety of sizes and shapes, as well as
turbations were added to the anchor points to make our              robustness and sensitivity studies are required and are
jellyfish look less artificial (Fig. 2). The tentacles were         subjects of future work. A few control points were used
modeled as soft cloth with one side attached to the bell            to specify bell contraction, and the movements of the
and deformed separately. The cloth structure was rep-               bell margins were set by only two axially-symmetric
resented by a triangular mesh, with nodes affected by               vectors. Real jellyfish have no brain or eyes (although
the water flow.                                                     some of them have photosensitive spots on their bells)
                                                                    and do not deliberately choose any complex swimming
                                                                    trajectory, so we think our simplification has no big im-
7   RESULTS AND DISCUSSION
                                                                    pact on the results veracity for jellyfish. However, fol-
We implemented this method and used it to find the op-              lowing complex paths would be crucial for jellyfish-like
timal swimming parameters for a simple oblate jelly-                robots. To simulate such behavior, it may be necessary
fish similar to Aurelia aurita [DCCG05]. The program                to perform the simulation in 3D and with larger number
was written in C++ and CUDA C. UMFPACK library                      of (possibly asymmetric) control points.
[Dav04] was used for solving large sparse systems of                   We did not, either, take into account jellyfish feed-
linear equations. The algorithm terminates when a sat-              ing behavior and tentacles. Real jellyfish have an oral
isfactory fitness level has been reached for the popula-            opening inside the bell. Some of them also have nu-
tion. In practice it happens when the number of gen-                merous tentacles spread in the water. The tentacles add
erations is approximately 30. As Table 1 shows, the                 drag force and decrease swimming performance, but
path differs at the 3rd decimal digit. In our implementa-           are used to catch prey. Jellyfish create water flows to
tion we typically used 14 substeps with the calculation             carry their prey through the tentacles or into the oral
time of about 10 minutes on a single core of an Intel               cavity itself. Modeling such behavior is important for
Core 2 Quad computer. The size of the simulation area               computational biology. Additional parameters for it can
is 20x20 cm, with the resoultion of 100x100 particles.              be incorporated into the fitness function.
The bell diameter is 10 cm.                                            Another possible future work would be optimization
   In the particular case (see Fig. 2) weights (lazy mode)          of the shape itself, e. g. finding both optimal movement
wb = 0.3, we = 0.3 and wd = 0.4 were used. The vor-                 and optimal shape, satisfying constraints imposed by a
tices produced by the simulation (Fig. 3) were similar              3D designer.



WSCG 2012 Communication Proceedings                           136                                         http://www.wscg.eu
Figure 2: Animation sequence of two contraction steps (left) and two expansion steps (right) for a jellyfish with
floating tentacles and small additional deformations applied to the bell.



                                                                  where m is a parameter of the variational function and
                                                                  α is a multi-index. It is equivalent to using the RBFs
                                                                  φ (r) = rlog(r) or r3 for m = 2 and 3 respectively,
                                                                  where r is the Euclidean distance between two points.
                                                                     The volume spline f (P) having values hi at N points
                                                                  Pi is the function
                                                                                       N
                                                                           f (P) =   ∑ λ j φ (|P − Pj |) + p(P),          (15)
                                                                                     j=1


              Figure 3: Vortex formation                          where p = ν0 + ν1 x + ν2 y + ν3 z is a degree-one polyno-
                                                                  mial. To solve for the weights λ j we have to satisfy the
                                                                  constraints hi by substituting the right part of Equation
                                                                  (15), which gives
                                                                                   N
                                                                            hi =   ∑ λ j φ (|Pi − Pj |) + p(Pi ).         (16)
                                                                                   j=1

                                                                  λ and ν are the coefficients that satisfy a linear system
                                                                  T x = b, where 
                                                                                  A BT
                                                                                             
                                                                            T=                  ,
                                                                                  B D
                                                                                                                        (17)
                                                                            x = [λ1 , λ2 , ..., λN , ν0 , ..., ν3]T ,
                                                                            b = [h1 , h2 , ..., hN , 0, 0, ..., 0]T

                                                                    For 2D and 3D cases we call f (P) a volume spline.

Figure 4: Velocity change for a jellyfish during the ini-         REFERENCES
tial swimming cycle (10.00 cm diameter, 0.39 sec con-             [CC02]   S. P. Colin and J. H. Costello. Morphol-
traction time, 0.61 sec expansion time)                                    ogy, swimming performance and propul-
                                                                           sive mode of six co-occuring hydrome-
9    ACKNOWLEDGEMENTS                                                      dusae. The Journal of Experimental Biol-
                                                                           ogy, 206:427–437, 2002.
We thank the anonymous reviewers for their helpful
                                                                  [CGFO06] N. Chentanez, T. G. Goktekin, B. E. Feld-
comments.
                                                                           man, and J. F. O’Brien. Simultaneous
                                                                           coupling of fluids and deformable bodies.
10    APPENDIX
                                                                           Eurographics/ ACM SIGGRAPH Sympo-
We consider a mapping function as a thin-plate inter-                      sium on Computer Animation, pages 83–
polation. For an arbitrary area Ω, the thin-plate inter-                   89, 2006.
polation is a variational solution that defines a linear          [CMT04] M. Carlson, P. J. Mucha, and G. Turk.
operator T when the following minimum condition is                         Rigid fluid: Animating the interplay be-
used:                                                                      tween rigid bodies and fluid. ACM Trans-
        Z
                   m!/α!(Dα f )2 dΩ → min,          (14)                   actions on Graphics, volume 23, pages
            ∑
         Ω |α|=m                                                           377–384, 2004.



WSCG 2012 Communication Proceedings                         137                                             http://www.wscg.eu
[Dav04]  T. A. Davis. Umfpack, an unsymmetric-                           Biology, 208:3819–3834, 2005.
         pattern multifrontal method.           ACM            [MJ03]    M. J. McHenry and J. Jed. The ontoge-
         Transactions on Mathematical Software,                          netic scaling of hydrodynamics and swim-
         30(2):196–199, June 2004.                                       ming performance in jellyfish (aurelia au-
[DC96]   M. Desburn and M. P. Cani. Smoothed                             rita). The Journal of Experimental Biology,
         particles: A new paradigm for animating                         206:4125–4137, 2003.
         highly deformable bodies. Computer ani-               [MST+ 04] M. Müller, S. Schirm, M. Teschner, B. Hei-
         mation and simulation, pages 61–67, 1996.                       delberger, and M. Gross. Interaction of flu-
[DCC03] J. O. Dabiri, S. P. Colin, and J. H. Costello.                   ids with deformable solids. Computer An-
         Fast-swimming hydromedusae exploit ve-                          imation and Virtual Worlds, 15:159–171,
         lar kinematics to form an optimal vortex                        2004.
         wake. The Journal of Experimental Biol-               [PTB+ 03] S. Premože, T. Tasdizen, J. Bigler,
         ogy, 206:3675–3680, 2003.                                       A. Lefohn, and R.T. Whitaker. Particle-
[DCC07] J. O. Dabiri, S. P. Colin, and J. H. Costello.                   based simulation of fluids. Computer
         Morphological diversity of medusan lin-                         Graphics Forum, 22(3):401–410, 2003.
         eages constrained by animal-fluid interac-            [RM09]    D. Rudolf and D. Mould. Interactive jelly-
         tions. The Journal of Experimental Biol-                        fish animation using simulation. Interna-
         ogy, 210:1868–1873, 2007.                                       tional Conference on Computer Graphics
[DCCG05] J. O. Dabiri, S. P. Colin, J. H. Costello,                      Theory and Applications (GRAPP), pages
         and M. Gharib. Flow patterns generated                          241–248, 2009.
         by oblate medusan jellyfish: field measure-           [She68]   D. Shepard. A two-dimensional interpo-
         ments and laboratory analyses. The Jour-                        lation function for irregularly spaced data.
         nal of Experimental Biology, 208:1257–                          Proceedings of the 23th Nat. Conf. of the
         1265, 2005.                                                     ACM, pages 517–523, 1968.
[DG03]   J. O. Dabiri and M. Gharib.           Sensi-          [Sim91]   K. Sims. Artificial evolution for computer
         tivity analysis of kinematic approxima-                         graphics. Computer graphics, pages 319–
         tions in dynamic medusan swimming mod-                          328. ACM SIGGRAPH, July 1991.
         els. The Journal of Experimental Biology,             [Sim94]   K. Sims. Evolving virtual creatures. Com-
         206:3675–3680, 2003.
                                                                         puter graphics, pages 15–22. ACM SIG-
[HK03]   J. Hirato and Y. Kawaguchi. Calcula-                            GRAPH, July 1994.
         tion model of jellyfish for simulating the
                                                               [SS01]    V. Savchenko and L. Schmitt. Reconstruct-
         propulsive motion and the pulsation of
                                                                         ing occlusal surfaces of teeth using genetic
         the tentacles. 18th International Confer-
                                                                         algorithm with simulated annealing type
         ence on Artificial Reality and Telexistence,                    selection. 6th ACM Symposium on Solid
         2003.                                                           Modeling and Applications, pages 39–46,
[LM09]   D. Lipinski and K. Mohseni. Flow struc-                         June 2001.
         tures and fluid transport for the hydrome-            [TGTL11] J. Tan, Y. Gu, G. Turk, and C. K. Liu. Ar-
         dusae Sarsia tubulosa and Aequorea victo-                       ticulated swimming creatures. Computer
         ria. The Journal of Experimental Biology,
                                                                         graphics, volume 30. ACM SIGGRAPH,
         212:2436–2447, 2009.
                                                                         July 2011.
[LS10]   V. Lazunin and V. Savchenko. Vortices
                                                               [TTG94] D. Terzopoulos, X. Tu, and R. Grzeszczuk.
         formation for medusa-like objects. Pro-
                                                                         Artificial fishes: autonomous locomotion,
         ceedings of Fifth European Conference on
                                                                         perception, behavior, and learning in a
         Fluid Dynamics (ECCOMAS CFD 2010),                              simulated physical world. Artificial Life,
         June 2010.                                                      1(4):327–351, 1994.
[MG92]   S. W. Mahfoud and D. E. Goldberg. A ge-               [YKO99] H. Y. Yoon, S. Koshizuka, and Y. Oka. A
         netic algorithm for parallel simulated an-                      particle-gridless hybrid method for incom-
         nealing. Parallel problem solving from na-                      pressible flows. International Journal for
         ture, 2:301–310, 1992.
                                                                         Numerical Methods in Fluids, 30:407–424,
[MGB05] W. M. Megill, J. M. Gosline, and R. W.                           1999.
         Blake.      The modulus of elasticity of
         fibrillin-containing elastic fibres in the
         mesoglea of the hydromedusa polyorchis
         penicillatus. The Journal of Experimental



WSCG 2012 Communication Proceedings                      138                                       http://www.wscg.eu
                                 A Comprehensive Taxonomy
                               for Three-dimensional Displays†
                                 Waldir Pimenta                                        Luı́s Paulo Santos
                           Departamento de Informática                            Departamento de Informática
                             Universidade do Minho                                   Universidade do Minho
                                 Braga, Portugal                                         Braga, Portugal
                             wpimenta@di.uminho.pt                                   psantos@di.uminho.pt


                                                                  ABSTRACT
Even though three-dimensional (3D) displays have been introduced in relatively recent times in the context of dis-
play technology, they have undergone a rapid evolution, to the point that a plethora of equipment able to reproduce
dynamic three-dimensional scenes in real time is now becoming commonplace in the consumer market.
This paper’s main contributions are (1) a clear definition of a 3D display, based on the visual depth cues supported,
and (2) a hierarchical taxonomy of classes and subclasses of 3D displays, based on a set of properties that allows
an unambiguous and systematic classification scheme for three-dimensional displays.
Five main types of 3D displays are thus defined –two of those new–, aiming to provide a taxonomy that is largely
backwards-compatible, but that also clarifies prior inconsistencies in the literature. This well-defined outline should
also enable exploration of the 3D display space and devising of new 3D display systems.

Keywords
three-dimensional displays, depth cues, 3D vision, survey, taxonomy

1. INTRODUCTION                                                                  This profusion of implementations has plagued at-
The human ability for abstraction, and the strong de-                            tempts to define a nomenclature system for 3D dis-
pendence on visual information in the human brain’s                              plays. While a few comprehensive classification
perception of the external world, have led to the emer-                          schemes have been proposed, most based on or com-
gence of visual representations of objects, scenery and                          patible with Okoshi’s seminal work [Oko76], several
concepts, since pre-historical times. Throughout the                             are now obsolete, while recent attempts tend to be
centuries, many techniques have been developed to in-                            specific to a subset of displays [MK94, Hal97, BS00,
crease the realism of these copies.                                              Dod05, GW07, UCES11].

Recent years have revealed a focusing of these efforts                           Mostly what is seen are overview sections in publi-
in devising ways to realistically recreate the sensa-                            cations that on the one hand assume implicit defini-
tion of depth, or three-dimensionality, of the depicted                          tions of 3D perception and 3D displays, and on the
scenes. 3D displays thus emerged as an active area of                            other hand frequently avoid taking a stance (or do so
research and development.                                                        inconsistently) in undecided issues emerging from par-
                                                                                 tially incompatible previous classifications, such as the
Despite this being a relatively recent field, many dif-                          placement of holographic technology [Fav05] or in-
ferent approaches for 3D displays have been already                              tegral imaging [DM03]. A definitive, exhaustive and
proposed and implemented, and new ones surface with                              unambiguous categorization system for 3D displays
some regularity. Moreover, these implementations                                 has thus been lacking in the literature [CNH+ 07, p.1],
provide different sets of approximations for the depth                           which hinders the classification and evaluation of dif-
cues that our visual system uses to perceive the three-                          ferent implementations, especially hybrid ones.
dimensionality of a scene.
                                                                                 The approach presented in this paper focuses in the
Permission to make digital or hard copies of all or part of                      formalization of the properties of each category of 3D
this work for personal or classroom use is granted without
fee provided that copies are not made or distributed for profit                  displays, to provide a stable system for classifying ex-
or commercial advantage and that copies bear this notice and                     isting or new implementations. Specifically, the defini-
the full citation on the first page. To copy otherwise, or re-                   tion and categorization of 3D displays is based in their
publish, to post on servers or to redistribute to lists, requires                fundamental properties, rather than in implementation
prior specific permission and/or a fee.
                                                                                 details, as is the case with most current classifications.
†This work has been funded by the Portuguese agency FCT – Fundação para a Ciência e a Tecnologia, through the grant SFRH/BD/74970/2010.




 WSCG 2012 Communication Proceedings                                       139                                                    http://www.wscg.eu
As a necessary foundation for this taxonomy, Sec-                 Shading and shadow projection. Effects caused by
tion 2 presents a general overview of the depth cues                 the relationship between objects and light sources.
used by the human visual system to perceive three-                   The distribution of brightness and color in an ob-
dimensionality. With this knowledge, we can then, in                 ject’s surface provides information about (among
Section 3, determine the specific subset of these that               other things) its shape and position relative to the
clearly mark the frontier between 2D and 3D displays,                light sources that illuminate it. Also, the location,
and define basic properties of 3D displays. Section 4                format and darkness of shadows projected into the
then delves into the 3D display realm, defining a hi-                object (due to parts of it or other objects obscuring
erarchy of types and subtypes for 3D displays, based                 the light) and into its vicinity allow us to interpret
primarily on the depth cues they implement.                          its 3D form and relative position to other objects
                                                                     and/or the environment.
By employing a systematic approach, we expect the
outcome to be a logical, well-structured and extensi-             The above are all static cues. There are two more psy-
ble taxonomy that will facilitate comparison of dif-              chological cues, which are dynamic; that is, they man-
ferent approaches, and the evaluation of appropriate              ifest when there is movement either of the observer or
techniques for a given application. The Conclusion as-            of the observed object (or both):
sesses the degree to which this objective was fulfilled,
                                                                  Motion parallax. Relative changes in perceived po-
and illuminates what further work is to be performed
                                                                    sition between two objects when we move. For
to complement the proposed taxonomy.                                example, during a car trip a tree seems to be “trav-
                                                                    elling past us” faster than the distant mountains.
2. VISUAL CUES TO THREE-DIMEN-                                    The kinetic depth effect. Changes in the appearance
   SIONALITY                                                         of an object due to its own motion. For example,
The origins of the Human species, as primates living                 when a spherical object –say, a football– is uni-
and moving in trees, and later as hunter-gatherers, con-             formly illuminated so that no shadows give away
tributed significantly to make perception of depth a                 its round shape, a slow rotation around itself is suf-
                                                                     ficient for our visual system to infer that it is a solid
very important feature of our vision. Developments in
art and research in optics and display technology have               body and not a flat disk facing us, due to the rela-
revealed some of the cues that our visual system uses                tive motions of features in its surface.
to interpret the location of objects. These hints, known          The physiological depth cues consist of:
as depth cues, can be divided into two main groups:
                                                                  Binocular disparity (or stereo parallax)1 . Differ-
psychological cues, which depend on acquired knowl-
                                                                     ences in images received by each eye, commonly
edge of the visual aspect of familiar objects, and phys-             called stereoscopy2 . Studies indicate [Oko76] that
iological cues, which manifest through the anatomy of                for a moderate viewing distance, binocular dispar-
our visual system [Oko76].
                                                                     ity is the dominant depth cue to produce depth sen-
The main psychological depth cues are:                               sation, through a process called stereopsis, which
                                                                     is the effort made by the brain to fuse the images
Occlusion. The overlap of some objects by others that                together into a 3D perception of the scene. This
   are closer to us. This is one of the most fundamen-               fusion effort is always necessary because conver-
   tal ways we perceive depth on a scene.                            gence of the eyes can only produce a perfect match
                                                                     for a limited subset of the points from the images,
Linear perspective. Given prior knowledge of com-
                                                                     due to projection geometry constraints.
   mon shapes and/or sizes of objects, we interpret
   perceived distortions in their shape (parts farther            Convergence. When both eyes rotate inwards to aim
   away from us appear smaller), differences in size                 at the object of interest, thus aligning the differ-
   between them, and variation of their angular size                 ent images they receive, so they can be more ef-
   (how much of our visual field they cover) as indi-                fectively combined by the brain. As with ac-
   cators to their location in three-dimensional space.              commodation, this rotation manifests itself with
                                                                     greater amplitude when differences in distance oc-
Atmospheric perspective. Commonly known as “dis-
                                                                     cur closer to the eye, so it is also a cue that is
   tance fog”, it refers to the fading in contrast and               more strongly perceived for nearby objects (less
   detail, and shift to bluish colors, of objects located
   at a great distance. This happens because the light             1 “Binocular” comes from the Latin bini (pair) + oculus (eye).

   we get from them had to travel an increased dis-               “Stereo” comes from the Greek stereós (solid).
                                                                   2 It’s been known since as early as 300 B.C. that depth perception
   tance through air and thus underwent more scat-
                                                                  in human vision is related to the fact that we have two eyes, in sep-
   tering from the atmospheric gases and particles.               arate physical locations, which collect different simultaneous per-
                                                                  spectives of the same object [EucBC].




WSCG 2012 Communication Proceedings                         140                                                http://www.wscg.eu
   than 10m, according to [Wid01]). If they are close             Table 1: Summary of visual depth cues for three-
   enough, one can clearly feel the eyes “crossing” so            dimensional vision
   that they can keep aiming at the same point.                                   static                            dynamic
Accommodation. The effort made by the muscles in                    psycho-       occlusion (overlap); linear       motion
   the eye that control the shape of its lens in order              logical       perspective; atmospheric          parallax;
   to bring the image into focus in the retina. Even                              perspective (distance fog);       kinetic
   though we usually do not consciously control these                             shading and shadows.              depth effect.
   actions, our brain uses this muscular contraction                physio-       accommodation (focus);
   information as an indicator of the distance of ob-               logical       binocular disparity (stere-
   jects we are observing. Since the focusing effort                              oscopy); convergence.
   varies much more for distance changes near the
   eye, the effect is particularly notable for nearby ob-
                                                                  3. DEFINITION OF A 3D DISPLAY
   jects (less than 2m, according to [MZ92]).
                                                                  Before defining what a 3D display is, it is necessary
The depth cues described above are summarized in Ta-              to clarify what is meant by “display”. As a word with
ble 1.                                                            multiple meanings, we will assume the context of vi-
                                                                  sual perception and the word’s usage as a concrete
The Accommodation-Convergence Mis-                                noun (i.e., the name of a thing). As such, the defi-
match                                                             nition adopted will be “a visual output device for the
The fact that most visual representational media are              presentation of images”.
unable to implement all depth cues –especially the
physiological ones–, does not pose a serious problem,             It’s worth pointing out that the word “images” is in
either because the scenes represented are meant to take           plural, because we will consider only display media
place (or be viewed from) a distance where the physi-             that don’t produce permanent records, but instead are
ological cues aren’t relevant [Oko76, p.39], or because           mutable, or rewritable, by comprising reconfigurable
we can cognitively ignore the mismatch in psycholog-              active elements, such as pixels, voxels3 or catoms4
ical vs. physiological depth cues, as our abstraction             – in other words, electronic visual displays. This, as
ability allows us to understand their purported three-            [Oko76] pointed out, effectively excludes static visual
dimensionality regardless.                                        representations such as paintings, photographs, sculp-
                                                                  tures, and even classical (static) holograms, for they
However, a mismatch among the physiological cues                  are not displays in the sense adopted above, but merely
is less tolerable. This mismatch is common in current             the physical embodiment of a specific image. These
3D displays, because every display that provides stere-           will therefore be left out of this taxonomy. Never-
oscopy (one view for each eye) is theoretically able to           theless, all the principles behind them are present in
implement proper convergence cues for each object in              the displays we consider, the only difference being the
the scene depending on their location. But accommo-               adoption of a rewritable medium.5
dation (provided by the ability to make the light rays
diverge not from the screen, but from the virtual posi-           With the clarification of what constitutes a display
tions of the scene objects) is much harder to achieve;            device, we can now approach the question of what
therefore, most of these displays end up forcing the              makes a display three-dimensional. Firstly, we must
eye to always focus at the screen to get a sharp im-              acknowledge that the line separating 3D displays from
age, which conflicts with the cues of convergence and             2D displays is not always clearly defined, despite
stereopsis. The resulting phenomenon is called the                what the dichotomic “2D/3D” nomenclature seems to
accommodation-convergence mismatch.                               suggest. This fuzziness occurs because, on the one
                                                                  hand, the psychological 3D depth cues can, in fact, be
This mismatch is more serious than the aforemen-                  reproduced in media traditionally considered as 2D;
tioned one, because providing the brain with conflict-            and on the other hand, many displays deemed three-
ing physical signals causes discomfort, the same way              dimensional are actually flat screens, which means that
mismatch between visual and vestibular (from the bal-             the images are emitted from a two-dimensional sur-
ance system in the inner ear) perception of movement              face.
causes motion sickness. The consequences may in-
clude headaches, fatigue or disequilibrium, preventing
                                                                  3A  portmanteau of the expression “volumetric pixels”.
continued use of these displays. This, of course, in ad-          4 Inthe (still theoretical) field of claytronics –dynamic sculptures
dition to the reduction it causes in the realism of the           made of microscopic robots–, ”catom” is a combination of the words
3D visualization, which might become uninteresting                “claytronic atoms” [GCM05].
                                                                  5 For instance, when we mention holography, or stereoscopic dis-
or even visually confusing [Hal97].
                                                                  plays, we will be referring to their electronic counterparts.




WSCG 2012 Communication Proceedings                         141                                                http://www.wscg.eu
With these limitations in mind, we define 3D displays                          The other main axis we’ll use to map the 3D displays
as visual output devices that evoke at least one of                            space is the effective shape of the display medium
the physiological depth cues (stereoscopy, accommo-                            itself, which can be “flat” or “deep”. This doesn’t de-
dation and convergence) – besides, naturally, the psy-                         pend strictly on the shape of the display surface, but
chological cues enabled by the specific display tech-                          rather on the effective volume it occupies while dis-
nology used. This definition ensures that the 3D per-                          playing the 3D image. The flat displays can be com-
ception is truly engaged in a natural way, and not by                          pared to a window, a planar surface which provides
ignoring the apparent flatness of the scene, as happens                        different perspectives as one moves around, but limits
with displays based only in psychological depth cues.                          the scene at its boundaries. For the deep displays, there
                                                                               is a volume of space occupied by the display medium
4. PROPOSED TAXONOMY FOR 3D                                                    (either permanently or due to moving elements) and
   IMAGING TECHNIQUES                                                          the virtual object is displayed inside the volume, also
To define a basis for the proposed taxonomy, we will                           not able to exist outside the volume’s boundaries as
apply two general criteria as orthogonal axes of cat-                          they are perceived by the observer. We can say that
egorization. We’ll demonstrate that by intersecting                            one looks through flat displays as if through a window,
these two basic properties, it is possible to estab-                           and looks into deep displays as if they were a crystal
lish a well-grounded, formally-defined taxonomy that                           ball.
largely validates current consensus but also clarifies
conflicting definitions.                                                            Aside: the projection constraint

The first axis is the number of views supported by                                  The boundary limitation of both the flat and the deep
                                                                                    displays are manifestations of the “projection con-
the display. The reasoning behind this is that most of
                                                                                    straint”.8 Countering this effect may be done by increas-
the depth cues for 3D perception (occlusion, motion
                                                                                    ing the absolute size of the display (for example, a cin-
parallax, convergence, stereopsis, etc.) are dependent                              ema screen), shaping it in order to surround the viewer
on the angle from which the observer views the scene.                               (as is done in the CAVE virtual reality environment), or
3D displays will employ different methods to emulate                                increasing its relative size by bringing it closer to the ob-
this viewpoint-dependent variation of the light field.                              server (the technique used by virtual reality glasses).

One such method consists simply in producing two
                                                                               These two criteria allow us to effectively separate the
views and ensuring that each is only seen by the appro-
                                                                               displays into five main categories, most of which are
priate eye of the observer. Another approach employs
                                                                               already well-established in the literature. Table 2 sum-
displays that are able to project multiple views into dif-
                                                                               marizes this division.
ferent directions. This is implemented by segmenting
the image into as many perspectives as desired, mul-                           It might be noticed that two of those terms are not
tiplexing them into the display, and using a filtering                         common in most taxonomies, namely “virtual vol-
mechanism to direct each view to the corresponding                             ume displays” and “multi-directional displays”. They
direction. Finally, a third type comprises displays that                       are, in fact, key components of this taxonomy, in that
can generate or approximate a continuous wavefront of                          they clarify the classification of techniques for which
light that propagates as coming from the actual 3D po-                         past works have not been able to agree on a category.
sition of the virtual object, rather than dispersing from                      Other categories, however, were included with their
its projection in the display surface.6                                        currently de facto standard names, in order to prevent
                                                                               excessive disruption and preserve as much backwards-
Throughout the years, as 3D displays advanced past
                                                                               compatibility as sustainable without breaking the con-
the two-views (binocular) approach, the word “stereo-
                                                                               sistency of the proposed framework.
scopic” has gradually expanded its range to become
largely synonymous with three-dimensional vision
(and rightly so), and is thus routinely applied to dis-
plays of all of these types. Therefore, in the spirit                                          Table 2: Proposed Taxonomy
of unambiguity, the three meta-categories described                                                                 display shape
above will be named “duoscopic”, “multiscopic” and
“omniscopic”, respectively.7                                                                                   flat                 deep
                                                                              # views




                                                                                        duoscopic      stereoscopic
6 Head tracking by itself only implements monocular directional                         multiscopic    autostereoscopic      multi-directional
variation; thus, it doesn’t constitute a 3D display as defined above.                   omniscopic     virtual volume        volumetric
7 Prior attempts to define the difference between these types of dis-

plays have entailed the use of the terms “stereograms” and “panora-            8 [Hal97] describes the projection constraint by stating that “a dis-

magrams” [Oko76, Hal97], but the distinction hasn’t been widely                play medium or element must exist in the line of sight between the
adopted in the literature, and even less in the industry.                      viewer and all parts of the [visible] image.”




WSCG 2012 Communication Proceedings                                     142                                                 http://www.wscg.eu
In the following subsections we will complete the def-            As previously mentioned, HMDs can overcome the
inition of these five groups by specifying their main             projection constraint by displaying the image closer to
properties and, where applicable, defining relevant               the eye, thus increasing its relative size and coverage
subcategories inside them.                                        of the visual field.

Flat 3D displays                                                  There are two key characteristics of stereoscopic dis-
Flat-type, screen-based 3D displays are the most pop-             plays that separate them from other 3D vision tech-
ular kind of 3D displays used currently, with commer-             niques: (1) they require either the whole display sys-
cial use now common in movie theaters and domestic                tem or the filtering mechanism to be fixed regard-
entertainment devices. They work mostly by provid-                ing the eyes, which in most cases implies some sort
ing stereoscopy (different images for each eye), which,           of headgear, thus being potentially invasive to varied
as mentioned in Section 2, is the main depth cue for 3D           degrees (ranging from light and inexpensive filtering
vision at moderate distances.                                     glasses to surgery-requiring neural implants), and (2)
                                                                  because they only present two views, they only sup-
These displays can be further divided in three main               port a single user/perspective.9
groups: stereoscopic devices, which work in conjunc-
tion with glasses to provide two distinct views; au-              Motion parallax is not natively supported by stereo-
tostereoscopic screens, which can generate multiple               scopic displays, but they can be enhanced to support it
views without requiring any headgear; and virtual                 by employing head tracking [Dod05].
volume displays, which recreate the 3D wavefront as
if propagating from the actual location of the 3D im-             4.1.2     Autostereoscopic Displays
age – the most notable example being the hologram.                Autostereoscopic screens are usually implemented us-
                                                                  ing two techniques:
4.1.1   Stereoscopic Displays
Stereoscopic 3D displays can display one image to                 Parallax barriers, which work by sequentially inter-
each eye in two ways: either by combining (i.e, mul-                 lacing the images for each perspective in vertical
tiplexing) two separate streams of images in one de-                 strips, and employing a fence-like barrier that re-
vice, and filtering them with special glasses, or by us-             stricts the light from each strip to propagate only
ing separate display devices for each eye.                           in its corresponding direction.
                                                                  Lenticular displays, which do this filtering by using
Glasses-based stereoscopic displays can be imple-                    an array of lenses that direct each part of the im-
mented through three filtering techniques [Ben00]:                   age to the correct direction. These lenses are usu-
Wavelength multiplexing. Separating the left-eye and                 ally cilindrical, providing only horizontal parallax,
   right-eye images in different colors, the most well-              but spherical lenslets have been proposed to over-
   known example of which is the anaglyph, with its                  come this limitation, resulting in what is called an
   characteristic “red-green” glasses;                               “integral imaging” device.
Temporal multiplexing. Using shutter glasses syn-                 Autostereoscopic screens exploit the fact that the eyes
   chronized with the screen and a doubled frame-rate             occupy different points in space to provide stere-
   that displays the images for the left and right eye            oscopy. In other words, they employ direction-
   alternatively;                                                 multiplex to channel information of the left and right
Polarization multiplexing. Achieved by emitting im-               views into appropriate eyes [DM03].
   ages for each eye with different light polarizations
   (direction of wave oscillation), and filtering them            These direction multiplexing techniques can be gener-
   with polarized-filter glasses.                                 alized to produce more than two views, which enables
                                                                  motion parallax, and consequently the ability to sup-
The stereoscopic displays that use separate screens for           port multiple observers with a single display, without
each eye are usually called head-mounted displays                 any headgear. However, undesired optical distortions
(HMDs). This name is justified because the whole                  caused by too small lenses or barriers limits the num-
display system is head-mounted, rather than only the              ber of possible views. The motion parallax supported
filtering mechanism.                                              is thus markedly non-continuous, which reduces the
                                                                  realism of the 3D effect [Hal97].
HMDs include mostly devices such as virtual reality
(VR) or augmented reality (AR) glasses, but also com-              9 It is possible, using HMDs, to implement multi-user applications
prise techniques still largely embryonic, such as retinal
                                                                  by having each user wear their own device, and keeping all of them
projection, contact lens displays and brain-computer              synchronized, but this is obviously a costly and technically chal-
interfaces.                                                       lenging approach.




WSCG 2012 Communication Proceedings                         143                                               http://www.wscg.eu
Anisotropic diffusers (surfaces that scatter light in              ing the pattern with a copy of the reference coherent
very narrow horizontal directions) have been presented             beam. All optical effects such as shadows, reflections
as a potential solution to such limits [UCES11]. It’s              and occlusions are present in the resulting image.10
been reported[Tak06] that with enough angular resolu-
tion, such displays could even create accomodation re-             Unlike most autostereoscopic screens, virtual volume
sponses in the eye. Therefore, by sufficiently approx-             displays can provide all the physiological depth cues
imating (in the assigned visualization area) the conti-            (particularly accommodation), as well as continuous
nouous wavefront that a real object would create, they             motion parallax. The recent advances in anisotropic
could be considered omniscopic instead.                            screens have shortened this gap, but further properties
                                                                   such as vertical parallax are yet unreported in such dis-
                                                                   plays, which positions virtual volume displays favor-
4.1.3   Virtual Volume Displays
                                                                   ably in the realism of the 3D effect and the compact-
Virtual volume displays, as the name says, are able
                                                                   ness and portability of the display system.
to generate the sensation of depth by placing virtual
images in 3D space, without having to physically span
                                                                   Deep 3D Displays
the imaging volume [Hal97]. Since each point of the
                                                                   Deep displays physically occupy a volume of space
image is optically located at the correct depth, these
                                                                   and display the object inside it. Two methods can be
displays are able to provide proper accommodation.
                                                                   used to implement such a system: volumetric dis-
This can be implemented either by adaptive optics, or
                                                                   plays, which place the virtual points of the object
through the holographic technique.
                                                                   in physical 3D space, and multi-directional screens,
Adaptive optics employ dynamic optical systems that                which, as the name says, have either a single rotating
can change their focusing power. These can be de-                  screen, or multiple static screens facing different di-
formable (varifocal) membrane mirrors, or “liquid”                 rections – in either case, users in a given position will
lenses, usually produced through an effect called                  see only the appropriate perspective.
“electrowetting”. They are similar to the old illu-
                                                                   Volumetric displays are omniscopic, since having the
sion called Pepper’s ghost, which consists in a semi-
                                                                   object displayed in actual 3D space allows virtually
transparent mirror that superimposes a reflection (of
                                                                   any viewpoint to get the correct perspective. Multi-
a real object, or a verisimilar 2D projection) over the
                                                                   directional screens will have to subdivide the perspec-
background scene, producing a ghostly image of the
                                                                   tives into a finite number of views, and are therefore
object, and which still finds modern use in many theme
                                                                   part of the multiscopic meta-category. Both can po-
parks and live shows.
                                                                   tentially implement a 360◦ viewing angle.
In displays based on adaptive optics, the flexible opti-
cal element will reflect or transmit a static screen that          4.2.1     Volumetric Displays
displays a sequence of depth slices, synchronized with             Volumetric displays use several techniques to display
the curvature of the mirror or lens to place the image of          an image in real 3D space. This means that each point
the slice in the appropriate depth location. This kind of          of the image is actually located at the position they
display will prevent occlusion, since the virtual slices           seem to be. This can be achieved by two main meth-
cannot block the light from those behind it. But if a              ods: static volume displays, and swept-volume dis-
single-perspective is acceptable, such as in HMDs or               plays.
single-user desktop displays, occlusion can be simu-
lated by subtracting a depth layer from those behind it            Static volume displays use a substrate (solid, liquid,
(from the perspective of the observer).                            or gas) that is transparent in its resting state, but be-
                                                                   comes luminous, or opaque, when excited with some
While the surface of the lens or mirror is not strictly            form of energy. If specific points can be selectively
planar, slight changes in their focal length lead to large         addressed inside a volume of space filled with such
variations in the virtual image’s location [DM03].                 a material, the activation of these points (called volu-
Coupled with the window-like viewing mode they en-                 metric pixels, or voxels) forms a virtual image within
able, this means that adaptive optics-based displays               the limits of the display.
can be considered flat displays.
                                                                   Naturally, gaseous substrates are preferred, and dis-
Holography, on the other hand, works by storing the                plays have been made using artificial haze to produce
shape of the wavefront of the light emanating from the             unobtrusive, homogeneous clouds suspended in the air
scene, by recording the interference pattern of its in-
teraction with a clean, coherent light source. The orig-           10 Holograms   store the entirety of the information from a scene –
inal wavefront can then be reconstructed by illuminat-             hence their name, which derives from the Greek “holo”, the same
                                                                   root that the word “whole” came from.




WSCG 2012 Communication Proceedings                          144                                               http://www.wscg.eu
that make light beams visible. Purely air-based dis-              position, so light would still pass through that space in
plays have also been proposed, using infrared laser               the fractions of time where the display surface isn’t
light to produce excited plasma from the gases in the             sweeping through that particular location.
air, at the focal points of the laser. Advanced forms
of such displays are common in science fiction, often             Still, swept-volume displays purported as “occlusion-
mistakenly referred to as “holograms” [Hal97]. How-               capable” have been presented in recent research (for
ever, the actual visual quality of such displays is very          instance, [CNH+ 07]). They work by employing
far from their imagined counterparts, and even quite              highly anisotropic diffusers to ensure that light pro-
low compared to other current methods of 3D vision.               duced or projected in the display surface is only emit-
                                                                  ted in roughly the direction the display is facing, thus
Swept-volume displays use a two-dimensional sur-                  ensuring that only the correct view is observed in each
face that cyclically sweeps through a volume (either              direction. By correctly varying the image presented
moving from one extremity to another, or rotating                 in the screen according to the direction it is facing, a
around an axis) and display, at each point of this path,          3D image is produced, which can also appear to float
the corresponding slice of the virtual object. Due to             outside the display volume.
the temporal persistence of vision, this results in what
resembles a 3D object.                                            This kind of display, however, while very similar to
                                                                  swept-volume volumetric displays, is not volumetric
The main problem with volumetric displays is that,                itself, since the image points are not located in the ac-
since most of the substrates used become bright when              tual position they appear to be; in other words, they
excited, rather than opaque, each point of the vir-               manifest the property we earlier associated with mul-
tual object won’t block light from the other points               tiscopic displays, that light from each point disperses
[Fav05], which undermines the very basic depth cue                from the screen itself rather than from the correct loca-
of occlusion; that is, observers would see the back               tion of the virtual point, which disables the provision
side of objects as well as their front side. This is              of the accommodation depth cue.
the same problem that plagues varifocal mirror dis-
plays. Such devices are therefore better-suited to                These rotating screen displays are fundamentally
display hollow or naturally semi-transparent objects,             similar to an earlier technique known as cylindrical
or non-photorealistic scenes – for example, icons, or             hologram [FBS86], in which a series of images taken
wireframe 3D models [Hal97].                                      of a subject, with a camera performing a 360◦ orbit
                                                                  around it, are recorded in thin vertical holographic
This difficulty could be surpassed in static-volume dis-          strips, which are then assembled in a cylindrical shape
plays, if the substrate can be made opaque; however,              to provide full panoramic view of the 3D object.
a solid, static substrate would make direct manipula-
tion and interaction with the object impossible (which            In both cases, the viewer-depended variation is im-
is also true of swept-volume displays). The ideal vol-            plemented explicitly through segmenting the viewing
umetric display would thus be a “dynamic sculpture”               field, rather than producing the appropriate wavefront
that is able to change its shape and appearance accord-           of the 3D scene. Cylindrical holograms, however,
ing to the desired properties of the object being visu-           can potentially implement accommodation if the strips
alized. This has already been proposed, in a concept              aren’t holograms of a flat photograph, but of the actual
called “claytronics” [GCM05], but remains a strictly              3D object itself.
theoretical possibility, with no practical implementa-
tions produced so far.
                                                                  5. CONCLUSIONS AND FUTURE
4.2.2   Multi-Directional Displays                                   WORK
Recently, some claims have been made in the litera-               3D displays are increasingly popular choices to pro-
ture that the lack of occlusion in volumetric displays            vide new, more immersive and intuitive tools for edu-
is not an intrinsic characteristic of the category, but a         cation, entertainment (especially in gaming, television
technical limitation that can be addressed.                       and cinema), telepresence, advertising, among others.

While, as described above, this is true of static-                Moreover, as the technology advances, more demand-
volume volumetric displays, swept-volume displays                 ing uses of such displays have started becoming fea-
are strictly unable to overcome this property because             sible or expectable in the near future. Such uses re-
they work through persistence of vision, and therefore            quire high-fidelity 3D reproductions of objects, and
even if the active elements could be made opaque, no              include areas as diverse as product design, medical
part of the image is permanently located in its physical          imaging and telemedicine, 3D cartography, scientific




WSCG 2012 Communication Proceedings                         145                                        http://www.wscg.eu
visualization, industrial prototyping, remote resource                       capable volumetric 3D display. Applied
exploration, professional training and architecture.                         Optics, 46(8):1244–1250, 2007.

Such wide appeal has led to the rapid development of              [DM03]     S. Dudnikov and Y. Melnikov. Review of
many techniques for 3D visualization, and sometimes                          technologies for 3D acquisition and dis-
this has resulted in poorly-defined boundaries between                       play. Technical report, EU Project IST-
techniques – especially hybrid ones. This work pre-                          2001-38862 TDIS, 2003.
sented a comprehensive taxonomy of 3D displays, fo-
                                                                  [Dod05]    N.A. Dodgson. Autostereoscopic 3D dis-
cusing on fundamental characteristics rather than im-
                                                                             plays. Computer, 38(8):31–36, 2005.
plementation details. This property should make the
taxonomy robust and expansible to include new tech-               [EucBC]    Euclid. Optics. 300 B.C.
niques and innovations. It also provides a high-level
overview of the 3D displays landscape, a useful tool              [Fav05]    G.E. Favalora. Volumetric 3D displays
for researchers entering the field.                                          and application infrastructure. Computer,
                                                                             38(8):37–44, 2005.
An important property of the proposed taxonomy is
that it equips both researchers and practitioners with            [FBS86]    D.S. Falk, D.R. Brill, and D.G. Stork. See-
a well-defined field map which enables application-                          ing the Light. chapter 14: Holography,
based exploration of the 3D display space. Logically                         pages 389–391. Wiley, 1986.
separated groups of technologies allow a faster analy-            [GCM05] S.C. Goldstein, J.D. Campbell, and T.C.
sis of desired properties, such as the ability to perform                 Mowry. Programmable matter. Computer,
direct manipulation on the virtual objects at their ap-                   38(6):99–101, 2005.
parent locations, or to overlay the images onto the
real world, to provide augmented reality, or to operate           [GW07]     T. Grossman and D. Wigdor. A taxonomy
without headgear. Proper accommodation might be                              of 3D on the tabletop. In IEEE Int’l Work-
crucial for high-precision applications, while support                       shop on Horiz. Interactive Hum.-Comp.
for multiple users is relevant in design contexts.                           Systems, pages 137–144, 2007.

Furthermore, a well-defined taxonomy should also en-              [Hal97]    M. Halle. Autostereoscopic displays and
able informed speculation over the 3D display space                          computer graphics. ACM SIGGRAPH
henceforth outlined, regarding possible new tech-                            Computer Graphics, 31(2):58–62, 1997.
niques and analysis of their feasibility and properties,
                                                                  [MK94]     P. Milgram and F. Kishino. A taxonomy
or alternatively, discarding of a specific combination
                                                                             of mixed reality visual displays. IEICE
of properties (or set thereof) due to economic, phys-
                                                                             Transactions on Information and Systems,
ical or technological limitations. This is expected to
                                                                             77(12):1321–1329, 1994.
enable new 3D display systems to be conceived. As an
example, one could easily conceive a static volumetric            [MZ92]     M. McKenna and D. Zeltzer. Three di-
display that provides occlusion, by using a substrate                        mensional visual display systems for vir-
that becomes opaque when excited. This could be a                            tual environments. Presence, 1(4):421–
relevant research topic in materials science.                                458, 1992.
This study now calls for further developments in the              [Oko76]    T. Okoshi. Three-Dimensional Imaging
form of an exhaustive listing of implementations and                         Techniques. Academic Press, 1976.
their calatoguing in a table or database that will allow
manual or automatic filtering and comparison of dif-              [Tak06]    Y. Takaki.     High-density directional
ferent display technologies and respective features.                         display for generating natural three-
                                                                             dimensional images. Proceedings of the
                                                                             IEEE, 94(3):654–663, 2006.
6. REFERENCES                                                     [UCES11] H. Urey, K. V. Chellappan, E. Erden, and
[Ben00]     S.A. Benton. Selected papers on three-                         P. Surman. State of the art in stereoscopic
            dimensional displays. SPIE, 2000.                              and autostereoscopic displays. Proceed-
[BS00]      B. Blundell and A. Schwarz. Volumetric                         ings of the IEEE, 99(4):540–555, 2011.
            Three-Dimensional Display Systems. Wi-
                                                                  [Wid01]    T. Widjanarko. Brief survey on three-
            ley, 2000.
                                                                             dimensional displays. Media, pages 1–27,
[CNH+ 07] O.S. Cossairt, J. Napoli, S.L. Hill, R.K.                          2001.
          Dorval, and G.E. Favalora. Occlusion-



WSCG 2012 Communication Proceedings                         146                                      http://www.wscg.eu
 Approximating the Fire Flicker Effect Using Local Dynamic
                      Radiance Maps
                           Jonathan Brian Metzgar
                                                                                 Sudhanshu Kumar Semwal
                           University of Colorado at
                                                                                  University of Colorado at
                               Colorado Springs
                                                                                     Colorado Springs
                             jonathan@metzgar-
                                                                                    ssemwal@uccs.edu
                                research.com

                                                            ABSTRACT
Realistic fire and the flicker eﬀect is a complicated process to simulate in realtime and little work has been done
to simulate this complicated illumination eﬀect in realtime. Fire is not a directionally uniform source of light but
varies in intensity not only with time but also with direction. Most realtime applications use a standard point light
source model for local illumination eﬀects and may use a model to change the light source intensity with time but not
direction. The problem is that point light sources are isotropic, but many sources of light have anisotropic qualities
as well. Radiance maps and Precomputed Radiance Transfer (PRT) have been used to increase realism at realtime
interactive frame rates. These models approximate global illumination by applying an environment map (typically
approximated with spherical harmonics) to get their soft lighting eﬀect. In this paper we present Local Dynamic
Radiance Maps (LDRM) which uses radiance maps in a local illumination model to add anisotropic behavior to
light sources. We implemented a realtime rendering engine that supports shadow mapping and the physically based
Cook-Torrance model to approximate global illumination. In particular, we generate dynamic radiance maps using
Perlin noise to simulate the nonlinear radiance of fire and we also implement a rudimentary Lattice-Boltzmann
flame rendering eﬀect. Finally, we show how LDRM can be applied not just to approximating the fire flicker eﬀect,
but as a general framework for simulating the illumination properties of other nonlinear light sources.
Keywords:        radiosity, global illumination, fire, Lattice-Boltzmann, radiance maps, shadow-mapping.

1    INTRODUCTION                                                            comes from a point light source with a dynamic inten-
Advances in graphics processing units (GPU) have re-                         sity. But, simply modulating the intensity and location
sulted in not only improved speed and quality of com-                        of a point light source does not adequately model the
puter generated images, but now feature massively par-                       way that fire radiates in a nonlinear way. It is this non-
allel processors capable of running several general pur-                     linear radiance that causes the flicker eﬀect to occur.
pose programs. This parallelism allows for the imple-                        For the last decade, radiance maps and precomputed ra-
mentation of global illumination algorithms. Physical                        diance transfer have become essential for creating high
simulations of natural phenomena like fire and water are                     quality realtime visualizations. Essentially by utiliz-
taking advantages of the hardware acceleration.                              ing approximations like spherical harmonics, they can
Rendering fire is a big challenge for computer graph-                        quickly apply the illumination model to objects in a
ics because it touches so many areas of image genera-                        scene and get radiosity like shading eﬀects. This model
tion. It is even harder to do it well in realtime. One                       works with an outward-in approach where the incoming
would want to eventually render fire based on a full 3D                      light is mapped to a sphere which is mapped to the pre-
simulation using Navier-Stokes equations and render a                        computed radiance map. We propose a variation of this
scene in realtime using the illumination eﬀects mod-                         method that 1) dynamically computes the radiance and
eled by a radiosity algorithm. Since this is not practical,                  2) is a local source of radiance which can move around
fire imagery is often created through precomputed ren-                       inside an environment. We call these local dynamic ra-
derings, video, or particle eﬀects and the illumination                      diance maps or LDRM. The main goal of the LDRM
                                                                             model is to make lights appear and act like they belong
                                                                             in the scene by modeling their anisotropic behavior as a
Permission to make digital or hard copies of all or part of this             function of time.
work for personal or classroom use is granted without fee pro-
vided that copies are not made or distributed for profit or com-             In this paper, we present a new way to approximate the
mercial advantage and that copies bear this notice and the full              flicker eﬀect by procedurally generating a LDRM into
citation on the first page. To copy otherwise, or republish, to              a cube map. This LDRM is projected outwards from
post on servers or to redistribute to lists, requires prior specific         the position of the fire source through the cube texture
permission and/or a fee.                                                     and onto the surrounding scene’s geometry simulating



WSCG 2012 Communication Proceedings                                    147                                         http://www.wscg.eu
the first bounce of radiosity. To achieve our results, we           expanding for dynamic scenes as well. For example,
have chosen bump mapping, physically based lighting,                [Kri05a] relight architectural models in real-time with
and soft shadow mapping algorithms to calculate direct              moving lights by combining precomputed point light
and indirect illumination of the fire source. Finally, we           source clouds.
implement a procedurally simulated fire eﬀect based on
a Lattice-Boltzmann model which can be simulated on                 3   THE POINT LIGHT MODEL
the GPU or CPU to approximate the flames emitted from
                                                                    The predominantly implemented illumination model for
a torch. This is rendered in our scene at the location of
                                                                    fire in realtime applications is the point light source
our fire source.
                                                                    model. It is used widely because there is hardware sup-
                                                                    port for the algorithm and also because it is easy to com-
2   PREVIOUS WORK                                                   pute the shading value since the light position is sub-
Simulating fire is a fluid simulation problem that has a            tracted from the vertex or fragment position to get the �L
variety of solutions ranging from artist derived flame              vector that is used by a Lambertian illumination model.
profiles, procedural generation of flames, and fluid                In some implementations, the intensity is constant with
simulation of flames. [Ngu01a] and [Ngu02a] present                 a distance based falloﬀ function. It can become more so-
a solution for Navier-Stokes equations to simulate the              phisticated by varying the intensity of the light or adding
flames but is time consuming. [Hon07a] combine                      a random perturbation to the coordinates of the light
both a Navier-Stokes simulation with detonation shock               source. The intensity and position are often varied us-
dynamics to get wrinkled flames and cellular patterns               ing a smooth noise or interpolation scheme. The easiest
but also takes a long time to render. A near realtime fire          way to think of this is a person holding a simple light
simulation and control system that uses artist derived              bulb with a rapidly sliding dimmer switch and a jittery
flame profiles is described by Lamorlette and Foster                hand. In Figure 1, you can see the smooth uniform in-
in [Lam02a]. The Perlin noise function and an artist                tensity that the point light model has. The problem is
specified flame profile is used to generate procedural              that point light sources are isotropic, but many sources
fire by [Ful07a].                                                   of light are anisotropic. We will now present the LDRM
The groundbreaking 1970’s work by [Har73a] and                      model which attempts to model the anisotropic features
[Har76a] first introduce the Lattice-Boltzmann Model                that fire and other natural phenomena possess.
(LBM) methods which have become the basis of many
non Navier-Stokes fluid simulations. [Wei02a] use                   4   THE LDRM MODEL
overlapping textures with turbulence details employing
LBM for motion of the flames. [Zha03a] also employ
LBM to simulate fire fronts around solid objects.
Some models, such as [Lam02a], discuss in much de-
tail how to compute the lighting eﬀects. For example
[Lam02a] uses an emitting sphere at each flame seg-
ment to generate lighting. It is assumed that the light-
ing details are automatically handled at the renderer
level which combine several global illumination algo-
rithms like radiosity, ray tracing, and/or photon map-
ping. Importance sampling using volumetric illumina-
tion has been used by [Zha11a]. GPU simulation with
volumetric data creates a variety of realistic and detailed
fire simulation such as moving fire [Hor09a],
Radiance maps are images that store the intensities of
                                                                    Figure 1: The LDRM method is compared to the point
light passing through each pixel. The direction of the
                                                                    light source method. In the LDRM image, the intensity
light is determined by the projection used to create the
                                                                    and frequency is turned up very high to clearly show
image. They are used in high dynamic range imagery
                                                                    the diﬀerence of the LDRM method and the point light
(HDRI) as described by [Deb97a]. Methods such as
                                                                    source method. Realistic settings would be tuned to be
precomputed radiance transfer (PRT) first introduced
                                                                    more subtle.
by [Slo02a] use a spherical harmonics representation
for low-frequency radiance computation. These spher-                Our Local Dynamic Radiance Map (LDRM) model
ical harmonics representations approximate a high res-              stores a dynamically computed radiance map at co-
olution HDRI radiance map and are very eﬀective for                 ordinates P. The light emitted from P is cast in all
relighting objects in an environment. Primarily they                directions simulating the first bounce of radiosity. The
have been used for static scenes but the technique is               radiance may either be precomputed as an animation



WSCG 2012 Communication Proceedings                           148                                          http://www.wscg.eu
or procedurally generated in realtime. A cube map or
other similar abstraction is an ideal way of storing these
radiance values.
We reproduce Kajiya’s rendering equation [Kaj86a] be-
low so we can illustrate how the LDRM fits into this
standard model:
                                      �
  I(x, x� ) = g(x, x� )[ε(x, x� ) +       ρ(x, x� , x�� )I(x� , x�� )dx�� ]. (1)
                                                                                         Figure 3: The LDRM method (right) diﬀers from the
The radiance of the LDRM is represented by ε(x, x� )                                     point light source method (left) by modeling the light’s
while being directly aﬀected by the visibility of the point                              outgoing radiance as a function of time, intensity, and
x at point x� by the function g(x, x� ). More specifically,                              direction.
the function ε(x, x� ) is the radiance coming from direc-
tion �L where �L is the vector from the position of the light                            A blackbody radiation color map is used to give color to
source to the point x, and is the direction of the sample.                               the torch fire. An example color map is shown in Figure
This value can be obtained by looking into the radiance                                  7.
map using the direction provided. For example, most
                                                                                         The LDRM is flexible. If a variety of natural phenom-
graphics hardware have the ability to easily look up this
                                                                                         ena used the same kind of simulation algorithm but dif-
value from a cube map using a vector as an input.
                                                                                         fered only in color, a diﬀerent color map will easily al-
                                                                                         low for adjusting that. For example, fire could probably
                                                                                         use the same simulation code, but the specific chemical
                                                                                         combustion properties would be approximated by map-
                                                                                         ping the resulting intensity values with the appropriate
                                                                                         color map.
                                                                                         In our implementation, we have chosen Perlin noise be-
Figure 2: Two LDRMs generated using diﬀerent fre-                                        cause it generates smooth noise that can be animated
quencies of noise and their corresponding eﬀect on the                                   and provides enough variety for our ideas to be imple-
environment.                                                                             mented. Generation of a LDRM function that simulates
From Kajiya’s rendering equation, we then map this to a                                  the unique properties of fire (or other phenomena) is
simplified model where we can incorporate our render-                                    most definitely a topic for future study but is outside the
ing algorithms. Since we are not simulating the integral                                 scope of our research. Later we will discuss this pos-
in his equation, we decided to make that a constant value                                sibility, but our experiments with Perlin noise showed
and focus on just ε and g which is just the radiance of the                              significant enough improvement in scene realism ver-
light source and the visibility of the light source with the                             sus the traditional point light source method that we dis-
surface being illuminated. Essentially, the LDRM acts                                    cussed in the last section.
like a Gaussian surface in the sense, that instead of try-
ing to compute the interactions with the actual flames or
other phenomena and the surrounding environment, we
perform an intermediate step of mapping it to a surface
we can easily use in a rendering situation. This is clearly
seen in Figure 3.
The incoming radiance which we will now call R, is then
divided into the specular and diﬀuse reflection colors
kspecular R and kdi f f use R, respectively where kspecular +
                                                                                         Figure 4: The LDRM projects radially from the center
kdi f f use = 1. Depending on the reflectance model used,
                                                                                         of the light position. Areas in blue get ambient lighting
kspecular and kdi f f use may be computed diﬀerently. We
                                                                                         while others get direct illumination.
decided to implement the Cook-Torrance model and we
will discuss later in section 4.1 how to compute these                                   Figure 4 shows a diagram of how the LDRM method
values.                                                                                  works. The box located around the position of the light
The dynamic radiance of a torch fire is approximated                                     represents the cube map. The arrows emitted from the
by using Ken Perlin’s noise function. The radiance of                                    center of the light through the box will look up the ap-
each direction of the torch fire is computed and stored                                  propriate radiance and project it on the environment. If
inside the radiance map. The unit vector l is used as                                    the area is in shadow (represented by shaded blue areas)
input to the Perlin noise function and the resulting value                               then an ambient algorithm can determine the final illu-
is used to look up the color associated with the radiance.                               mination of those fragments. The containing rectangle



WSCG 2012 Communication Proceedings                                                149                                          http://www.wscg.eu
and the red and green rectangles represent the environ-            N can be varied based on the hardware capabilities of
ment and objects visible to the light source. Figure 2             the system. The penumbra width is adjustable as a con-
shows two example LDRMs generated using two diﬀer-                 stant parameter in the shader program. Figure 6 shows
ent frequencies of noise. It can be easily seen how the            two screenshots of the program using 1 sample shadows
LDRM works in a practical sense by observing that vari-            and 257 sample, wide penumbra shadows.
ation in intensity in the environment corresponds to the
frequency of noise.

5     IMPLEMENTATION


                                                                   Figure 6: These two images shows a basic one sample
                                                                   shadow and a wide penumbra, 257 sample shadow.

                                                                   5.2    Radiance Cube Map Generation
                                                                   Perlin noise is a simple solution to generating non-linear
                                                                   radiance that is repeatable, smooth, and fluid. Depend-
                                                                   ing on application performance, the noise can either be
Figure 5: A rendering of the fire flicker eﬀect program.           generated on the GPU or CPU. The fire program im-
                                                                   plemented in this paper used the GPU and a render-to-
Our fire flicker eﬀect simulation presented in this paper
                                                                   texture set up to render the six sides of a cube map. The
is designed to employ the LDRM model. A variety of
                                                                   GPU code for generating Perlin noise was implemented
rendering algorithms is used to simulate global illumi-
                                                                   by [Gus06a] which we slightly modified to adjust for
nation and a screenshot is shown in Figure 5. The global
                                                                   noise scaling and animation parameters used in the fire
illumination algorithm implements the Cook-Torrance
                                                                   program.
model, Blinn bump mapping, and cube map shadow
mapping. It uses a simple ambient function that approx-            The six textures are used as a cube map in the final ren-
imates indirect illumination by scaling the direct illu-           dering pass by the global illumination shader. The gray
mination by the amount of shadow present at that pixel             scale output of the radiance is then converted from heat
location. The Cook-Torrance model allows for physi-                values to RGB values by looking up the data in a color
cally based illumination while the bump mapping algo-              look up table. The texture is updated once per frame
rithm allows for increased higher-frequency pseudo de-             or as needed to maintain a target frame-rate. The color
tails. Finally instead of rendering the LDRM cube map              look up table is shown in Figure 7 and one side of a
in the scene, flames are dynamically computed and ren-             LDRM is shown in figure 8.
dered into 2D textures and drawn onto rectangles in a
fan like structure to give the torch a 3D look. In eﬀect,
the torch fire is used as an aesthetic place mat to show
where the LDRM is located in the scene.                            Figure 7: The color look up table mapping heat to their
                                                                   corresponding RGB values.
5.1    Illumination and Shadow Model
The Cook-Torrance model was chosen because it is a
physically based model. Other models can easily be in-
tegrated as desired. The LDRM was used to supply the
specular color kspecular R for the Cook-Torrance model.
This color is mixed in with the surface color of the frag-
ment being rendered and scaled by the dot product of               Figure 8: One face of a cube map generated by the
the surface normal and incoming light vector �L.                   LDRM method.
Since a torch is an omni-directional light source, cube            Special care needs to be taken to balance the noise so
mapped shadow mapping was selected to render the                   that it adds a subtle lighting eﬀect to the scene. If the
shadows. It is a fairly straightforward algorithm to im-           frequency of the noise is too high then the eﬀect may
plement, but it does take some tweaking to get the high-           look overdone where it can quickly be distracting. On
est image quality. We chose to write a scalable multi-             the other hand, using hardly any noise or under using the
sampled shadow algorithm that we could adjust to mea-              eﬀect will look as if the eﬀect is not being used, so care-
sure performance of our technique. The number of sam-              ful balancing needs to done to find a good range where
ples can go from one sample to N = 257 samples. Here               the eﬀect will be eﬀective. Figure 2 shows this eﬀect in



WSCG 2012 Communication Proceedings                          150                                          http://www.wscg.eu
practice. Note how the high frequency map may make                where �Nvertex is the interpolated vertex normal, �Nbump
the environment look splotchy which is not realistic.             is the per pixel normal derived from the normal map,
                                                                 �L is the incoming direction of the light source, n is the
The LDRM may be used to create good lighting eﬀects,
but it is not complete without some motion of the shad-           number of samples being used for the shadows, and s j is
ows in the environment. Perlin noise is used once again           the boolean result of comparing the jth pixel depth value
to compute a time-varying oﬀset which we add to the               to the light depth buﬀer which is either 1 or 0. Taking the
original position of the light. This new position is used         minimum of the dot products eliminates bump mapping
to render the shadow maps and lighting. The final prod-           on polygons not facing the light.
uct then has dancing shadows which enhance realism.                     Together the ambient and shadow terms are used to de-
                                                                        termine the minimum illumination level of the fragment
5.3 Global Illumination in the Simulation to be rendered. A simple maximum function is used to
                                                                        choose the ambient term or shadow term. If a fragment
The equation                                                            is completely shadowed, then the ambient term is used,
                                  �                                  � otherwise the fragment is in penumbra and has some il-
Cout = max (Rambient , Rshadow )· kdi f f use + Rspecular · kspecular .
                                                                  (2)   lumination.
is the basis for the global illumination algorithm for the
fire program. The Rambient term specifies the ambient 5.5 Diﬀuse and Specular Term
intensity of the pixel, the Rshadow represents the contri- The diﬀuse term kdi f f use is generated from the surface
bution of any direct lighting occuring at the pixel, the color or texture of the object. The specular terms
kdi f f use term is the color of the surface at that pixel, the Rspecular and kspecular are the coeﬃcient of the reflected
kspecular term is the color of the specular reflection of the light and its color, respectively. This is where we can
pixel, and the Rspecular term is the amount of reflected incorporate the LDRM model. The kspecular value is
light at the pixel.                                                     obtained by using the �L vector as the lookup in the
The terms are all computed from four diﬀerent algo- LDRM cube map. The Rspecular term is based oﬀthe
rithms. The first algorithm is bump mapping which cal- Cook-Torrance model equation [Coo81a]
culates the normal of the pixel. The second algorithm                                            F       DG
is the Cook-Torrance model which calculates the specu-                             Rspecular =                    .          (5)
                                                                                                 π (N · L)(N ·V )
lar reflectance values Rspecular and kspecular of the pixel.
The third algorithm is the Cube Map Shadow algorithm                 F is the Fresnel term, D is the micro-facet distribution
which allows for omni-directional point light sources.               factor, G is the geometric attenuation factor, V is the
Finally, the fourth algorithm is an intensity falloﬀmodel            view vector, and L is the vector from the light to the frag-
for the Rambient term to model indirect light. These have            ment. Additional details about using the Cook-Torrance
been covered in detail in previous works, but integrat-              may be found by referring to the original paper. It is im-
ing them together will be briefly explained in light of              portant to note that the LDRM model is not just limited
the equation to compute Cout .                                       to Cook-Torrance, but may be incorporated with any il-
                                                                     lumination model.
5.4    Ambient and Shadow Term
                                                                     5.6    CPU and GPU 2D Flame Simulation
The ambient term is a simple approximation based on
a inverse falloﬀ law from the distance to the fire. The              Our flame rendering system is based oﬀ a simple cellular
ambient term Ra is computed by the formula                           automata model to generate fire. This cellular automata
                                                                     is a simplified model of Lattice Boltzmann Methods
                                   1                                 (LBM) which originated with the work of [Har73a].
                     Rambient =        .                 (3)         [Che98a]’s work summarize the developments of the
                                  4|L|
                                                                     model into its more current form. The method works
This equation is a variation based on the inverse power              by using a lattice structure representing the fluid to be
           P
law I = 4πr  2 which gives us a brighter overall light in-           simulated. A convection operator and collision oper-
tensity which is normally lost unless you do a full on               ator transform the lattice over time and cause the fluid
radiosity simulation to get the intensity back through in-           process to occur. The nineties demo scene fire eﬀect
direct reflections. This gives us some of that light which           used a simple averaging function to cause convection
is normally “lost” in a local illumination model.                    and simulate collisions. Figure 9 shows a screenshot
The shadow term Rshadow is computed with the follow-                 that simulates this full screen fire eﬀect.
ing formula:                                                         This fire eﬀect can be modified to generate small flames
                                                                     or torch sources. Further improvements can be made to
                �                        � 1     n
                                                                     increase precision as well. Typically this eﬀect uses 8-
   Rshadow = min �Nvertex ·�L, �Nbump ·�L ∗
                                            n   ∑ sj     (4)
                                                                     bit integer mathematics to store the heat values. While
                                                j=0




WSCG 2012 Communication Proceedings                            151                                           http://www.wscg.eu
                                                                    by a factor li f e, respectively. Heat is added to all the
                                                                    fragments located inside a circle (a “heat sink") which
                                                                    is randomly jittered according to the desired turbulence
                                                                    of the flame.

Figure 9: This image shows a fire simulation where the
entire bottom row is used as the heat source and that
use of integer math causes noisy artifcats near these ran-          @VERTEXSHADER
domized heat sources.                                               uniform mat4 ProjectionMatrix;
                                                                    varying vec2 uv;

this is accurate enough for most of the eﬀect, it results           void main() {\\
in artifacts near the source of the fire as shown in Figure         uv = gl_MultiTexCoord0.st;
9. Changing the representation from integers to floating                gl_Position = ftransform();
point math eliminates these artifacts and increases dy-             }

namic range. This can be coupled with established HDR               @FRAGMENTSHADER
techniques and physically based color computations for              #version 140
diﬀerent chemical reactions.                                        uniform sampler2DRect FireLattice;
                                                                    uniform sampler1D radianceCLUT;
The flame is generated by adding or seeding heat to                 uniform float a, b, radius;
points on the lattice. The flame will flow during the               uniform float width, height;
convection operator step. During the collision opera-               uniform float heat, life;
                                                                    uniform float turbulence;
tor step, the flame mixes together. The three steps will            in vec2 uv;
cause the flame to take shape as this process repeats.              out vec4 gl_FragColor;
A simple circular falloﬀ model is used for seeding the              float rand(vec2 co) {
heat to the fire. Notice in Figure 11 two diﬀerent kinds              return fract(sin(dot(co.xy ,vec2(
                                                                        12.9898,78.233))) * 43758.5453);
of falloﬀ patterns: the simple radial falloﬀ used in the            }
fire program and a noisy radial falloﬀ used for the fires           void main() {
in Figure 10. By quickly changing the location of the                 float x = uv.s, y = uv.t;
falloﬀ pattern, turbulence is created. The fire eﬀect and             float data = 0;
                                                                      float r2 = radius * radius;
varying levels of turbulence are shown in Figure 10.
                                                                        if (x >= 1 && x < width-2 &&
                                                                            y >= 3 && y < height-1) {
                                                                          if (x >= a-radius && x < a+radius &&
                                                                              y >= b-radius && y < b+radius) {
                                                                            float f = (x-a)*(x-a) + (y-b)*(y-b);
                                                                            if (f < r2) {
                                                                              data = texture(FireLattice,
Figure 10: The radius of the circle in which the center                                      vec2(x, y)).a;
of the flame source is moved causes a more turbulent                          data += heat * (1 - f/r2);
                                                                           }
flame. On the far right, improperly handled edges cause                   }
“heat sink” artifacts.                                                    data+=texture(FireLattice,
                                                                                        vec2(x, y+1)).a;
                                                                          data+=texture(FireLattice,
                                                                                        vec2(x-1, y-1)).a;
                                                                          data+=texture(FireLattice,
                                                                                        vec2(x+1, y-1)).a;
                                                                          data+=texture(FireLattice,
                                                                                        vec2(x, y-2)).a;
Figure 11: Two falloﬀ patterns for seeding fires. The                     data = clamp(data * life / 4.0,
second pattern adds some turbulence to the resulting                                   0.0, 1.0);
                                                                        } else {
flames.
                                                                          data = 0;
This fire eﬀect can be computed using a GPU and                         }
a graphics based shader language (i.e. GLSL) was                        vec3 color2 = texture(radianceCLUT,
                                                                                              data).rgb;
adequate for our simulation. The algorithm is shown                     gl_FragColor = vec4(color2,data);
in Listing 1 and is fairly straightforward. It determines           }
whether the fragment is in the simulation area or not
                                                                    Listing 1: A GLSL Shader that computes the flame sim-
(which if not handled correctly creates “heat sinks"
                                                                    ulation.
shown on the far right in Figure 10). Simulating
diﬀusion and cooling is obtained by averaging several
neighbor samples at each fragment and multiplying



WSCG 2012 Communication Proceedings                           152                                         http://www.wscg.eu
6   RESULTS
The LDRM model takes up relatively little extra load
in conjunction with normal rendering depending on the
number of lights being used. The majority of perfor-
mance loss comes from shadow mapping when large
numbers of samples are being used. Rendering high res-            Figure 13: On the left, we see that application perfor-
olution LDRM cube maps may also reduce performance                mance is not aﬀected until high resolution lattice sizes
but this can be mitigated by using low resolution maps            are used. On the right, the GPU far surpasses the CPU
when large numbers of lights are being used.                      in raw fire rendering performance.
The benchmarks were conducted using a Windows 7
OS, Intel i7 930 2.80GHz processor with 6GB of RAM,               our flame rendering algorithm with others because ours
and a NVIDIA GeForce GTX 480 graphics card with                   is not volumetric and has very strong boundary condi-
1.5 GB of GDDR5 memory. Each benchmark was mea-                   tions which make it incapable of handling interactions in
sured by recording the number of frames per second                a 3D environment which a volumetric simulation could.
(FPS) once per second over a period of 25 seconds. The            When integrated with the global illumination simula-
mean frame rate was then computed to filter noise in              tion, the GPU advantage becomes more obvious. CPU
the readings, though the noise present was so low that            performance drops oﬀ fast when using high resolution
it had an insignificant eﬀect on the final numbers. Fi-           lattice simulations, though it is almost unnoticeable
nally, we kept the frame rate as high as possible so we           when using reasonably sized maps. In contrast, the
could ensure that our simulation would run on less ca-            GPU simulations have a very small performance
pable graphics cards.                                             penalty when using large lattices. It should be noted
                                                                  that multithreading was not used in the CPU simulation,
                                                                  but the GPU still has enough compute power for the
                                                                  large lattice sizes that even an 8 core CPU could not
                                                                  outperform it. Figure 13 shows the performance graphs
                                                                  for running the global illumination simulation and
                                                                  rendering the flames with either the CPU, GPU, or
                                                                  both. It also shows the baseline performance of the GI
                                                                  simulation without rendering the flames. Eﬀectively,
                                                                  you get the flame rendering for free for small resolution
                                                                  flame images.
Figure 12: A comparison of radiance cube map size ver-
sus performance.
We tested the performance of our method using diﬀer-
ent resolution LDRM cube maps and by not rendering
them at all. Figure 12 shows the results when 1, 2, 4, or
8 lights are being used. When using a 256x256 LDRM
cube map, performance drops by 41%, 68%, and 83%                  Figure 14: On the left, antialiasing halves overall per-
for 2, 4, or 8 lights, respectively. When using 512x512           formance. On the right, reasonable numbers of shadow
cube maps, performance drops by 39%, 66%, and 82%                 samples still allow interactive frame rates.
for 2, 4, or 8 lights, respectively. For 1024x1024 cube
maps, performance drops by 33%, 60%, and 77% for                  Finally, we examine the performance of the global illu-
2, 4, or 8 lights, respectively. Compared to not us-              mination algorithm. Figure 14 shows the performance
ing LDRMs at all, performance drops by 4% to 17%                  of the global illumination algorithm both when using
for 256x256 cube maps, 6% to 25% for 512x512 cube                 multi-sampled shadows and diﬀerent resolution shadow
maps, and 11% to 42% for 1024x1024 cube maps.                     maps, respectively. When using a reasonable number of
Next, we tested the performance of our flame rendering            lights, it is very easy to obtain very interactive rates. Per-
system on the CPU and the GPU. Overall, the GPU had               formance drops quite a bit when using anti-aliasing, but
a clear lead in performance especially as resolution is           the image quality is greatly improved and small pixel ar-
increased. However, until much higher resolutions of              tifacts that show up when not using anti-aliasing almost
flame simulations are used, the number of flames ren-             entirely disappear.
dered per second on the CPU was in the hundreds which             Shadow quality is very good at 33 samples per pixel and
is suﬃcient. It is also clearly shown that using the GPU          the frame-rate is quite interactive. Wide penumbras are
in conjunction with the CPU yielded little decrease in            allowed which increases the realism of far oﬀ shadows
overall performance. We are unable to easily compare              where the area lighting eﬀect of the flames would not



WSCG 2012 Communication Proceedings                         153                                            http://www.wscg.eu
create sharp edges. The shadow map size aﬀects per-                   [Gus06a] Gustafson, S., Dsonoises, a set of useful functions
formance but not as dramatic as varying the number                       for sl., 2007. url:http://staﬀwww.itn.liu.se/~stegu
of samples. Memory usage does increases quickly so                       /aqsis/DSOs/DSOnoises.html
tweaking is necessary to determine the lowest accept-                 [Har73a] Hardy, J., Pomeau, Y., and de Pazzis, O., Time evo-
able shadow map resolution.                                               lution of a two-dimensional classical lattice system. Phys.
                                                                          Rev. Lett. 31, 5, 1973, pp. 276–279.
Adding motion to the shadows does a good job of dis-
                                                                      [Har76a] Hardy, J., de Pazzis, O., and Pomeau, Y., Molecular
tracting the observer from noticing some minor prob-
                                                                          dynamics of a classical lattice gas: transport properties
lems with shadow mapping. Some of these problems
                                                                          and time correlation functions. Phys. Rev. A 13, 5 (May),
include light leakage or surface acne. Ultimately, the                    1976, pp.1949-1961.
moving shadows create the realistic appearance that the
                                                                      [Hon07a] Hong, J.-M., Shinar, T., and Fedkiw, R., Wrinkled
fire has on the scene while the LDRM model adds a sub-
                                                                         flames and cellular patterns. In ACM SIGGRAPH 2007
tle ambience to the scene, that when switched oﬀ, makes                  papers, 2007, ACM, New York, NY, USA, SIGGRAPH
the simple intensity modulation based fire flicker eﬀect                 ’07.
seem somewhat lifeless.
                                                                      [Hor09a] Horvath C and Geiger W., Directable high Resolu-
                                                                          tion simulation of fire on the gpu. In ACM SIGGRAPH
                                                                          2009 papers, 2009, ACM, New York, NY, USA, SIG-
7   CONCLUSION AND FUTURE WORK
                                                                          GRAPH ’09, 28(3).
The LDRM model presented in this paper helps add re-                  [Kaj86a] Kajiya, J. T., The rendering equation. In ACM SIG-
alism to scenes where torch fires are being used. The                     GRAPH 1986 papers, ACM, New York, NY, USA, SIG-
ambiance created by using shifting anisotropic illumi-                    GRAPH ’86, 1986, pp. 143-150.
nation patterns add subtle depth and realism to scenes                [Kri05a] A. W., Akenine-Möller, T., and Jensen, H. W., Pre-
compared to the simple point light source model. The                      computed local radiance transfer for real-time lighting
performance penalty is small and the algorithm is trivial                 design. ACM Trans. Graph. 24, July 2005, pp. 1208-
to implement for any realtime graphics engine.                            1215.
Future study of LDRMs to enhance direct illumination                  [Lam02a] Lamorlette, A., and Foster, N. Structural model-
                                                                         ing of flames for a production environment. Proceedings
is promising and is an excellent extension to normal pre-
                                                                         of the 29th annual conference on Computer graphics and
computed radiance transfer. In the future we are looking
                                                                         interactive techniques, ACM, New York, NY, USA, SIG-
into simulating a volumetric fire and comparing the ac-                  GRAPH ’02, 2002, pp. 729-735.
tual radiance with our approximation. We believe that
                                                                      [Ngu01a] Nguyen, D. Q., Fedkiw, R. P., and Kang, M. A
creating LDRM models of other nonlinear light sources
                                                                         boundary condition capturing method for incompressible
would be highly beneficial towards accurately simulat-                   flame discontinuities. Journal of Computational Physics
ing other phenomena in a realtime application. Finally,                  172, September, 2001, pp. 71–98.
we are looking into using spherical harmonics as a sub-
                                                                      [Ngu02a] Nguyen, D. Q., Fedkiw, R., and Jensen, H. W.
stitute for cube maps which may allow LDRMs to be                        Physically based modeling and animation of fire. Pro-
used in resource limited environments.                                   ceedings of the 29th annual conference on Computer
                                                                         graphics and interactive techniques, ACM, New York,
                                                                         NY, USA, SIGGRAPH ’02, 2002, pp. 721–728.
8   REFERENCES
                                                                      [Slo02a] Sloan, P.-P., Kautz, J., and Snyder, J. Precomputed
[Che98a] Chen, S., and Doolen, G.D., Lattice boltzmann                    radiance transfer for real-time rendering in dynamic, low-
   method for fluid flows. Annual Review Fluid Mechanics,                 frequency lighting environments. Proceedings of the 29th
   1998, pp.329-364.                                                      annual conference on Computer graphics and interactive
[Coo81a] Cook, R. L., and Torrance, K. E., A reflectance                  techniques, ACM, New York, NY, USA, SIGGRAPH ’02,
   model for computer graphics. Proceedings of the 8th an-                2002, pp. 527–536.
   nual conference on Computer graphics and interactive               [Wei02a] Wei, X., Li, W., Mueller, K., and Kaufman, A.
   techniques, ACM, New York, NY, USA, SIGGRAPH                          Simulating fire with texture splats. Proceedings of the
   ’81, 1981, pp. 307–316.                                               conference on Visualization ’02, IEEE Computer Soci-
[Deb97a] Debevec, P. E., and Malik, J., Recovering high dy-              ety, Washington, DC, USA, VIS ’02, 2002, pp. 227–235.
   namic range radiance maps from photographs. Proceed-               [Zha03a] Zhao, Y., Wei, X., Fan, Z., Kaufman, A., and Qin,
   ings of the 24th annual conference on Computer graphics                H. Voxels on fire. Proceedings of the 14th IEEE Visual-
   and interactive techniques, ACM, New York, NY, USA,                    ization 2003 (VIS’03), IEEE Computer Society, Wash-
   SIGGRAPH ’97, 1997, pp. 369–378.                                       ington, DC, USA, VIS ’03, 2003, pp. 36.
[Ful07a] Fuller, A. R., Krishnan, H., Mahrous, K., Hamann,            [Zha11a] Zhang, Y., Zhu, D., Qiu, X., Wang, Z. Important
    B., and Joy, K. I., Real-time procedural volumetric fire.             Sampling for volumetric illumination of flames. Visual
    In Proceedings of the 2007 symposium on Interactive 3D                Computing in Biology and Medicine, VR in Brazil, Com-
    graphics and games, ACM, New York, NY, USA, I3D                       puter & Graphics, 35(2), 2011, pp. 312-319.
    ’07, 2007, pp. 175–180.



WSCG 2012 Communication Proceedings                             154                                             http://www.wscg.eu
    A Matching Shader Technique for Model-Based Tracking
                                   Martin Schumann, Jan Hoppenheit, Stefan Müller
                                              University of Koblenz-Landau
                                         Institute of Computational Visualistics
                                                56070 Koblenz, Germany
                                      {schumi, silver, stefanm}@uni-koblenz.de


                                                         ABSTRACT
We present a line feature matching method for model-based camera pose tracking. It uses the GPU for computing
the best corresponding image line match to the edges of a given 3D model on a pixel basis. Further, knowledge
about the model is considered to improve the matching process and to define quality criteria for match selection.
Each edge is rendered several times with image offsets from the last estimated position of the model. The shader
counts the number of pixels in an underlying canny-filtered camera input image. Returning the best fit by pixel
count can be done applying occlusion queries. A speed-up can be achieved using a more elaborate shader with
texture read-back reducing the number of rendering passes. The matching shader is not limited to work with lines
and can be extended to other structures as well.

Keywords
Model-Based Camera Pose Tracking, Line Feature Matching, GPU Shader.

1    INTRODUCTION                                                         feature cloud maps are reconstructed from the visible
                                                                          surroundings.
Camera pose tracking is the process of estimating the
viewing position and orientation of a camera. This can                    In the approach of analysis-by-synthesis even further
be performed using a model of the environment repre-                      knowledge about the model is used for the tracking pro-
sented by 3D data available from a modeling process                       cess. Beginning from an initially estimated pose or the
or created online. Using a model leads to more stable                     pose of the given 3D model in the last image, a ren-
tracking without drift occurrence, as it is the case for                  dered image or a structure of features is synthesized to-
frame-to-frame tracking. Further the model serves as                      gether with a collection of additional information avail-
an absolute reference for initialization.                                 able from rendering process or from global knowledge.
                                                                          In the analysis step these are compared to a real camera
The pose estimation problem is based upon establish-                      image to estimate the current camera pose. In [Wue07]
ing 2D-3D correspondences between features of the                         they use depth and normal information to derive the
model and features in the camera image that may be                        line trait of the model. The work of [Sch09] analyzes
points, lines or higher structures. The aim is to min-                    similarity-based and feature-based methods for com-
imize the distance between projected 3D features and                      paring synthetic and real image and [Bra11] simulate
their 2D correspondences in the camera image. Estab-                      the lightning conditions to improve tracking.
lishing these correspondences is crucial for estimating
a good camera pose. False matches lead to shifting in                     We present a method for matching model edges to lines
the pose, jittering or even loss of the tracking.                         in the camera image using the GPU. It uses the model
                                                                          knowledge to define the quality of the matches for
Tracking on CAD models was realized by [Com03],                           match selection. In our approach we work with straight
and respectable success in combination of edges with                      lines but the technique is not limited to this type of fea-
texture information could be demonstrated by [Vac04].                     ture and can be used for other structures as well.
Current research is focused on SLAM (Simultaneous
Localization and Mapping) algorithms [Kle07], where                       2   RELATED WORK
                                                                          The problem of matching and registration of images
                                                                          does not only appear in camera pose tracking but also
Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without
                                                                          in applications of object recognition and image regis-
fee provided that copies are not made or distributed for profit           tration e.g. for medical purposes. In our approach we
or commercial advantage and that copies bear this notice and              want so solve for the 3D pose of a camera in a model-
the full citation on the first page. To copy otherwise, or re-            based tracking system. What we are focusing on, is
publish, to post on servers or to redistribute to lists, requires         a method for line feature-based model-image matching
prior specific permission and/or a fee.                                   so that the knowledge about the model geometry and



WSCG 2012 Communication Proceedings                                 155                                          http://www.wscg.eu
perspective can be used to improve the correspondences             pose, detecting lines in the image and back-projecting
and to define quality criteria, which is not a usual task.         to the model in order to gain 3D coordinates. Instead
Possible approaches for matching are intensity-based               we use the model data structure directly by selecting
similarity measures regarding the entire image or                  individual edges and performing a visibility test. So
patches of it, analysis of the image in the frequency              the image processing step on the rendered image can be
domain or discrete image features like points and lines,           omitted. The advantage of a candidate edge list is that
describing visually perceivable structures in the image.           the matching result can be sorted and weighted by the
                                                                   quality of the matches.
Detection and matching of feature points has been de-
veloped for a long time. First, the locations of interest-         For these 3D model edges corresponding 2D line
ing points like edge crossings or corners are detected in          matches should be found in the camera image. This
the image. The pixel surrounding of an interest point              camera image is canny-filtered so that natural structures
is then described by a vector of intensities, and may              are expressed as a binary image. The model edges
also include scale and orientation as SIFT [Low99] and             selected for matching are projected and rendered with
SURF [Bay08] do. Matching is realized by comparing                 a matching shader from the pose of the last estimation
the entries of these descriptors which may be very time            with frame buffer write disabled. For each drawn
consuming due to scale space calculation. For accelera-            pixel of the model edge the called pixel shader reads
tion there exist GPU-based implementations of feature              the value of the underlying canny image at the pixel
detection, matching or tracking algorithms as the well-            position. If there is a black edge pixel in the canny
known KLT [Shi94] by [Sin06].                                      image, the shader outputs a color. Otherwise it is
                                                                   discarded and the render pass will interrupt. The
Feature edges can be detected by common image pro-
                                                                   concept is displayed in figure 1 and listing 1 shows the
cessing filters like the Sobel operator or more advanced
                                                                   matching shader in GLSL code.
developments as the canny algorithm [Can86]. Sobel
and Canny implementations using the GPU in the con-
text of a particle filter framework are shown in [Kle06]
and [Bro12]. Line matching is mainly realized by min-
imizing the Euclidean distances between the projected
model edges and corresponding gradients in the image.
A simple distance measure may be gained by projecting
the start and end point of the model edge to the image
line or matching in parameter space. However, this re-
quires a parameter transformation as Hough [Dud72],
which may be expensive. In [Low91] simply the per-
pendicular distances of the projected model and the 2D
image segments are used and in [Low92] a combination
of distance and orientation is proposed.
Another popular distance-based matching method
is the Moving Edges algorithm [Bou89].         It is
used in various tracking frameworks as shown in                    Figure 1: Rendered edge (red), image pixel edge
[Har90],[Dru02],[Com03] or [Vac04] to name some of                 (black) and common pixel to be counted (hatched).
them. The model edge is sampled for control points
                                                                   The number of successful render passes now corre-
and from these, orthogonal search lines are spanned
                                                                   sponds to the number of image line pixel counted. Re-
in both directions. Alongside these line normals the
                                                                   trieving this result number can be done by running oc-
gradient maximum of the image is calculated and the
                                                                   clusion queries while rendering (Section 3.3). The pixel
distance between 3D control point and 2D image point
                                                                   count itself tells us about the probability that a found
found is minimized. To deal with possible multiple
                                                                   line in the canny image corresponds to a model edge.
gradient maxima the approach can be improved using
                                                                   The number of counted pixel is a measure of the line
multiple hypotheses for each sample point which
                                                                   length. Ideally the matching shader count equals the
provides higher stability [Vac04][Wue05].
                                                                   model edge length.

3     THE MATCHING SHADER                                          Using information of the model can help to improve the
                                                                   matching process. From the known pixel length of the
3.1    Shader Outline                                              rendered model edge we can expect a certain length of
The model-based tracking approach uses a 3D model of               the image line response and thus define a threshold for a
the object to be tracked. Model edges can be obtained              minimum pixel count. If the image line found does not
from this model by rendering an image with the last                fulfill this minimum length, it will be rejected as corre-



WSCG 2012 Communication Proceedings                          156                                         http://www.wscg.eu
Vertex shader
void main()
{
    gl_Position = ftransform();
    gl_TexCoord[0] = gl_MultiTexCoord0;
}

Fragment shader
uniform sampler2D cannyImg;
vec3 val;
void main()
{
   val = texture2D(cannyImg, gl_TexCoord[0].st).xyz;
                                                                   Figure 2: Generating edge samples (dotted) for a given
   if(( val != vec3(1.0,1.0,1.0) ))
                                                                   model edge on a 3x3 window.
          gl_FragColor = vec4(0.0,0.0,0.0,1.0);
   else
          discard;
}
              Listing 1: Counting shader.



spondence. In the next section we show further criteria
for evaluation of the matches like depth and distance.

3.2    Sample Edge Generation
While we assume small movements of the camera from
one image to the next, the model edges must be varied
in position and orientation covering translations and ro-          Figure 3: Sampled edges from the model in 2D image
tations of the camera. This is done by sampling several            space. Only some random edges are shown for better
new image edges around the known projected model                   visibility.
edge. For each model edge start and end point in the
image are known. Around these new points are sam-
                                                                   ments of the line features because more lines are sam-
pled with an offset, e.g. on a 3x3 window around the
                                                                   pled at greater distances and with wider angles, but this
central model edge start and end point, 8 new possible
                                                                   leads to a decrease in performance, especially when us-
points are generated for each one. All generated points,
                                                                   ing occlusion queries. From the model knowledge the
including the original ones, are then connected to new
                                                                   information about the depth of the 3D edge can be used
edges, resulting in a total of 81 candidate edges in this
                                                                   to improve the sample edge generation. Movements
case. Figure 2 and 3 show an example for some sample
                                                                   far away from the camera lead to smaller shifting in
edges covering possible translations and rotations of the
                                                                   pixel space while the same movement next to the cam-
model edge. Notice that the sampling is done in 2D im-
                                                                   era is expressed in a large shift in pixel space. Knowing
age space for projected edges. The 3D model data itself
                                                                   the depths of start and end point from the model edge,
remains rigid.
                                                                   we can define different sizes for the sampling windows
For all of these candidate sample edges the matching               for both points, i.e. perspectively dependent genera-
shader returns a number of pixel counted as described              tion of sample edges. If the depths of both points differ
in Section 3.1. The candidate edge returning the high-             more than a threshold, for the nearer point more sample
est pixel count can be regarded as the best fitting match.         points are generated than for the farther point. This re-
Beneath the expected pixel length, it is even possible to          duces the total number of sample edges to be rendered.
consider the distance between the pixel count results of
each candidate as quality criterion for matching. Sim-             3.3   Occlusion Query Management
ilar parallel lines will return almost equal numbers of            Retrieving the pixel count from the matching shader by
pixel count and thus can be recognized as ambiguous                occlusion queries affords a management process that
features. Such results may be rejected for matching.               can handle multiple queries to be executed fast. We
Choosing the offset depends on accuracy and compu-                 have a list of model edges to be rendered and for each
tational speed. A higher offset covers stronger move-              one a separate occlusion query has to be run. But the



WSCG 2012 Communication Proceedings                          157                                         http://www.wscg.eu
graphics hardware limits the number of queries that can             this solves the problem of stalling the CPU while wait-
efficiently return a result in sequence. Trying to retrieve         ing for the occlusion query result. The pixel count of
the counter result immediately after each query has fin-            all sample edges belonging to one model edge can be
ished would stall the CPU [Fer04].                                  retrieved with one texture read-back.
While there are more edges to be rendered than queries              In addition to the canny texture the pixel shader now
can be executed, the task has to be splitted in several             gets the coordinates of projected 2D start and end point
passes. A set of n maximal queries is created. A part of            of the model edge and the offset for sample edge gen-
the model edges can be rendered until the maximum                   eration as input variables. As described below, the
number of n available queries is reached. Then the                  shader calculates new sample points in a window with
results of all n queries have to be retrieved before a              the given offset around the start and end point of the
new block of n queries can be started for the remain-               model edge.
ing edges. The result with the highest count is stored.
Alternatively an ordered list of the results can be cre-
ated for better comparing of the results. The absolute
number of query calls is also counted and the process
finishes, when all edges have been drawn (See listing
2).

create n query objects
generate sample edges
enable shader
load canny texture
disable color and depth buffer write
while( query count != number of edges ){
    for n queries{                                                       Figure 4: Shader target texture organization.
              start query
              render edge                                           Each new start and end point is then connected to a sam-
              end query                                             ple edge. This is done by the shader performing the
              query count++                                         Bresenham line algorithm [Bre65a] between every start
    }                                                               and end point generated. The pixel coordinates result-
    for n queries{                                                  ing from the line calculation are used to search for cor-
              retrieve result                                       responding pixels on the canny-texture. The number of
              if query result > last query                          counted pixels is written as output value on the render-
                        save result                                 target texture. One render-target texture can store all
    }                                                               counter results of the sample edges generated for one
}                                                                   model edge. The texture has the size of all possible
enable color and depth buffer write                                 sample edges, e.g. when 9 sample points are generated
disable shader                                                      in a 3x3 window for every start and endpoint, 81 sam-
                                                                    ple edges are checked and this number of results has to
      Listing 2: Using managed occlusion queries.                   be stored in the texture. Thus the texture must have size
                                                                    9x9 for 81 entries.
                                                                    Figure 4 shows the organization of the texture. For the
3.4    Advanced Texture Read-Back                                   start points A and end points B every column and its u-
The results showed that using a simple shader with oc-              coordinate correspond to one start point in the sample
clusion queries does not perform very well with large               window and every row and its v-coordinate correspond
sets of edges to match (see section 4). Therefore, we               to one end point. Every pixel in the texture is now ad-
developed a more sophisticated shader for matching a                dressed for the result of one sample edge. The pixel
significant amount of edges in short time by extending              shader is aware of the texture coordinate (u,v) it is go-
our first shader approach. It is based on texture read-             ing to write its value to, so it can use this information
back incorporating the sample edge generation. Oppos-               to apply an offset to the start and end point of the input
ing to the occlusion query approach the sample edges                model edge to generate the sample points. Thus, each
are not precomputed on CPU. The generation and com-                 pixel shader call calculates one sample edge depending
putation of the sample edges is entirely transferred to             on its writing position as follows.
graphics hardware. Thus the number of render passes                 Subtracting the offset from the x and y coordinates of
is reduced to the number of model edges, instead of                 the model edge start point A gives us the position of the
rendering each sample edge in its own pass. Further                 first start point A0 with the lowest coordinates in the



WSCG 2012 Communication Proceedings                           158                                         http://www.wscg.eu
sample window. From that point, adding the modulo of              4    RESULTS
the u coordinate and window width s to the x coordi-              We tested our shader-based matching approach by
nate and adding the division of the u coordinate by the           tracking simple and complex objects on indoor and
window width s to the y coordinate results in the new             outdoor scenes (figure 5). As camera input we used
sample start point:                                               video streams at a resolution of 640x480 pixels with
                      s = 2*offset+1                              varying lighting conditions. The canny filter applied to
     sampleStartX = modelStartX - offset + (u % s)                the camera images is taken from the OpenCV [Ocv11]
      sampleStartY = modelStartY - offset + (u / s)               implementation with standard parameters (threshold1
      sampleEndX = modelEndX - offset + (v % s)                   = 50, threshold2 = 200, aperture = 3).
       sampleEndY = modelEndY - offset + (v / s)
                                                                  The initial camera pose is assumed to be roughly known
Doing the same for the end point B and v coordinate               at the start of the sequence, which is a prerequisite for
returns the corresponding end point. Between these the            model-based tracking. This may be done by manually
Bresenham line will be calculated by the shader and the           aligning the model in the camera frame. The intrin-
pixel count is written to the current position.                   sic parameters of the video camera delivering the input
After the shader run the texture is read-back to CPU              stream are gained from previous calibration. Models of
and the maximum value is determined by comparing                  the tracking scenes are available and from these lists of
all pixel values. From the pixel position (u,v) of the            the model edges are built to be used for the matching
maximum and the offset applied, the pixel coordinates             process. Each model edge is projected from the cur-
of the resulting sample edge can now be identified by             rent camera pose and the matching shader returns the
the formulas shown above. It is also possible to use              corresponding image line. For the computation of the
parallel reduction as described in [Fer04a] to directly           new camera pose from the line correspondences we use
obtain the maximum on the texture instead of using the            a non-linear Levenberg-Marquardt optimization.
CPU. Advanced computation of the resulting texture,
like applying a non-maximum suppression leads to fur-
ther quality criteria beneath the length of the matching
line. The counting results of parallel edges are ordered
diagonally on the target texture, which enables a quick
check for this second quality criterion, e.g. given the
case of figure 4 having a maximum at pixel position
(2,2) and one or more significant high counts on one of
the pixels in the diagonal from (0,0) to (8,8) this shows
the existence of at least one parallel line with similar
length and may lead to rejection of this match.
3.5   Optical Flow Support
Strong shaking of the camera may introduce a high
level of motion blur. Due to the limited search area de-
fined by the generated sample edges, the correct match-
ing and subsequently the tracking may be lost when the
image line is shifted too far. To prevent this, strong
movements can be detected by estimation of optical
flow [Bea95]. Calculating optical flow predicts the dis-
placement of pixels in between two frames of an image
sequence. The movement of the distinct start and end
points of each projected model edge between the last
                                                                      Figure 5: Test scenes (video and rendered model).
and the current camera frame can be determined using
a sparse optical flow function from OpenCV [Ocv11]                We compared our shader approach to two other
that accepts an array of feature points as input. The             distance-based matching methods. One method is
predicted new start and end point positions corrected             to parameterize the binary canny image by a Hough
by optical flow are then used for sample edge genera-             transform [Dud72]. The model edge is projected into
tion. Matching with optical flow support allows reduc-            the image plane and a window around this edge defines
ing the distribution of sample edges because the match-           a region of interest where the Hough transform is run.
ing can be performed in a smaller region. Using fewer             The output is a list of straight image lines defined
sample edges leads to faster computation time. Another            by the parameters of line angle to the y-axis and line
method to overcome motion blur is using additional in-            distance to the image origin. These can be directly
ertial sensors [Rei06] to estimate rapid camera motion.           compared to the parameters of the corresponding



WSCG 2012 Communication Proceedings                         159                                         http://www.wscg.eu
model edge. However, matching in two dimensional                   The line projection approach (figure 7) obviously gets
parameter space proves to be very unstable. Neither                distracted by the parallel pipe next to the house cor-
length nor line similarity is judged this way, so we               ner, which is an ambiguous feature. The error ends up
did not further consider this approach. Measuring the              with a maximum displacement of 37 pixels. The match-
distance in image space can be done by projection                  ing with orthogonal search (figure 8) is more precise
of the start and end point of the model edge to the                concerning ambiguities but also gets disturbed by the
straight image line found by the Hough transform.                  motion blur up to an error of 24 pixels. The match-
The problem is the high dependency of the results                  ing shader approach with optical flow support (figure
on the chosen parameters of the transform. Possible                9) overcomes the blur and results in an error of 3 pixel
matches are extremely ambiguous and lead to jittering              displacement.
in the estimated pose. At strong motion some matches
completely fail.
Another popular approach is to set control points along
the model edge and search for strong image gradients
on orthogonal lines through these control points. We
used an implementation from [Vis11] for our tests. The
pose becomes more stable, but movements or interrup-
tions in the image lines corrupt the matching result.
Generally, these distance measures in image space can
lead to stable camera pose estimation when the cam-
era movement is slow and smooth enough, which is the
case in the controlled indoor test scenario (figure 5 top,
middle). However, at fast camera movements inducing
motion blur the matching fails. We will show this on
examples of the outdoor scene (figure 5 bottom).
                                                                       Figure 7: Line projection results after motion.
Our matching shader has proven to deliver good match-
ing results with minimal error even in the worst scenario
of a video captured with a strongly shaking hand cam-
era. The following figures illustrate the results of three
matching approaches on a test sequence after the oc-
currence of strong motion. The motion blur occurs for
duration of 6 frames while the image content is shifted
over 80 pixels in this time. We regard the matching er-
ror in the frame right after this strong motion. Figure
6 shows the shift of the image within the 6 frames and
the sub-picture the moment of strongest motion blur in
the video sequence which disturbs the canny image to
a large extent. Image lines are only partly visible and
hard to handle by the matching methods.

                                                                      Figure 8: Orthogonal search results after motion.
                                                                   In figure 10 the moment of strongest motion blur in the
                                                                   outdoor scene can be seen together with the resulting
                                                                   pose estimation overlay from the matches proposed by
                                                                   our method. Concerning the parameters, in our tests we
                                                                   found a good window size for the generation of sample
                                                                   edges at 4x4 with supporting optical flow. Without op-
                                                                   tical flow the best trade-off between computation time
                                                                   and matching quality could be reached with generating
                                                                   sample edges at 7x7 windows. The best threshold for
                                                                   rejection of the image line length as match is 34 of the
                                                                   model line length.
                                                                   Table 1 lists the average computation time in millisec-
    Figure 6: Image shift with strong motion blur.                 onds of the components for our matching approach on



WSCG 2012 Communication Proceedings                          160                                        http://www.wscg.eu
                                                                        Canny filter        5 ms
                                                                        Optical flow        4 ms
                                                                        Occlusion query     97 ms (11), 54(7), 7(2)
                                                                        Texture read-back 18 ms (11), 14,(7), 7(2)
                                                                                 Table 1: Computation time.

                                                                   ously using a query executes fast only when a very little
                                                                   number of edges is used. But enlarging the number of
                                                                   model edges the texture approach quickly outperforms
                                                                   the query usage. Overall, real-time capability for track-
                                                                   ing is ensured, however the implementation is not yet
                                                                   optimized.


    Figure 9: Matching shader results after motion.
                                                                   5   CONCLUSION
                                                                   We presented a shader approach for matching corre-
                                                                   sponding image lines to model edges in a model-based
                                                                   tracking scenario. The knowledge about the model is
                                                                   used to improve the matching and to define criteria for
                                                                   match selection. For a given model edge based on the
                                                                   last pose several sample edges are generated and ren-
                                                                   dered with a matching shader. The shader counts un-
                                                                   derlying pixels of a canny-filtered camera input image
                                                                   at the position of the edges. The image line with highest
                                                                   accordance to criteria of length and distance is chosen
                                                                   as match. This procedure delivers good matching and
                                                                   results in a correct camera pose estimation even at oc-
                                                                   currence of strong motion blur.
                                                                   We compared two methods to realize the matching
                                                                   shader. Using a simple counting shader and occlusion
                                                                   queries to retrieve the pixel count result is straightfor-
                                                                   ward but significantly lowers the frame rate when many
                                                                   sample edges are generated because each render pass.
                                                                   The more sophisticated way is to read-back a texture
                                                                   value which can be done quite fast. The whole process
                                                                   of sample edge generation can be transferred into the
                                                                   shader, so a render pass is only called once per model
                                                                   edge instead for each sample edge.
                                                                   Although we used straight lines from our testing mod-
                                                                   els, the work is not limited to this type of feature. The
                                                                   counting shader can be extended to run on other render-
                                                                   able structures as well, like circles, curves or NURBS.
Figure 10: Correct camera pose computation at strong               For this purpose the sample generation algorithm has
motion blur (top) and for other scenes.                            to be adapted to the wanted structure to match. A fur-
                                                                   ther advancement could be the integration of gradient
a Intel Core2Duo 3.2GHz with nVidia GeForce GTX                    calculation into the shader. This would save the canny
285. The canny filter and optical flow calculation step            preprocessing step to the camera image. Additionally,
are called only once for a new frame, independently of             when calculating the gradient, the gradient orientation
the number of edges to match. Although time consump-               is also known. This could be used by the matching
tion for the canny filter is not too high, this additional         shader to count those pixels only, which have the same
step can be reduced by implementing gradient search                gradient direction and thus belong to the same line.
inside the shader instead.
Next, times for the occlusion query approach and the               6   ACKNOWLEDGMENTS
texture read-back variant are compared. The number                 This work was supported by grant no. MU 2783/3-1 of
in brackets names the amount of edges to render. Obvi-             the German Research Foundation (DFG). We also want



WSCG 2012 Communication Proceedings                          161                                         http://www.wscg.eu
to thank our former colleague Niklas Henrich for his                 Vision Conference (BMVC’06), 3, pp. 1119-
support on GPU programming and Carsten Neumann                       1128, 2006.
from University of Louisiana at Lafayette for technical         [Kle07] G. Klein and D. Murray. Parallel Tracking and
discussions.                                                         Mapping for Small AR Workspaces. ACM/IEEE
                                                                     Int. Symp. on Mixed and Augmented Reality, pp.
7   REFERENCES
                                                                     225-234, Nara, Japan, 2007.
[Bay08] H. Bay, A. Ess, T. Tuytelaars and L. Van Gool.
                                                                [Low91] D.G. Lowe. Fitting Parameterized Three-
     SURF: Speeded Up Robust Features. Computer
                                                                     Dimensional Models to Images. IEEE Transac-
     Vision and Image Understanding (CVIU), 110(3),
                                                                     tions on Pattern Analysis and Machine Intelli-
     pp. 346–359, 2008.
                                                                     gence, 13(5), 1991.
[Bea95] S.S. Beauchemin and J. L. Barron. The com-
                                                                [Low92] D.G. Lowe. Robust model-based motion
     putation of optical flow. ACM Computing Sur-
                                                                     tracking through the integration of search and
     veys, 27(3), pp. 433-466, 1995.
                                                                     estimation. International Journal of Computer Vi-
[Bou89] P. Bouthemy. A Maximum Likelihood Frame-                     sion, 8(2), pp. 113-122, 1992.
     work for Determining Moving Edges. IEEE
                                                                [Low99] D.G. Lowe. Object Recognition From Local
     Transactions on Pattern Analysis and Machine
                                                                     Scale-Invariant Features. International Confer-
     Intelligence, 11, pp. 499-511, 1989.
                                                                     ence on Computer Vision, pp. 1150-1157, Corfu,
[Bra11] A.K. Braun and S. Mueller. GPU-assisted 3D                   Greece, 1999.
     Pose Estimation Under Realistic Illumination.
                                                                [Ocv11] OpenCV library, version 2.3.1, taken from
     18th WSCG International Conference on Com-
                                                                     http://opencv.willowgarage.com/wiki/
     puter Graphics, Visualization and Computer Vi-
     sion, Plzen, Czech Republic, 2011.                         [Rei06] G. Reitmayr and T.W. Drummond. Going out:
                                                                     Robust Tracking for Outdoor Augmented Real-
[Bre65] J.E. Bresenham. Algorithm for Computer
                                                                     ity. International Symposium on Mixed and Aug-
     Control of a Digital Plotter. IBM Systems Journal,
                                                                     mented Reality (ISMAR06), pp. 109-118, 2006
     4(1) , pp. 25-30, 1965.
                                                                [Sch09] M. Schumann, S. Achilles and S. Mueller.
[Bro12] J.A. Brown and D.W. Capson. A Framework
                                                                     Analysis by Synthesis Techniques for Marker-
     for 3D Model-Based Visual Tracking Using a
                                                                     less Tracking. Virtuelle und Erweiterte Realität,
     GPU-Accelerated Particle Filter. IEEE Transac-
                                                                     6. Workshop der GI Fachgruppe VR/AR, Braun-
     tions on Visualization and Computer Graphics,
                                                                     schweig, Germany, 2009.
     18, pp. 68-80, 2012.
                                                                [Shi94] J. Shi and C. Tomasi. Good Features to Track.
[Can86] J. Canny. A Computational Approach To Edge
                                                                     IEEE Conference on Computer Vision and Pattern
     Detection. IEEE Trans. Pattern Analysis and Ma-
                                                                     Recognition, pp. 593-600, 1994.
     chine Intelligence, 8(6), pp. 679-698, 1986.
                                                                [Sin06] S. N. Sinha, J.-M. Frahm, M. Pollefeys, and
[Com03] A.I. Comport, E. Marchand and F.
                                                                     Y. Genc. GPUbased video feature tracking and
     Chaumette. A Real-Time Tracker for Markerless
                                                                     matching. Workshop on Edge Computing Using
     Augmented Reality. ACM/IEEE Int. Symp. on
                                                                     New Commodity Architectures (EDGE 2006),
     Mixed and Augmented Reality, pp36-45, Tokyo,
                                                                     2006.
     Japan, 2003.
                                                                [Vac04] L. Vacchetti, V. Lepetit and P. Fua. Combin-
[Dru02] T. Drummond and R. Cipolla. Real-time vi-
                                                                     ing Edge and Texture Information for Real-Time
     sual tracking of complex structures. IEEE Trans-
                                                                     Accurate 3D Camera Tracking. ACM/IEEE Int.
     actions on Pattern Analysis and Machine Intelli-
                                                                     Symp. on Mixed and Augmented Reality, pp. 48-
     gence, 24(7), pp. 932-946, 2002.
                                                                     57, Arlington, USA, 2004.
[Dud72] R.O. Duda and P.E. Hart. Use of the Hough
                                                                [Vis11] ViSP Visual Servoing Platform
     Transformation to Detect Lines and Curves in
                                                                     library, version 2.6.1, taken from
     Pictures. Communications of the ACM, 15(1), pp.
                                                                     http://www.irisa.fr/lagadic/visp/visp.html
     11-15, 1972.
                                                                [Wue05] H. Wuest, F. Vial and D. Stricker. Adaptive
[Fer04] R. Fernando. GPU Gems. Programming Tech-
                                                                     Line Tracking with Multiple Hypotheses for Aug-
     niques, Tips, and Tricks for Real-Time Graphics.
                                                                     mented Reality. ACM/IEEE Int. Symp. on Mixed
     Addison-Wesley, Longman, Amsterdam, 2004
                                                                     and Augmented Reality, pp. 62-69, Santa Barbara,
[Har90] C. Harris and C. Stennet. RAPID - A Video                    USA, 2005.
     Rate Object Tracker. In Proc. British Machine Vi-
                                                                [Wue07] H. Wuest and D. Stricker. Tracking of Indus-
     sion Conference, pp. 73-77, Oxford, UK , 1990.
                                                                     trial Objects by Using CAD Models. Journal of
[Kle06] G. Klein and D. Murray. Full-3D Edge Track-                  Virtual Reality and Broadcasting, 4(1), 2007.
     ing with a Particle Filter. Proc. British Machine



WSCG 2012 Communication Proceedings                       162                                       http://www.wscg.eu
      Active Segmentation in 3D using Kinect Sensor
            Zoltan Tomori                          Radoslav Gargalik                               Igor Hrmo
  Inst. of Experimental Physics                Inst. of Computer Science                Inst. of Experimental Physics
  Slovak Academy of Sciences                     P.J. Safarik University                Slovak Academy of Sciences
           Watsonova 47                                 Jesenna 5,                               Watsonova 47
     040 01 Kosice, Slovakia                    040 01 Kosice, Slovakia                   040 01 Kosice, Slovakia
          tomori@saske.sk                    radoslavgargalik@gmail.com                        hrmo@saske.sk


                                                      ABSTRACT
The combination of color image and depth map significantly improves the segmentation. The Kinect sensor with
pan/tilt motorized movement captures both images and segments them separately by the Grab Cut method. The
resulting contours are converted to polar coordinates. After the floor plane detection, corresponding "depth" and
"color" contours are combined such that the importance of depth /color information is proportional to the
distance from the floor. The segmentation is followed by the extraction of simple scale invariant features like
color components and height/width ratio. Subsequently, features are used to train Normal Bayes Classifier. The
algorithm was tested on a set of simple objects (mugs) on the table.
Keywords
Active segmentation, Kinect, Depth map, RGBD image
1. INTRODUCTION                                                      alignment they can be considered as one RGBD
Segmentation in 3D is a critical problem in many                     image.
areas of computer vision. The information about the                  The concept of active segmentation was inspired by
depth can be obtained by various methods like stereo                 the biological visual system where the object of
vision, moving camera or object, defocusing,                         interest is segmented with high resolution by the
structured light or comparison with known                            fovea, while the rest of the scene is captured in lower
geometrical model [Mir04]. Kinect - a low-cost 3D                    resolution on the periphery of retina [Mis09].
sensor for gaming console was launched in                            Another important concept is the "contact boundary"
November 2010 and achieved big commercial                            introduced in [Mis11] where the boundary pixels
success. Support for programmers appeared shortly                    touching the surface (floor, desk) are distinguished
after it (Microsoft Kinect SDK, OpenNI,                              from the remaining ones. Contact boundary is
OpenKinect, Freenect etc.). A more comprehensive                     important in segmentation based on the combination
source of information about Kinect hardware and                      of color image and depth map, both provided by
programming was published only recently - e.g.                       Kinect.
[Web12].
                                                                     2. ACQUISITION
Kinect provides 3D information which can be easily
retrieved (color, depth, points cloud in real distance               Mechanical Construction
units). Its depth sensor consists of infrared                        For testing purposes, we constructed a motorized
transmitter/camera system. The transmitter projects                  equipment and we named it KATE (Kinect Active
small dots (speckles) on the surrounding scene and                   Tracking Equipment) - see Figure 1a). It consists of
the IR camera acquires image and compares their                      the Kinect sensor attached to the turntable driven by
position with the reference one. The depth is then                   a precise stepper motor (Intelligent Motion Systems,
calculated from the displacement of the individual                   USA). This allows horizontal movement around its
speckles. The color + depth images are acquired                      vertical axis (pan) with the resolution 51200 steps per
approximately at the same moment and after their                     360 degrees. For the vertical movement (tilt) we
                                                                     exploited the built-in Kinect stepper motor controlled
 Permission to make digital or hard copies of all or part of
                                                                     via the same USB port as cameras. This solution is
 this work for personal or classroom use is granted without
 fee provided that copies are not made or distributed for            simple but it has a poor resolution in range <-31, 31>
 profit or commercial advantage and that copies bear this            degrees. Another problem is that the tilt value is
 notice and the full citation on the first page. To copy             related to the absolute horizontal position measured
 otherwise, or republish, to post on servers or to                   automatically by Kinect accelerometer. However,
 redistribute to lists, requires prior specific permission           this solution is sufficient for testing purposes.
 and/or a fee.




WSCG 2012 Communication Proceedings                            163                                       http://www.wscg.eu
                                                                  Figure 2). Face detector is based on the Haar cascade
                                                                  classifier included in the OpenCV library. If a face is
                                                                  recognized (in the predefined depth range) then the
                                                                  displacement between the face rectangle and the
                                                                  center of the image is calculated. Pan/tilt motors
                                                                  correct the Kinect position trying to keep the face
                                                                  rectangle in the image center.




           a)                           b)
  Figure 1. a) KATE (Kinect Active Tracking
  Equipment) b) Object as seen by Kinect.
  Central rectangle represents the region of
  interest   (fovea)   where    the    active
  segmentation is performed.


Calibration and tracking
Camera calibration based on the pinhole camera
                                                                   Figure 2. Face tracking. Kinect detects face by
model is necessary in applications like objects
                                                                   using Haar classifier included in OpenCV
reconstruction from multiple views. As Kinect
                                                                   library. In the next step, pan/tilt motors adjust
consists of 2 cameras (RGB and IR), more
                                                                   Kinect position such that the face rectangle is in
complicated model is required [Paj11]. However,
                                                                   the center of image.
initial calibration included in OpenNI library is
sufficient for segmentation and tracking purposes.
The depth values correlate well with real values, the
                                                                  Preprocessing
correspondence between the depth (Z) and XY
                                                                  The depth image contains a lot of artifacts resulting
dimensions can be easily corrected by scale
                                                                  from the depth measurement principle. Shadows-like
constants.
                                                                  defects appear in places visible from the depth
We developed a supervised calibration function                    camera but not illuminated by IR projector.
adjusting the scale in both horizontal and vertical
                                                                  We exploited the "Inpaint" method described in
directions. KATE watches the given scene displaying
                                                                  [Tel04], which recovers the missing depth
live image along with the cross in the image center
                                                                  information. Implementation of this method is easy
pc(x,y) - see Figure 1b). The operator clicks a position
                                                                  as it is included in OpenCV library.
p1(x,y) representing the desired future position of the
image center. From pc and p1 values in pixels we can              3. PROCESSING
find corresponding 3D points Pc=(0, 0, Z0) and                    Processing consists of the following sequence of
P1=(X1, Y1, Z1) in real units (meters). Using simple              steps: plane detection, finding volume of interest and
trigonometry we obtain pan angle as arcsin(X1/Z1)                 segmentation.
and tilt as arcsin(Y1/Z1).
Calculated angles are converted to the stepper motor
                                                                  Floor Plane Detection and Region of
units and both motors move the given number of                    Interest (ROI)
steps. In ideal case, the new central position pc(x,y)            One of the basic operations in computer vision is the
should show the same place of the scene as the                    detection of the plane where the segmented objects
p1(x,y) before the movement. If it is not the case, the           are standing on (floor, table top). The detection is
scale factor can be adjusted. The calibration function            based on the popular RANSAC algorithm which
allows manual control of the motors. The number of                finds the plane representing the input points cloud.
correcting steps is recalculated to the new scale                 The plane is determined by the equation
factors for both pan and tilt values.                                             ax  by  cx  d  0                (1)
We created a face tracking system to test KATE                    where [a,b,c] is the normal vector and d is the
[Sen12]. The new position of motors is not given by               distance from the origin.
the click as described in the previous section, but it is
determined by the face bounding rectangle (see



WSCG 2012 Communication Proceedings                         164                                       http://www.wscg.eu
Although RANSAC eliminates the influence of                         3) Side limits of our volume of interest are
outliers in principle, it is possible to improve the                   created by planes perpendicular to the floor
plane detection by filtering the points that are                       crossing the polygon sides.
definitely outliers. There are a lot of RANSAC                      4) Project voxels lying between the top and the
modifications performing this task [Chu03]. It is                      bottom limits. If there are points falling
possible to iterate plane detection and calculation of                 inside the polygon and connected with its
the volume of interest (described in the next section).                sides, algorithm returns to step 2.
Data for the next plane detection create only voxels
from the volume of interest.                                    Segmentation by Grab Cut
                                                                The algorithm exploited in our experiments is based
                                                                on the idea of active segmentation briefly explained
                                                                in the introduction. From a lot of possible
                                                                segmentation methods we focused on these which are
                                                                based on the classification of pixels inside a region of
                                                                interest (ROI) and the minimization of an energy
                                                                function. We assume that the segmented object is
                                                                placed inside the square frame which is our initial
                                                                region of interest.
                                                                GrabCut [Rot04] is a very popular segmentation
                                                                algorithm based on the Gaussian Mixture Model and
          a)                          b)                        energy minimization. All pixels outside the ROI are
 Figure 3. Floor plane detection. a) RANSAC                     labeled as background ones, pixels inside ROI are
 detects not only table-top voxels but also                     labeled according to the energy function consisting of
 unwanted strips of voxels on the walls b)                      regional and boundary terms. Briefly speaking,
 Region of interest represented by the top of the               regional term reflects the likelihood that a given label
 table (green) outlined by polygon (red).                       is appropriate for the given pixel and the boundary
                                                                term reflects how easily the label can expand to its
                                                                neighborhood. Assigning a boundary label to a pixel
Figure 3a) shows the detected plane (green pixels).             inside a homogeneous region is penalized. The
As can be seen there, plane consists of the table top           strength of N-link (the link between pixels m and n)
area as well as some undesirable green strips on the            is calculated
walls. We filtered them as follows:
        Erosion filter isolates table top from the
         connected strips (like this on the top right
                                                                          N (m, n) 
                                                                                          
                                                                                       d (m, n)
                                                                                                       
                                                                                                  exp   Z m  Z n
                                                                                                                        2
                                                                                                                               (2)
         corner of table).
        Table top contour is found as the largest
         contour which includes center of image                                                    1
                                                                                                                              (3)
                                                                                          2 Zm  Zn
                                                                                                           2
         (fixation point).
        Convex hull function eliminates local
         discontinuities and gives the polygon
         outlining the table top (red in Figure 3b).            where Zm is the color of the pixel m, constant γ has
                                                                recommended value = 50, d(m,n) is the unit distance
Finding the Volume of Interest (VOI)                            (1 for horizontal and vertical neighbors and sqrt(2)
The majority of applications focus on a limited space           for diagonal ones). Value of β is the average inverse
above the floor or the table-top. The bottom of this            difference value calculated in advance.
space is represented by the ROI detected in the
previous step. Its top is given by the maximal                  Combined Segmentation
expected height of our objects.                                 Kinect generates two images reflecting the same
Limits from sides (walls, furniture) can be found by            scene - color image and the depth map. Figure 4
an algorithm based on a simplified model of the                 shows the problem of color image segmentation if
scene assuming that side limits are higher than                 the other object with similar color is behind our
segmented objects.                                              object of interest. On the other hand, the depth map
                                                                segmentation has troubles with parts near the floor
    1) Project all voxels higher than the top limit to          where the depth of object and the background are
       the floor plane.                                         almost the same.
    2) Find the polygon with maximal area that
       does not include projected points. The
       number of vertices is given in advance.



WSCG 2012 Communication Proceedings                       165                                              http://www.wscg.eu
                                                                 blue curve in Figure 5c corresponds to the contour
                                                                 from the color image and the green one to the depth
                     a)                         b)
                                                                 image, the resulted red curve is their combination.
                                                                 After its conversion back to the Cartesian coordinates
                                                                 we can obtain a contour shown in Figure 5d.
                                                                 Several alternatives exist how to change the
                                                                 color/depth influence, depending on the type and
                                                                 configuration of the objects on the scene.
                                                                 a) Piecewise combination of segmented curves is the
                                                                    simplest method. The depth image contour
                                                                    representing the contact boundary is replaced by
                     c)                         d)
                                                                    the corresponding part of the color image contour
                                                                    in a pre-selected interval. For instance, an interval
                                                                    <45,135> degrees represents the bottom of an
                                                                    object where a low contrast of depth image is
                                                                    expected. Selection of the end points of the
                                                                    interval should respect the continuity of resulted
                                                                    curve. Good candidates are intersections of the
                                                                    both curves.
                                                                 b) Derivation of k on the distance from the plane
 Figure 4. a) Overlapping objects with shadow                       using normal vector (1). As Kinect gives (x,y,z)
 on the right side. b) Segmentation by the                          values in the camera coordinate system, the
 GrabCut method exploiting only the color                           calculation of the distance from the plane is
 image. c) Floor plane detection (green pixels)                     straightforward.
 d) Segmentation of depth image masked by the
 floor plane.
                                                                                                                      d)
Above mentioned problems can be solved combining
information from both - the depth map and color
image. Several authors [Kar10], [Mut10] combine
color and depth information. In [Sil11] a model was
proposed which modifies computation of N-link as                                                                      c)
follows:

    N (m, n)  kNC (m, n)  (1  k ) N D (m, n)      (4)

where NC and ND are N-links from color image and
depth image respectively and k controls the influence
of both links to the final N-link calculation (e.g. 80%
color and 20% depth). Modified N-link calculation is
incorporated into the energy function optimized by
graph cut method.                                                    Figure 5. Segmentation by combined GrabCut
                                                                     method. a) color image b) depth map c)
Distance Dependent Segmentation                                      contours in polar coordinates d) result of
Our approach is based on the assumption that the
                                                                     segmentation
value of k from (4) is not constant but depends on the
distance from the floor. In standard situations, the
object standing on the floor is captured by Kinect               Classification of segmented objects
under tilt angle. The top part of the object has a much          The successful segmentation is often the crucial step
higher contrast of the depth image than its bottom               in computer vision (e.g. in objects recognition based
(Figure 5b). We combine the results of contours                  on machine learning principles). We tested our
segmented from both color and depth images.                      system to recognize several simple objects (like mugs
The initial step is a conversion of both contours from           on the table). We exploited supervised learning based
Cartesian to polar coordinates using the center of the           on the Normal Bayes Classifier. System segmented
image as the origin. The x-axis corresponds to the               each object on the table and found the bounding
angle in the clockwise direction starting from the               rectangle of its contour as well as the average color.
position "3" on the clock, y-axis is magnitude. The              Feature vector consisting of 4 components (R, G, B
                                                                 color components and height/width ratio of bounding



WSCG 2012 Communication Proceedings                        166                                        http://www.wscg.eu
rectangle) was used in the supervised learning stage             5. ACKNOWLEDGMENTS
taking cca 5 seconds. After the learning, the system             This work was supported by the Slovak research
was able to recognize object and display its label               grant agencies APVV (Project No. 0526-11) and
(Figure 6).                                                      VEGA (Project No. 2/0191/11).
                                                                 6. REFERENCES
                                                                 [Chu03] Chum, O. Matas, J. and Kittler, J. Locally
                                                                   Optimized RANSAC, Lecture Notes in Computer
                                                                   Sciences, 2781, pp. 236-243, 2003.
                                                                 [Kar10] Karthikeyan, V. Anil, A. and Ebroul, I.
                                                                    GrabcutD: improved grabcut using depth
                                                                    information. Proc. ACM workshop on Surreal
                                                                    media and virtual cloning, Firenze, Italy, pp. 57-
                                                                    62 , 2010.
                                                                 [Mir04] Mirzabaki, M. Depth Detection Through
                                                                    Interpolation Functions: A New Method. Proc.
                                                                    WSCG, Plzen, Czech Rep., pp. 105-108, 2004.
                                                                 [Mis11] Mishra, A. and Aloimonos, Y. Visual
                                                                   Segmentation of "Simple" Objects for Robots.
 Figure 6. Classification of segmented objects                     Proc. Robotics Science and Systems conference
 using Normal Bayes Classifier based on the                        (RSS), Los Angeles, June 27 - July 1, 2011.
 feature vector created by RGB color                               http://www.umiacs.umd.edu/~mishraka
 components and height/width ratio.                              [Mis12] Mishra, A. K. Aloimonos, Y. Cheong, L. F.
                                                                   and Kassim, A. A. Active Visual Segmentation.
4. SUMMARY                                                         IEEE Transactions on Pattern Analysis and
Motorized pan/tilt Kinect system was constructed for               Machine Intelligence, 34, pp. 639-653, 2012.
the acquisition of color and depth images. This                  [Mut10] Mutto, C. D. Zanuttigh, P. and Cortelazzo,
system tracks the object of interest keeping it in the             G. M. Scene Segmentation by Color and Depth
center of image (tested with face tracking based on                Information and its Applications. Proc.
the Haar cascade classifier included in OpenCV                     Streaming Day, Udine, 2010.
library).                                                        [Paj11] Pajdla, T. Smisek, J. and Jancosek, M. 3D
We applied Grab Cut method to segment both color                    with Kinect. Proc. of the 1st IEEE Workshop on
and depth images using the image center as the                      Consumer Depth Cameras for Computer Vision,
fixation point. We transformed contours into the                    Barcelona, Spain, pp. 1154-1160, 2011.
polar coordinates and combined them. The weights                 [Rot04] Rother, C. Kolmogorov, V. and Blake, A.
controlling the importance of color/depth edges was                 "GrabCut" - Interactive foreground extraction
dependent on the distance from the floor detected by                using iterated graph cuts. ACM Transactions on
RANSAC method. This approach significantly                          Graphics, 23, pp. 309-314, 2004.
improved segmentation near the floor as well as in
partially overlapping objects. Segmented contours                [Sen12] Senaj, M. OpenCV library in computer
were used for the features extraction (R, G, B color                vision applications in robotics, Master's thesis,
components and height/width ratio). We used this                    FEI, Technical University of Kosice, 2012.
features vectors for supervised training of Normal               [Sil11] Silberman N. and Fergus, R. Indoor Scene
Bayes Classifier and for the classification of simple                Segmentation using a Structured Light
objects like mugs on the table.                                      Sensor.Proc. Int. Conf. on Computer Vision -
Future work                                                          Workshop      on    3D    Representation and
                                                                     Recognition, Barcelona, 2011.
This communication paper reflects our experiments
with Kinect as the initial stage of the project oriented         [Tel04] Telea, A. An image inpainting technique
to application of Natural User Interface. We plan to                based on the fast marching method. Journal of
exploit RGBD images for wider group of problems                     Graphics Tools, 9, pp. 23-34, 2004.
like active segmentation, tracking and control of                [Web12] Webb, J. and Ashley, J. Beginning Kinect
specific devices. Growing number of papers                         Programming with the Microsoft Kinect SDK.
combining color + depth along with the progress in                 Apress, (ISBN 978-1-4302-4104-1), 2012.
sensors hardware make this research area very
promising.




WSCG 2012 Communication Proceedings                        167                                      http://www.wscg.eu
WSCG 2012 Communication Proceedings   168   http://www.wscg.eu
    Using Game Engine Technology for Virtual Environment
                    Teamwork Training
                 Stefan Marks                                John Windsor                           Burkhard Wünsche
            Auckland University of                    The University of Auckland,                The University of Auckland,
          Technology, New Zealand                            New Zealand                               New Zealand
         stefan.marks.ac@gmail.com                    j.windsor@auckland.ac.nz                  b.wuensche@auckland.ac.nz


                                                         ABSTRACT
The use of virtual environments (VE) for teaching and training is increasing rapidly. A particular popular medium
for implementing such applications are game engines. However, just changing game content is usually insufficient
for creating effective training and teaching scenarios. In this paper, we discuss how the design of a VE can be
changed to adapt it to new use cases. We explain how new interaction principles can be added to a game engine
by presenting technologies for integrating a webcam for head tracking. This enables head-coupled perspective as
an intuitive view control and head gestures that are mapped onto the user’s avatar in the virtual environment. We
also explain how the simulation can be connected to behavioural study software in order to simplify user study
evaluation. Finally we list problems and solutions when utilising the free Source Engine Software Development
Kit to design such a virtual environment. We evaluate our design, present a virtual surgery teamwork training
scenario created with it, and summarize user study results demonstrating the usefulness of our extensions.
Keywords: Serious Game, Source Engine, Medical Teamwork Training, Head Tracking, Non-Verbal Communi-
cation, Head-Coupled Perspective

1    INTRODUCTION                                                             not easily be extended with additional functionality
                                                                              required for a specific simulation scenario.
In recent years, virtual environments (VEs) have                          2. To build a VE from scratch. This enables complete
become increasingly popular due to technological                              freedom in the design and usability of the VE, but
advances in graphics and user interfaces [MSL+ 09].                           significantly extends development time.
One of the many valuable uses of VEs is teamwork                          3. To use a simulation framework that can be flexibly
training. The members of a team can be located                                extended to account for special design requirements,
wherever it is most convenient for them (e.g., at home)                       but already provides a solid foundation of function-
and solve a simulated task in the VE collaboratively,                         ality to achieve a quick working prototype.
without physically having to travel to a common                           Whereas the first two options are located at the oppo-
simulation facility. Medical schools have realised                        site extremes of the spectrum, the last option is located
this advantage and, for example, created numerous                         between these extremes in terms of development flex-
medical simulations within Second Life or similar VEs                     ibility and rapid prototyping. A game engine, the un-
[DPH+ 09].                                                                derlying component of computer games, can be used as
To implement a VE, the developer has to choose be-                        such a framework. This is the principle behind “seri-
tween three possibilities:                                                ous games”: To use the technology of computer games,
                                                                          e.g., graphics, sound, physical simulation, multi-user
1. To use a completely implemented commercial or
                                                                          support, but to replace and adapt the original content to
   free VE solution like Second Life [Lin10]. This has
                                                                          build “serious” applications, e.g., for education, train-
   the advantage of being able to completely focus on
                                                                          ing, or simulation.
   content creation instead of having to deal with tech-
   nical implementation questions and problems. How-                      The literature provides several examples of studies with
   ever, the disadvantage is that these frameworks can-                   simulation environments based on game engines, e.g.,
                                                                          [TSHW08], [MRL+ 06], [ST09]. For an extended re-
                                                                          view of serious games, see [SJB07].
                                                                          However, rarely does the reader find information dis-
Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without
                                                                          cussing design options, the advantages and disadvan-
fee provided that copies are not made or distributed for profit           tages of tools such as game engines, and how to use
or commercial advantage and that copies bear this notice and              them effectively and integrate new functionalities. This
the full citation on the first page. To copy otherwise, or re-            makes it difficult for researchers to extend existing sim-
publish, to post on servers or to redistribute to lists, requires         ulations or create new ones. One example of the few
prior specific permission and/or a fee.                                   exceptions is the publication of Ritchie, Lindstrom, and



WSCG 2012 Communication Proceedings                                 169                                         http://www.wscg.eu
                                     User Monitor (Xpressor)           Client Computer                 Server
                                     - Face detection                           Data Model
                                     - Head tracking
                                     - Eye tracking                        - Head orientation
                                                                           - Gaze direction
                                     - Expression recognition
                                                                           - Facial expression
                                                                                                             Network
                                     - ...                                                         VE Engine Server
                                                                           - ...


                                                                                                  Game Engine Server
                                         VE Engine Client             Plug-In

                                                                                                   Training Scenario
                                        Game Engine Client




                               Figure 1: The functional blocks of the simulation framework


Duggan, where not only the used tools for the develop-                2    DESIGN
ment process but also source code details are provided
[RLD06].                                                              The goal of the research described in this paper is to
                                                                      utilise a game engine to implement a virtual environ-
We have created a VE for medical teamwork training                    ment for surgical teamwork training. An important
which provides additional control mechanisms by using                 component in teamwork is non-verbal communication,
a webcam to capture the head movement of the user.                    such as head gestures, which we capture with a web-
This head movement is decomposed into the transla-                    cam and then map onto avatars in the virtual environ-
tional part which is used for head-coupled perspective                ment. In addition we need an intuitive method for view
(HCP), and the rotational part which is used to control               control, since most surgical procedures require the sur-
head gestures of the user’s avatar to convey non-verbal               geon to use both hands for instruments. We therefore
communication cues. The results of our user studies                   decided to implement HCP using webcam input. HCP
show that HCP improves the usability of the VE as it                  changes the view of the VE based on the movements of
introduces an intuitive view control metaphor that even               the user’s head position in front of the monitor. The im-
inexperienced users were able to master within seconds.               plementation of these additional hardware components
In addition, tracking-based head gesture control of the               and control metaphors is also part of this paper.
avatar improved the perceived realism of the simulation               Realising our simulation scenario requires changing the
[MWW11].                                                              game content, gameplay, and integration of webcam in-
                                                                      put to control game engine parameters. Figure 1 gives
This paper provides insight into the design of game                   an overview of the architecture of the resulting system.
engines and their modification for advanced “serious                  The sections marked in red were developed, extended,
games” applications. In particular we explain how new                 or modified for the implementation. Figure 2 shows a
user interface devices can be integrated. In our discus-              screenshot of the final VE for teamwork training simu-
sions, we use the Source Engine [Val07] as an example,                lations.
which, at the time of the survey, fulfilled most of our
                                                                      The simulation is run on a central server that all users
simulation requirements: good graphical and animation
capabilities, availability of an Software Development                 connect to with their client computers. The server as
Kit (SDK) and developer tools, and reliable synchroni-                well as the clients run their part of the VE engine, be-
                                                                      ing constructed on top of the Source Engine. It is im-
sation of physically simulated objects among multiple
                                                                      portant to distinguish between the terms “game engine”
clients. The details of the selection process of a suitable
                                                                      referring to components of the simulation framework
engine can be found in [MWW07].
                                                                      that are parts on the Source Engine itself and therefore
                                                                      cannot be altered, and “VE engine” referring to compo-
Section 2 presents the design of our VE framework. In
                                                                      nents that are based on the Source SDK and have been
Section 3, we describe the details of the implementa-
                                                                      altered to create a VE with new features and interac-
tion. A summary of the results of our user studies con-
                                                                      tions.
ducted with the framework are then presented and dis-
cussed in Section 4, and we finish with the conclusion                The original game content on the server is replaced by
in Section 5.                                                         the teamwork training scenario. This includes virtual



WSCG 2012 Communication Proceedings                             170                                         http://www.wscg.eu
                                                                      3     IMPLEMENTATION
                                                                      In the following three sections, we will explain the im-
                                                                      plementation of the three important components of this
                                                                      framework: the virtual environment, the user monitor
                                                                      Xpressor, and the data model.

                                                                      3.1     Virtual Environment
                                                                      3.1.1    Steam Client
                                                                      The modification of a game that utilises the Source
                                                                      Engine starts off with Steam, a client software of the
                                                                      manufacturer Valve, designed to enable the user to buy
                                                                      games online, download and install them, and to keep
Figure 2: Screenshot of the final surgical teamwork simulator         the games updated when bugfixes or extras are released.
MedVE created from the original deathmatch game code
                                                                      3.1.2    Creating a New Project
                                                                      The Source SDK is available to anybody who has pur-
rooms, objects, instruments, sounds, textures, 3D mod-                chased at least one game that utilises the Source En-
els, etc.                                                             gine, e.g., Half-Life 2, Team Fortress 2, Portal. With the
                                                                      help of the Steam client, the SDK is easily downloaded
On each client, an additional program, called Xpressor,               and installed like any other game. The SDK gives a
is running, using the input from the webcam for track-                good starting point to experiment with the code and the
ing the user’s head and face. The tracking information                game engine, and to start modifying certain aspects of
is sent in the form of a specific data model (see Sec-                the game. Several pages on the Valve Developer Com-
tion 3.3) to a plug-in of the VE engine. By using an                  munity (VDC) website give additional hints and ideas,
external tracking program and the plug-in architecture,               for example the “My First Mod” tutorial [Val10b].
it is easily possible to exchange these components later
with more advanced ones, without having to modify the                 3.1.3    Version Control
actual VE engine.                                                     Directly after creating and compiling the modification
                                                                      project, we put the code under version control, using
The translational head tracking information is used to
                                                                      Subversion [The11], as described in [Val10e]. That
control the view “into” the VE. This so called head-
                                                                      way, we were able to update the code with changes that
coupled perspective (HCP) enables intuitive control,
                                                                      were applied to the SDK later in the development pro-
such as peeking around corners by moving the head
                                                                      cess.
sideways, or zooming in by moving the head closer to
the monitor.
                                                                      3.1.4    Concepts of the Source Engine
The rotational head tracking information is used to con-              Gaining an understanding of the design and function-
trol the head rotation of the user’s avatar. That way,                ality of the Source Engine was a very time-consuming
other users in the VE can see head movement that is                   process. At first sight, the developer website creates an
identical to the movement actually performed physi-                   impression of a thorough documentation. But when it
cally by the user, such as nodding, shaking, or rolling               comes down to details and specific questions, this doc-
of the head.                                                          umentation reveals large gaps and provides outdated or
                                                                      even contradictory information.
In addition, data from face tracking can be used to de-
tect facial expressions and transfer them onto the user’s             The majority of work necessary for understanding the
avatar. Bartlett et al [BLWM08], for example, present a               Source Engine was to read through the provided code
system that recognises a large set of movements of fa-                of the SDK, ignore several inconsistently implemented
cial keypoints, such as lip corners, eyebrows, or blink-              naming conventions, insert execution breakpoints, trace
ing. Using a simpler set of movements and keypoints,                  method calls through several class inheritance levels,
the authors of [QCM10] created a virtual mirror, where                and much more.
an avatar mimics smile, gaze and head direction, and an               Good documentation and a well structured codebase is
opened/closed/smiling mouth of the user in realtime. In               important for any game engine that is to be the founda-
our implementation, we use a non-commercial version                   tion of a simulation. Without these prerequisites, a lot
of the face tracking library faceAPI which does not in-               of time is spent on deciphering the inner workings of
clude the detection of facial expressions.                            the underlying code or on figuring out how to achieve



WSCG 2012 Communication Proceedings                             171                                         http://www.wscg.eu
                           Game Client / client.dll                                                     Game Server / server.dll




                                                                                 Source SDK
                     ▪ Initialise/shutdown                                                        ▪ Initialise/shutdown
                     ▪ Calculate view, render frame                                               ▪ Simulate one frame
                     ▪ Process mouse/keyboard event                                               ▪ Provide map information
                     ▪ Prepare data for saving/loading                                            ▪ Prepare data for saving/loading
                     ▪ ...                                                                        ▪ ...
                     IBaseClientDLL                                                               IServerGameDLL

                                        cdll_int.h                                                                    eiface.h




                                                                            Source Engine
                                                     IVEngineClient                                                                   IVEngineServer




                                                                                        hl2.exe
                                                 Get player/object info ▪                                                        Load/change map ▪
                                       Execute server/client command ▪                                                 Create/destroy/move entities ▪
                                              Get map/game settings ▪                                                     Simulate physical objects ▪
                                  Get system settings/time/screen size ▪                                                              Emit sounds ▪
                                                                     …▪                                                                          …▪

                                    Engine Client                                                                Engine Server
                           Figure 3: Boundaries between the Source SDK and the Source Engine


a certain functionality, instead of implementing the es-                                      3.1.6         Client-Server Architecture
sential parts of the program.                                                                 Games and VEs for multiple users are mostly con-
In the following sections, we will present some of the                                        structed using a client/server architecture [Val10d]. The
major concepts of the Source Engine that played an im-                                        basic principle of client and server communication of a
portant role in the development and modification phase.                                       game based on the Source Engine is shown in Figure 9.
                                                                                              The server is mainly responsible for running the simu-
3.1.5   Game Engine/SDK Boundaries                                                            lation, updating the position, orientation, and speed of
When a multi-user Source Engine game is started, four                                         animated and physically simulated objects. In regular
program parts are involved (see Figure 3):                                                    intervals, e.g., every 33 ms (=30 Hz), it receives com-
                                                                                              pressed command packets from the clients, carrying in-
• The game engine (hl2.exe) is executed, consist-                                             formation about mouse movement, keyboard input, and
  ing of the server                                                                           other events that the users on the clients have triggered.
• and the client part.                                                                        These command packets are unpacked, checked, and
Depending on whether the user chooses to start a new                                          their effect is taken into consideration for the simula-
game server or to connect to an existing game server,                                         tion: avatars move, objects are picked up or released,
the engine then activates                                                                     sounds are played, etc. After each simulation step, the
• either the game server dynamic link library (DLL)                                           new state of all objects and avatars is sent to the clients
  server.dll                                                                                  which can in turn update the changed state of the world
• or the game client DLL client.dll.                                                          on the screen.
The game engine itself cannot be modified at all. No                                          During runtime, each simulated object in the VE ex-
source code of the inner workings of the engine is given.                                     ists in two versions: One version, the “server entity”, is
                                                                                              managed on the server, and is actively simulated. The
The SDK contains header files with interface defini-                                          second version, the “client entity”, exists on each client
tions for the server (eiface.h) and the client part                                           and is kept in sync with the server version by network
(cdll_int.h) of the engine. These interfaces pro-                                             variables [Val10c].
vide access to very basic entity and resource manage-
                                                                                              These variables automatically take care of maintaining
ment, and to the sound, graphics, and system functions
                                                                                              a synchronous state between the server and all clients.
of the engine.
                                                                                              As soon as a variable value changes, its value is marked
It is possible to build a game completely from scratch,                                       for transmission on the next update data packet from the
using only those header files. However, the SDK deliv-                                        server to the clients. To conserve bandwidth, the val-
ers a comprehensive set of classes and methods that, in                                       ues are being compressed and only sent when they have
its entirety, already constitutes a complete game. Start-                                     changed. This mechanism is important to enable fluid
ing from this point, the developer can now modify, re-                                        gameplay on low-bandwidth connections, e.g., dial-up.
move or add custom parts to this framework. The ad-
vantage is rapid prototyping, as long as the result does                                      3.1.7         Prediction
not differ much from the original kind of game.                                               The fact that clients have to wait for a data packet from
However, with every additional change that is necessary                                       the server to show the updated world has a major draw-
to get away from the original game towards the final                                          back: Users would experience a noticeable delay to
product, it gets more and more difficult to implement                                         their actions, especially on slow network connections.
the changes. Some of these difficulties are described                                         To avoid this delay and to provide a fast and respon-
further down in this section.                                                                 sive game, the client predicts the response of the server



WSCG 2012 Communication Proceedings                                           172                                                                       http://www.wscg.eu
and uses this prediction for an immediate response to
user input. When the client later receives the real server
response, it corrects the prediction, if necessary.
For the prediction, the client needs to have the same
rules and simulation routines as the server. In the
SDK, this is implemented by a major duplication
of code for the server and client entity representa-                   Figure 4: Different styles for the viewpoint indicator
tions. However, instead of physical file duplication,
shared code is contained in shared source files (e.g.,
physics_main_shared.cpp) that are included in
both, client and server projects.

3.1.8   Stripping Down the Engine
The next big step, after understanding the engine, was
to strip the project code of unnecessary classes and enti-               Figure 5: Creating body awareness for the avatar
ties, e.g., weapons and the player health indicator. This
step proved very difficult due to numerous interdepen-
dencies within the code. Weapon related code espe-
cially, was very deeply integrated into basic classes.
Removal of one class file would break several other
classes. It required a lot of re-compilation passes and
uncommenting of large code sections until the code
would compile again.                                               Figure 6: Examples of a room with the original Source SDK
                                                                   textures (left) and the custom textures for the user studies
3.1.9   Changing the Interaction
One major change in the original SDK deathmatch                    in realism did not distract the participants of the user
game style was the primary interaction type. After we              studies. Some of them even made fun of the strange
had removed all weapons, we wanted to assign the left              appearance, mentioning “Jedi-powers”.
mouse button click to grabbing and releasing of phys-              3.1.10    Changing the User Interface
ical objects, and to triggering of interactions with ob-
jects, e.g., buttons or patients.                                  Together with the change of the interaction style, we
                                                                   redesigned parts of the user interface. In the original
This seemingly simple change required a lot of rework-             SDK, the centre of the screen is marked by a crosshair,
ing in the code to create access methods to the objects            indicating the point where the weapon would be fired
that the user interacts with, to enable users to take ob-          at.
jects from each other, and to log all of those interaction
events.                                                            With the removal of any weapon related code, the
                                                                   crosshair turned into a viewpoint indicator. After
On a visual level, we wanted the avatars to grab an ob-            some experiments with different indicator styles, We
ject with the right hand as soon as the user would pick            chose a segmented circle that turns green as soon as
it up. This can be implemented with inverse kinemat-               an interactive object is in focus, and closes when a
ics (IK): When the target position of the hand is given,           physical object is grabbed and held (see Figure 4).
IK calculates the position of the animating bones of the           Such a circle has an improved visibility over, e.g., a
arm so that the attached hand reaches that position ex-            simple point. It is also less associated with weapons
actly.                                                             than, e.g., a crosshair.
The Source Engine is capable of IK, as can be seen in              The original weapon crosshair was simply painted at
Half-Life 2 – Episode 2, where a certain tripod char-              the centre of the screen. With the inclusion of head
acter always touches uneven ground with all three feet.            tracking however, we also had to consider a position
However, the Source SDK website states that in multi-              offset caused by the avatar head rotation and translation.
player games, IK is not activated due to difficulties and
performance reasons on the server [Val10a].                        3.1.11    Body Awareness
Our queries in the developer forums resulted in a con-             In the original SDK, the user cannot see the avatar’s
firmation that the engine is capable of IK, but nobody             own body when looking down, as shown in the left im-
was able to give an answer on how to do it.                        age of Figure 5.
For this reason, grabbed objects “float” in front of the           To create body awareness, we had to change several as-
avatar while they are carried around. However, this flaw           pects:



WSCG 2012 Communication Proceedings                          173                                            http://www.wscg.eu
                                                                      Figure 8: Screenshot of the user interface of Xpressor

Figure 7: Observer XT visualising interactions, movements,
and talk patterns of a teamwork simulation
                                                                   The regular style of the tile textures creates a very or-
                                                                   ganised, sterile look. The realism of the rooms created
                                                                   with these textures could be increased further by using
1. The body model has to be drawn, even when the                   photos of real rooms. However, this was not a priority
   game is in first-person view.                                   for our research, but it is an indicator of the complexity
                                                                   of designing and creating realistic environments.
2. The camera viewpoint has to be synchronised with
   any animation of the body model, e.g., walking,                 3.1.13    Data Logging
   standing idle. To achieve this, the camera position
   is constantly updated with the position of the eye-             We also implemented a data logging module that
   balls of the avatar model.                                      records user head movement, user interactions, and
                                                                   gaze targets and duration. The generated logfiles enable
3. When looking up or down, the vertical head rotation             us to analyse individual and teamwork scenarios for
   cannot simply be translated into a camera rotation,             statistical evaluations. An additional benefit, especially
   because in that case the user would be able to see              for teamwork assessment, is the ability of the logfiles
   the inside of the head or the body (see left screenshot         to be imported into external assessment tools, like
   in Figure 5). We added a forwards translation to the            the behavioural analysis tool Observer XT shown in
   camera that is slightly increased when the user looks           Figure 7 [Nol10]. This import eliminates the need
   up or down. Together with correct settings for the              for human assessors to observe a teamwork recording
   near and far plane of the camera frustum, this creates          again to create a list of actions and behaviours. All this
   a realistic body awareness without literally having             information is already present in the VE engine during
   “insight” into the avatar model.                                the simulation and can therefore be directly exported
                                                                   into the logfile.
We had planned to use IK to visualise the head move-
ment caused by head tracking. Physical, translational              3.2    Xpressor
head movement of the user would then have resulted in              Xpressor is the program that we developed for encapsu-
identical translational upper body and head movement               lating the head tracking library faceAPI. The program
of the avatar. As a result, an avatar would lean forward           communicates bidirectionally with the VE engine, us-
or sideways in sync with the user who is controlling it.           ing two local user datagram protocol (UDP) connec-
However, we were not able to implement this feature                tions.
due to the insufficient documentation of the IK features
of the engine.                                                     The communication with the VE engine occurs through
                                                                   a plug-in, as shown in Figure 1. The Source SDK has
                                                                   certain settings and abstraction layers that prevent the
3.1.12    Textures
                                                                   direct use of networking functions and several other op-
The original textures of the SDK are designed for creat-           erating system related functions. However, it is possible
ing games that are set in a post-war era. These textures           to load plug-in DLLs and to exchange data with them.
are, in general, worn down and dull, creating a depres-            We therefore created a simple Xpressor plug-in that is
sive feeling in all maps created with them.                        loaded in the beginning, accepts the UDP connection,
We replaced some of the wall, floor, and ceiling tex-              and relays the following data into the VE engine:
tures with synthetic textures that look like clean tiles.          • translational and rotational tracking data,



WSCG 2012 Communication Proceedings                          174                                           http://www.wscg.eu
                 User         Xpressor               Client                          Server                           Clients
                                                                                                                      Clients
                                                                                                                      Clients
                         Head
                        Movement                 Input Module                      User Avatar                       User Avatar
                                     Tracking Data                                 Server Entity                     Client Entity
                     Control Input

                                               Build compressed
                                               client data packet
                                                                                                    other Client
                                                        Extended Client Data Packet
                                                                                                   Data Packets

                                                                              Unpack and evaluate

                                                 User Avatar
                                                 Client Entity                  Run simulation step


                                                                 Network Variables            Network Variables


                                               Display on screen                                                   Display on screen




                          Figure 9: Data exchange between Xpressor, the VE clients, and the server


• a low resolution video stream,                                          ject from the side, the adjustment is reduced towards
• information regarding whether the user is speaking                      the outer regions of the tracking volume.
  or not, and
• values to control the facial expression of the avatar.                  3.3      Data Model
                                                                          The data model is a description of how to pack the
The video stream is helpful for the user e.g., to ad-
                                                                          values from head tracking and future facial expression
just his or her position at the beginning of a simula-
                                                                          recognition into a data structure that can be easily ex-
tion. To conserve bandwidth, the video is resized to
                                                                          tended, but at the same time also easily compressed and
100 × 60 pixel, converted to 4 bit greyscale, and trans-
                                                                          transmitted.
mitted with 10 fps via a separate UDP connection.
                                                                          Figure 9 visualises the extension in the data flow be-
The program also monitors the signal strength of the                      tween the VE clients and server. Because of the fast
connected microphone, signalising the VE engine via                       local UDP connection between Xpressor and the client,
a flag whether the user is speaking or not. The state of                  the data is transferred uncompressed. Between the
this flag is determined by a simple signal energy thresh-                 clients and the server however, bandwidth can be lim-
old algorithm.                                                            ited, therefore the parameters are compressed.
Xpressor is written in C++, using the Microsoft Foun-
dation Classes (MFC) for the graphical user interface                     4     RESULTS
(GUI) (see Figure 8). For the control of the facial ex-                   The modification of the Source Engine into a virtual en-
pression, we developed a custom circular controller in-                   vironment for medical teamwork training with webcam
terface, visualising six expression types as circle seg-                  support for HCP and head gestures was a non-trivial
ments and the strength of the expression by the distance                  process due to the complexity and insufficient docu-
of the controller position from the centre of the circle.                 mentation of the engine, but allowed for rapid proto-
                                                                          typing of early design stages.
While sitting in front of the screen, the user inadver-
tently shifts his or her neutral head position relative to                All software outside of the VE engine, e.g., Xpressor,
the camera. As a result, any concept relying on an abso-                  was kept modular, as well as most of the code we cre-
lute position will reflect that drift in a slowly changing                ated to add functionality to the VE engine. This en-
view of the VE. Similar to the recommendations from                       abled us in the early stages of our experiments to easily
Sko and Gardner for games using HCP, we have imple-                       exchange our own head tracking module by faceAPI.
mented a configurable automatic slow adjustment of the                    However, features or modifications that required deep
neutral position towards the average of the measured                      changes within the original code had to be kept close
position over several seconds [SG09]. This adjustment                     to the coding style of the SDK itself, resulting in sub-
accommodates for the gradual change of the neutral po-                    optimal program code. The latter problem might be of
sition and rotation of the user’s head. To avoid an un-                   a different magnitude when using different game en-
wanted compensation when the user is at the extreme                       gines, e.g., Unity 3D [Uni11] that provide a more struc-
ends of the tracking range, e.g., when looking at an ob-                  tured codebase to program against. The problems with



WSCG 2012 Communication Proceedings                                 175                                                              http://www.wscg.eu
the complexity of the code of the Source SDK were in-               5   CONCLUSION
creased by insufficient documentation. A lot of devel-
                                                                    In summary, the Source Engine is suitable for rapidly
opment time was spent on deciphering the code or con-
                                                                    developing a teamwork training VE, as long as the
sulting the forums and developer websites for examples
                                                                    changes required to the original SDK code are not too
to compensate for the lack of documentation. To avoid
                                                                    major. The more functionality that is necessary for spe-
this problem, it is important to put more emphasis on
                                                                    cific features of the desired VE, the more complex the
the quality of the documentation and the code of a game
                                                                    coding task becomes. At a certain point, it would be in-
engine when engines are considered for selection.
                                                                    feasible to use this engine and alternative game engines
Content for our VE was created using the free 3D editor             would have to be considered.
Blender [Ble11] and the tools provided by the Source
                                                                    However, the Source Engine proved stable and flexi-
Engine, e.g., the map editor Hammer and the charac-
                                                                    ble enough for our medical teamwork training scenario
ter animation tool Faceposer. Most time during content
                                                                    with additional support for HCP and camera-controlled
creation was spent on figuring out ways how to simulate
                                                                    avatar head gestures. The user studies we have con-
a specific effect or physical behaviour with the engine
                                                                    ducted show that these extensions are well received, and
which is optimized for fast action gameplay, not for pre-
                                                                    improve the usability and the perceived realism of the
cise simulations. On several occasions, we had to com-
                                                                    simulation. In addition, the digital recording of the in-
promise between realism and the ability of the engine to
                                                                    teractions and behaviours within the VE is a valuable
simulate a specific feature. One example is the bleeding
                                                                    support for automated (e.g., with tools like Observer
that occurs during the surgical procedure we designed
                                                                    XT) as well as “manual” assessment of teamwork per-
for the multi-user study. The Source Engine does not
                                                                    formance.
provide physically correct fluid simulation. Instead, we
created a particle effect that resembles a little fountain.
We measured the “success” of the design and imple-                  6   REFERENCES
mentation of our VE indirectly by the user studies we               [Ble11]      Blender Foundation. Blender, 2011.
conducted for our overall goal: to show improvements                             http://www.blender.org.
of usability, realism, and effectiveness of VE-based
                                                                    [BLWM08]     Marian Bartlett, Gwen Littlewort, Tingfan
training scenarios by including camera-based non-
                                                                                 Wu, and Javier Movellan. Computer Ex-
verbal communication support and intuitive HCP-based
                                                                                 pression Recognition Toolbox. In Demo:
view control.
                                                                                 8th Int’l IEEE Conference on Automatic
Overall, the VE proved to be stable and intuitive to use                         Face and Gesture Recognition, 2008.
for the participants, regardless if they were experienced           [DPH+ 09]    Douglas Danforth, Mike Procter, Robert
in playing computer games or not. Our studies compar-                            Heller, Richard Chen, and Mary Johnson.
ing manual view control against HCP showed that HCP                              Development of Virtual Patient Simula-
is an intuitive and efficient way of controlling the view,                       tions for Medical Education. Journal of
especially for inexperienced users [MWW10].                                      Virtual Worlds Research, 2(2):3–11, Au-
For highest user comfort, it is important that the delay                         gust 2009.
between physical head movement and virtual camera                   [Lin10]      Linden Research, Inc. Second Life, 2010.
movement is as short as possible. Our framework was                              http://secondlife.com.
able to deliver a relatively short response time of about           [MRL+ 06]    Brian MacNamee, Pauline Rooney,
100 ms. However, this delay lead to participants repeat-                         Patrick Lindstrom, Andrew Ritchie,
edly overshooting their view target. We suspect that the                         Frances Boylan, and Greg Burke. Seri-
delay is a sum of several smaller delays in each process-                        ous Gordon: Using Serious Games To
ing stage of the data flow, therefore requiring several                          Teach Food Safety in the Kitchen. In Pro-
different optimisation steps for an improvement.                                 ceedings of the 9th International Confer-
For our latest user study, we created a surgical team-                           ence on Computer Games: AI, Animation,
work training scenario and alternated between HCP and                            Mobile, Educational & Serious Games
avatar control being enabled or disabled to investigate                          (CGAMES06), November 2006.
the effect of tracking-based avatar head movement on                [MSL+ 09]    Paul R. Messinger, Eleni Stroulia, Kelly
non-verbal communication within a VE. The results                                Lyons, Michael Bone, Run H. Niu, Kris-
showed an increase in perceived realism of the commu-                            ten Smirnov, and Stephen Perelgut. Vir-
nication within the environment [MWW11]. An effect                               tual Worlds – Past, Present, and Future:
on teamwork training effectiveness was not proven, but                           New Directions in Social Computing. De-
might have been masked by the experiment design. A                               cision Support Systems, 47(3):204–228,
clarification is subject to future research.                                     June 2009.



WSCG 2012 Communication Proceedings                           176                                         http://www.wscg.eu
[MWW07] Stefan Marks, John Windsor, and                              Overview. Technical report, School of
        Burkhard Wünsche. Evaluation of Game                         Humanities and Informatics, University of
        Engines for Simulated Surgical Train-                        Skövde, Sweden, February 2007.
        ing. In GRAPHITE ’07: Proceedings of                [ST09]   Shamus P. Smith and David Trenholme.
        the 5th international conference on Com-                     Rapid prototyping a virtual fire drill envi-
        puter graphics and interactive techniques                    ronment using computer game technology.
        in Australia and Southeast Asia, pages                       Fire Safety Journal, 44(4):559–569, May
        273–280, New York, NY, USA, December                         2009.
        2007. ACM.
                                                            [The11]  The Apache Software Foundation.
[MWW10] Stefan Marks, John Windsor, and                              Apache Subversion, 2011. http://
        Burkhard Wünsche. Evaluation of the Ef-                      subversion.apache.org.
        fectiveness of Head Tracking for View and           [TSHW08] Jeffrey Taekman, Noa Segall, Eugene
        Avatar Control in Virtual Environments.                      Hobbs, and Melanie Wright. 3DiTeams
        25th International Conference Image and                      — Healthcare Team Training in a Virtual
        Vision Computing New Zealand (IVCNZ)
                                                                     Environment. Simulation in Healthcare:
        2010, November 2010.
                                                                     The Journal of the Society for Simulation
[MWW11] Stefan Marks, John Windsor, and                              in Healthcare, 3(5):112, 2008.
        Burkhard Wünsche. Head Tracking Based
                                                            [Uni11]  Unity Technologies. UNITY: Unity 3 En-
        Avatar Control for Virtual Environment
                                                                     gine, 2011. http://unity3d.com/
        Teamwork Training. In Proceedings of                         unity/engine.
        GRAPP 2011, 2011.
                                                            [Val07]  Valve Corporation. Source Engine, 2007.
[Nol10] Noldus Information Technol-                                  http://source.valvesoftware.
        ogy.       Observer XT, 2010.                                com.
        http://www.noldus.com/
        human-behavior-research/                            [Val10a] Valve Developer Community. IK
        products/the-observer-xt.                                    Chain, 2010. http://developer.
                                                                     valvesoftware.com/wiki/
[QCM10] Rossana B. Queiroz, Marcelo Cohen, and
                                                                     $ikchain.
        Soraia R. Musse. An extensible frame-
        work for interactive facial animation with          [Val10b] Valve Developer Community. My First
        facial expressions, lip synchronization and                  Mod, 2010. http://developer.
        eye behavior. Computers in Entertainment                     valvesoftware.com/wiki/
        (CIE) - SPECIAL ISSUE: Games, 7:58:1–                        First_Mod.
        58:20, January 2010.                                [Val10c] Valve Developer Community. Net-
[RLD06] Andrew Ritchie, Patrick Lindstrom, and                       working Entities, 2010. http://
        Bryan Duggan. Using the Source En-                           developer.valvesoftware.com/
        gine for Serious Games. In Proceed-                          wiki/Networking_Entities.
        ings of the 9th International Conference            [Val10d] Valve Developer Community. Source
        on Computer Games: AI, Animation,                            Multiplayer Networking, 2010. http://
        Mobile, Educational & Serious Games                          developer.valvesoftware.com/
        (CGAMES06), November 2006.                                   wiki/Net_graph.
[SG09]  Torben Sko and Henry J. Gardner.                    [Val10e] Valve Developer Community. Us-
        Human-Computer Interaction — INTER-                          ing Subversion for Source Control
        ACT 2009. In Tom Gross, Jan Gulliksen,                       with the Source SDK, 2010. http:
        Paula Kotzé, Lars Oestreicher, Philippe                      //developer.valvesoftware.
        Palanque, Raquel Oliveira Prates, and                        com/wiki/Using_Subversion_
        Marco Winckler, editors, Lecture Notes in                    for_Source_Control_with_the_
        Computer Science, volume 5726/2009 of                        Source_SDK.
        Lecture Notes in Computer Science, chap-
        ter Head Tracking in First-Person Games:
        Interaction Using a Web-Camera, pages
        342–355. Springer Berlin / Heidelberg,
        August 2009.
[SJB07] Tarja Susi, Mikael Johannesson, and
        Per Backlund. Serious Games -– An




WSCG 2012 Communication Proceedings                   177                                      http://www.wscg.eu
WSCG 2012 Communication Proceedings   178   http://www.wscg.eu
           A morphing approach for kidney dynamic modeling
              From 3D reconstruction to motion simulation
    Valentin Leonardi                   Jean-Luc Mari                         Vincent Vidal                  Marc Daniel
    LSIS, UMR CNRS 7296              LSIS, UMR CNRS 7296                      L2PTV, EA 4264             LSIS, UMR CNRS 7296
      Campus de Luminy                 Campus de Luminy                         CERIMED                    Campus de Luminy
    13288 Marseille cedex 9,          13288 Marseille cedex 9,              13385 Marseille cedex 5,      13288 Marseille cedex 9,
           France                            France                                France                        France
    valentin.leonardi@univ-amu.fr      jean-luc.mari@univ-amu.fr              vincent.vidal@ap-hm.fr       marc.daniel@univ-amu.fr



                                                           ABSTRACT
Motion simulation of an organ can be useful in some cases like organ study, surgery aid or tumor destruction.
When using a non-invasive way of tumor destruction through transcutaneous transmition of waves, it is primordial
to keep the wave beam focused on the tumor. When the tumor is not in movement, such a task is trivial. But when
the tumor is located in a moving organ like the kidney, motion simulation is necessary. We present here an original
method to obtain the kidney motion simulation: this is done using a mesh morphing (we consider the kidney has
already been segmented and reconstructed for three different phases of the respiratory cycle). Such an approach
allows a smooth transition between the different kidney models, resulting in a motion simulation. Thus, the method
is purely geometric and does not need any kind of markers or tracking device. It gives directly a full 3D simulation
and models are animated in real time. Finally, our approach is automatic and fast, so that it can easily be used in a
medical environment.
Keywords: Geometrical modeling, organ motion simulation, kidney modeling, mesh morphing

1    INTRODUCTION                                                         moves because of the respiratory cycle. A kidney (and
Tumors can be treated by low-invasive approaches. The                     a tumor) tracking is therefore necessary. Before this or-
goal is to minimize interactions between the surroun-                     gan tracking stage, we need to obtain a solid 3D model
ding environment and the patient in order to limit the                    of it. This work is described in our previous paper
consequences of surgery (incision treatment, convales-                    [LMVD11].
cence) and their possible complications (nosocomial in-
fections). Kidney tumors can be treated by radiofre-                         What we propose here is a new method which has
quency. Radiofrequency is a low-invasive, non-surgical                    two goals: the first one is the motion and deformation
percutaneous heat treatment. The principle is to locate                   visualization of an organ (the kidney in this case) un-
the tumor through CT scan, and insert a radiofrequency                    der the influence of natural breathing. The second goal
electrode in its center. An electric current is then deli-                results directly from the first one and is the tracking of
vered, in order to destroy the tumor. However, there is a                 a part of this organ: its tumor. The originality of this
chance of cancerous cell displacement when removing                       method is that it uses a fully geometric approach: mesh
the electrode.                                                            morphing. Thus it is fast and only needs three models
                                                                          corresponding to three breathing phases: inhale phase
   The KiTT project (for Kidney Tumor Tracking, of                        (when volume of air in lungs is maximal), exhale phase
which we take part) is fully involved in the low-invasive                 (when volume of air in lungs is minimal) and the mid-
protocol. Its goal is to create a totally non-invasive new                dle phase between the two previous ones, which we re-
approach by transmitting radiofrequency waves, in a                       fer as the mid-cycle phase. Moreover, the results ob-
transcutaneous way until tumor eradication. The main                      tained are fully geometric; the output is an animated
difficulty is to keep the wave beam continuously fo-                      3D model. Thereby general motion and all deforma-
cused on the tumor while the kidney is deformed and                       tions can be studied at once where some methods only
                                                                          offer the possibility of a 2D visualization. Finally, as
                                                                          the tumor is also animated, it is possible to know its po-
                                                                          sition at any time.
Permission to make digital or hard copies of all or part of
this work for personal or classroom use is granted without                   Section 2 of this article deals with the previous work
fee provided that copies are not made or distributed for profit
or commercial advantage and that copies bear this notice and
                                                                          in organ tracking and mesh morphing. Section 3 in-
the full citation on the first page. To copy otherwise, or re-            troduces the general process of our method: a brief re-
publish, to post on servers or to redistribute to lists, requires         call of the kidney reconstruction is done, then the al-
prior specific permission and/or a fee.                                   gorithm used for mesh morphing is described in detail.



WSCG 2012 Communication Proceedings                                 179                                          http://www.wscg.eu
In section 4, we present the obtained results, their per-         nimization of the Extended Projective Point Criterion.
formances and we discuss them. Finally in section 5,              In [RSH+ 99] two operations are done to compute the
we present the limits of our method, the possibilities to         registration: affine transformation is used for global
overcome them and the perspectives of future work.                movements while Free Form Deformation is used for
                                                                  local movements. Two registration algorithms based
2     RELATED WORK                                                on optical flow are implemented and accelerated using
                                                                  GPU programming in [NdSE+ 08] in order to perform
2.1    Organ tracking
                                                                  an image-guided radiotherapy.
Organ tracking methods are either based on mathema-
tical models which represent the respiratory cycle as a
periodical function, or on empirical algorithms which             2.2   Principle of morphing
predict future movements by observation and analysis              Mesh morphing is a method used to transform progres-
of previous ones.                                                 sively a source model Ms into a target model Mt . The
                                                                  most usual method to perform a mesh morphing is to
  The most intuitive way to track an organ is to put a            find a common vertex/edge/face network for both mo-
marker which is highly detectable by a classic medical            dels in order to compute a metamesh Mm which con-
imaging acquisition near this organ [NUG+ 08, NPSA07,             tains the topology of Ms and Mt . The common net-
OTW+ 05, SGB+ 00, SSK+ 00]. This formalism is also                work is obtained by mapping the mesh into arbitrary
used in all-in-one robotic radiosurgery systems such as           shapes, using different kind of mappings based on the
the Cyberknife [MCG+ 03]. This kind of method re-                 resolution of a linear system. This approach was first
quires a surgical intervention which is not suitable for          used by Kent et al. in [KCP92], where both models
our problematic.                                                  are mapped onto the unit disk. In [ACOL00] Alexa
                                                                  et al. perform a mesh morphing where the interior is
   The following approaches assume the kidney has                 also considered. A 3D mesh is decomposed into a set
been segmented and reconstructed previously for two or            of tetrahedrons and a 2D shape into a set of triangles.
more phases of the respiratory cycle. Most of the time,           The interpolation of a tetrahedron is done using a ro-
only two models are needed, but three [SBMG06] or                 tation and a scale-shear with positive scaling matrices.
even more [RMOZ01] are sometimes necessary. These                 In [GSL+ 98] both Ms and Mt are divided into an equal
extra acquisitions can be used to improve the precision           number of patches P. Each patch is then mapped so
of the organ deformation. In other cases, it is not an            that the patch Pk of Ms is morphed into the patch Pk of
extra acquisition of the organ that is needed, but other          Mt . This approach is close to the one used by [KSK00]
kind of data essential to the method. Hostettler et al.           since models are also divided into n arbitrary shapes.
[HNS+ 08] use the diaphragm movement in order to re-              These shapes are mapped onto a polygon, where ver-
flect it on the abdomen organs. In [SBMG06], air, tis-            tices of the border of a shape lie on the border of the
sues and lungs have to be segmented for three acquisi-            polygon. The position of the remaining vertices is then
tions in order to get an organ tracking.                          computed by considering the shape as a spring-mass
                                                                  system at rest, the border vertices being the the fixed
   Deformation fields are used to understand the mo-              masses. In [LDSS99] Ms and Mt are simplified into Ms0
tion of an organ. This field computes the deforma-                and Mt0 using the MAPS method [LSS+ 98]. Vertices for
tions necessary to apply on a given source model Ms               which vertex-vertex correspondences is already known
to deform it into a given target model Mt . The defor-            are kept. Ms and Mt are finally mapped in order to com-
mation field can be computed using several methods                pute the correspondences. Unfortunately, the previous
like Maximum Likelihood / Exceptation-Maximisation                approaches always need either user interaction (which
[RMK+ 05], least squares [SBMG06] or approaches                   can be very time consuming for some methods) or ver-
based on Normalized Mutual Information [RSH+ 99].                 tices correspondence between Ms and Mt prior to the
Deformations can also be applied on a mesh through                mesh morphing. In both case, our constraints do not
a deformable superquadratic in order to get the move-             allow to spend a lot of time on cutting up a mesh man-
ment of an organ [BCA96].                                         ually or making vertex-vertex correspondences.

  Registration methods are also a good way to have                   It is possible to fully automate a mesh morphing al-
an organ tracking. Nicolau et al. [NPSA07] use two                gorithm by using an automatic mesh cutting up. In-
acquisitions: on the first one, markers are used in or-           deed, such a process allows to separate a model into
der to get the position of the organ of interest. Then a          at least two different parts which can then be mapped.
second acquisition is done without these markers. By a-           Several works can be found, although it is a difficult
nalyzing the difference of position of the spine for both         problem: some methods are based on the use of a sin-
acquisitions, the registration is performed using the mi-         gle patch [KSK97], where others are related to graph



WSCG 2012 Communication Proceedings                         180                                        http://www.wscg.eu
theory problem and aim at balancing the size of patches
[EDD+ 95, KK99]. However, our models are close to
each other, which does not justify such an advanced
approach. Another way to have a complete automatic
mesh morphing is to map models onto the unit sphere
[KCP92]. Indeed, there is no need to divide the models
anymore since they are homeomorphic to a sphere. On
the other hand, the model has to be star-shaped, which
is not the case of a kidney. Alexa [ACOL00] introduces
a variant for sphere mapping: as for barycentric map-
ping, each vertex is placed at the centroid of its neigh-
bors. Finally, a new approach for mesh morphing is
done by Lee et al. in [YHM07]. The principle is to
compute a constraints field C in order to deform Ms
into Mt . C is then interpolated in order to determine
a new constraints field C0 for an intermediate model Mi
between Ms and Mt .

3     GENERAL PROCESS
3.1    Overview
A general view of our entire workflow is presented
on Figure 1. It shows how to get the kidney motion
visualization from 3 sets of images. These sets result
from three CT-scan or MRI acquisitions. First, the kid-
ney and tumor are segmented for the three sets of im-
ages. Then, the organ is reconstructed in order to have
three 3D models (called M1 , M2 and M3 ), each one cor-
responding to a precise breathing phase. Mesh mor-
phing between M1 and M2 and between M2 and M3 is
computed. Our mesh morphing approach is based on
an automatic mesh cutting up, unit disk mapping and
metamesh creation. Since models are relatively close to           Figure 1: Overview of our entire workflow: three sets
each other, they can be divided into only two patches.            of images resulting from a medical imaging acquisi-
Moreover, the frontier between the two patches always             tion for the inhale, mid-cycle and exhale phase is done
has the same orientation on the kidney, which allows              (first line). The kidney and the tumor are segmented for
this step to be entirely automatic. Once the two patches          every images of these three acquisitions (second line).
are defined for Ms and Mt , a unit disk mapping is per-           The Poisson surface reconstruction is then applied to
formed in order to overlay the two mappings of a cor-             the point cloud extracted from the segmentation of each
responding patch of both models. We cannot use a                  three different phases. We call the resulting models M1 ,
sphere mapping here as kidney models are not star-                M2 and M3 (third line). Mesh morphing is computed
shaped. Detections of mapped edge intersections and               between M1 and M2 and between M2 and M3 . The re-
mapped vertices positions allow to create a metamesh              sults are two metameshes which allow a smooth transi-
which comprises the topology of both models. As an                tion between M1 to M2 and M2 to M3 (fourth line). By
initial and a final position are known for all metamesh           alternating the two metameshes, a full and smooth tran-
vertices, the metamesh is animated by linear interpola-           sition from M1 to M3 is possible, resulting in the kidney
tion. The alternation between metameshes coming from              motion visualization (fifth line).
M1 and M2 and from M2 and M3 gives a full kidney
animation from inhale to exhale phase, i.e. the whole             3.2   Kidney reconstruction
respiratory cycle. The only part of our method done in
real time and during the whole tumor destruction pro-             A full description of the kidney reconstruction is de-
cess is the metamesh vertices interpolation. Everything           tailed in [LMVD11]. The method used to get the kid-
before this step is done once and for all and takes less          ney model is divided into two stages: the first one is
than 2 minutes (from kidney segmentation to metamesh              the kidney segmentation from which a point cloud is
creation).                                                        extracted. The second stage consists in reconstructing




WSCG 2012 Communication Proceedings                         181                                         http://www.wscg.eu
this point cloud in order to obtain a model.

   A region growing approach is used in order to seg-
ment the kidney. Despite some methods exist to define
automatically the seed needed for initialization, our ap-
proach uses a minor user-interaction and needs a single
mouse click to define it. However, this is done only for
one image (since the whole kidney is present in at least
60 slices). The region segmented in an image Ik−1 is
used to get the seed for the next image Ik : the weighted         Figure 4: Final result of a kidney point cloud (left) and
barycenter of the points defining the contour in Ik−1 de-         its reconstruction using the Poisson surface reconstruc-
fines the seed for Ik . Results of this method are shown          tion (right).
on Figure 2
                                                                  3.3    Morphing
                                                                  The morphing stage must have very basic user- interac-
                                                                  tions. The two models to morph are close to each other
                                                                  since they both come from the same kidney. Thus, the
                                                                  mesh morphing method uses an automatic mesh cutting
                                                                  up in two patches, unit disk mapping and metamesh cre-
                                                                  ation. We cannot map onto a sphere as kidney models
                                                                  are not star-shaped. All these steps are described in de-
Figure 2: Final results using our kidney segmenta-                tail hereunder. For the rest of this paper, we will use
tion approach for left and right kidney on two different          the following symbols: M represents a given model, Ms
slices.                                                           is the source model and Mt the target model, as used
                                                                  in the previous sections. C is the connectivity between
   The point cloud extracted from the segmentation stage
                                                                  vertices, edges and faces of M. V = {v1 , v2 , v3 , ..., vn } is
is reconstructed using the Poisson Surface Reconstruc-
                                                                  the position in R3 of vertices. Edges are represented as
tion [KBH06]. The principle of this algorithm is to
                                                                  a pair of vertices {i, j} and faces as a triplet of vertices
define an indicator function χ peculiar to a model M,
                                                                  {i, j, k}. Finally, N(i) is the set of adjacent vertices to
which is 0 for every point outside the model and 1 in-
                                                                  vertex {i}, i.e. N(i) = {{ j}|{i, j} ∈ C}.
side. Deducing χ directly from the oriented point cloud
is the major problem in this case. The solution is to use
the gradient of χ since the point cloud can be conside-
                    →
                    −                           →
                                                −                 Mesh cutting up: obtaining the tearing path
red as samples of ∇ χ (see Figure 3). Indeed, ∇ χ is a
vector field that is 0 almost everywhere except near the          The first stage of the mesh dissection consists in com-
                         →
                         −                                        puting its principal axis. This can be done by consi-
surface. A vector field V which is an approximation of
→
−                                                                 dering only the vertices and using Principal Component
 ∇ χ is found using the original normals. χ must now
                    →
                    −        →
                             −      →
                                    −                             Analysis (PCA). Moreover, the PCA gives the 3 princi-
be deduced from V , i.e. ∇ χ = V . This is done by
applying the divergence operator to express it as a Pois-         pal vectors of the mesh; the firsts two and the barycenter
                           →
                           −         →
                                     −                            of the mesh define the principal plane. Thus, the next
son equation: ∆χ ≡ ∇ · ∇ χ = ∇ · V . The resolution
of this equation is a well known problem (especially in           stage consists in computing the intersections between
physics) but will not be discussed here. The final re-            edges of M and the principal plane, defining what we
construction is then obtained from the extraction of an           call the intersected edges. In the same way, the vertices
appropriated isosurface (see Figure 4).                           {i, j} of an intersected edge are called intersected ver-
                                                                  tices. This set of the intersected edges is the first stage
                                                                  of the final tearing path (see Figure 5).

                                                                The tearing path must be a unique loop of edges in
                                                              C, i.e. {{i1 , i2 }, {i2 , i3 }, ..., {in−1 , in } , {in , i1 } | {ik , im } ∈
                                                              C ∀(k, m) ∈ [1; n]; this set of edges is a subset of C
                                                              and is called c. Thus, two successive intersected edges
                                                              must share a same vertex. The purpose of the first
Figure 3: Overview of the Poisson surface reconstruc-         post-process of the intersected edges is to remove dead-
tion.                                                         end edges from c. Such edge has one of its vertices
                                                              which is not shared with any other intersected edge, i.e.
                                                              {{i, j}|∀l ∈ N( j){ j, l} ∈     / c}. To detect such edges, we



WSCG 2012 Communication Proceedings                         182                                                   http://www.wscg.eu
Figure 5: Intersection between the kidney model and its
principal plane (in blue). The resulting tearing path is
displayed in red.

first compute the partial adjacency list of each vertex in
c. This list is the set of adjacent vertices { j} in c to
a vertex {i}, i.e. {{ j}|{i, j} ∈ c}. A dead-end edge
is then simply detected when at least one of its ver-
tices has only one neighbor, i.e. its partial adjacency
list length is 1 (see Figure 6 - b). The second post-
process consists in removing local loops: the tearing
path must be a unique succession of edges and each ver-            Figure 6: Whole example of the post-process of a tear-
tex must be shared by two and only two edges. Thanks               ing path. Although this example cannot exist in a real
to the partial adjacency list, vertices from which the             situation, it presents all the cases needed to under-
tearing path separates are easily detected: such vertices          stand how the full post-process works. From top to bot-
have, at least, 3 neighbors. Thus, local loops are re-             tom: (a) Original tearing path - (b) 1-adjacency vertex
moved as follow. Starting from a 2-adjacency vertex we             detection (diamond) and dead-end edges suppression
choose arbitrarily one of its neighbors and so on, until           - (c) 3 (or more)-adjacency vertex detection (square).
a 3-adjacency vertex is reached. During this step, each            Starting from the pointed vertex, an arbitrary neigh-
vertex is skimmed only once so that it appears at most             bor is chosen. - (d) For a 3-adjacency vertex, we still
once in the final tearing path. An arbitrary neighbor of           choose an arbitrary neighbor, but every other edge is
the current 3-adjacency vertex is still chosen, but every          suppressed. - (e) To avoid apparition of new dead-
other edges containing the current vertex is suppressed            end edges when edges are suppressed, recursive sup-
from c. As such a process creates new dead-end edges,              pression of every edges from 2-adjacency neighbor is
every edge of each 2-adjacency neighbor is recursively             done. - (f) The final tearing path obtained after the
suppressed until the neighbor is a 3-adjacency vertex              post-process.
(see Figure 6 - c,d,e). As the current 3-adjacency ver-
tex becomes a 2-adjacency vertex, the whole process is
repeated until we fall back on the first vertex.                   meshes are then rotated so that their principal planes
                                                                   are aligned with xz-plane. This way, it is possible to
                                                                   check if parts tagged the same in the two models have
Mapping mesh onto the unit disk                                    the same y orientation. If not, tags 1 and 2 of one model
                                                                   are swapped. This step is essential since the part of Ms
Once the tearing path has been computed, vertices are              tagged as 1 (resp. 2) will be morphed into the part of
tagged in three different way. We call them tag 0, 1 and           Mt tagged as 1 (resp. 2) (see Figure 7).
2. Tagging the mesh allows to define the two parts of it
which will be mapped later. Vertices defining the tear-              Now that every vertex is tagged, they can be mapped
ing path are tagged as 0. A unique arbitrary neighbor              onto the unit disk. Although any kind of mapping is
of a vertex tagged as 0 is tagged as 1. We recursively             applicable, we choose the discrete harmonic mapping
tag all its neighbors, so that a whole part of the mesh            [Pol00] since it preserves as much as possible the topo-
is tagged as 1. The other part is tagged as 2. Both



WSCG 2012 Communication Proceedings                          183                                         http://www.wscg.eu
                                                                                                           −→                  −−→
                                                                            intersection point are v0i + α v0i v0j and v0k + β v0k v0l . Coef-
                                                                            ficient α and β are saved along with the new vertex.
                                                                            These coefficients will be necessary for intermediate
                                                                            models as they are sufficient to compute the coordi-
Figure 7: Example of two models for which a same tag                        nates of the vertex, even when vi , v j , vk and vl are inter-
has a different y orientation. Vertices in red are tagged                   polated. These kind of vertex is called an intersection
as 0, vertices in cyan tagged as 1 and vertices in ma-                      vertex. Once an intersection vertex is created, appro-
genta tagged as 2.                                                          priate edges and faces are created along with it in order
                                                                            to build the topology of the metamesh Mm . These new
                                                                            edges and faces will allow Mm to combine topology of
logy of faces of both models. The most straightforward                      both Ms and Mt and to have a continuous interpolation
step of this mapping is for the intersected vertices. They                  between the two models (see Figure 8).
are fixed on the unit circle in a way that arc length be-
tween each pair of successive vertices is proportional to
the original length of edge in mesh. For vertices tagged
as 1 or 2, discrete harmonic mapping (as well as other
mapping) amounts to solving a linear system described
as follow. Two distinct mappings are done, one for each
tag. Let Vi be the vertices to map with 0 ≤ i < n index
of vertices tagged as 1 (resp. 2) and n ≤ i < N index of
vertices tagged as 0. Then, the linear system to solve is:


                                                   ∑N−1
                                                               
                         v1                         i=n λ0,i vi
                        v2                      ∑N−1
                                                    i=n λ1,i vi
                                                                  
                                                                        Figure 8: Example of intersections between mapped
       (I − Λ) 
                        v3     =
                                                 ∑N−1
                                                    i=n λ2,i vi
                                                                  
                         ..                         ..
                                                                  
                                                                           edges of Ms (solid line) and Mt (dotted line). Intersec-
                          .                          .                  tion points 1, 2, 3 and 4 are created, as well as appro-
                       vn−1                   ∑N−1
                                               i=n λn−1,i vi                priate edges (C1, 1D, C2, 2F, ...) and faces (C12, F23,
                                                                            ...).
  where Λ = {λi, j } and λi, j is a coefficient depending
on the mapping used. Here, for discrete harmonic map-                         The second stage of the metamesh creation is the
pings, we have:                                                             computation of barycentric coordinates (BC) for every
                                                                            vertices of Ms and Mt . To do that, we first want to know
              (                                                             on which mapped face {i0 , j0 , k0 } of Mt0 (resp. of Ms0 ) a
                      cotαi, j +cotβ i, j
                  ∑ j∈N(i) (cotαi, j +cotβi, j )
                                                     if {i, j} ∈ C          mapped vertex v0m of Ms0 (resp. Mt0 ) lies on. The BC are
    λi, j =
                                              0      if {i, j} ∈
                                                               /C           a unique triplet u, v, w such that v0m = uv0i + vv0j + wv0k .
                                                                            The face where v0m lies on and its BC are saved. This
   with αi, j = ∠(i, k0 , j) and βi, j = ∠(i, k1 , j). Edge                 kind of vertex is called a mesh vertex (see Figure 9).
{i, j} is adjacent to two and only two faces since M
is a triangular mesh. k0 and k1 are the two vertices that
define these faces. We call Ms0 N Mt0 N the mapping of Ms
and Mt for tag N. Similarly we call {i0 } a mapped ver-
tex. Although such notation should not exist since only
the position of vertices (vi ) changed during the map-
ping, this notation will make further expressions more
straightforward.


Metamesh creation and animation: computing in-
tersections and barycentric coordinates                                     Figure 9: Mapped vertex of Mt0 , v0 lies on face
                                                                            {v01 , v02 , v03 } of Ms0 . Its BC are computed so that v0 =
The next step is to overlay Ms0 N Mt0 N for both tags in                    0.8v01 + 0.7v02 + 0.2v03 . Position of v on face {v1 , v2 , v3 }
order to compute the metamesh. The first stage is to                        of Ms is then known thanks to these coordinates: v =
detect intersections between mapped edges. When two                         0.8v1 + 0.7v2 + 0.2v3 .
edges {i0 , j0 } ∈ C for Ms0 and {k0 , l 0 } ∈ C for Mt0 cross,
a new vertex is created. Two valid definitions of this                        Thus, the metamesh is completely built and com-
                                                                            posed of a set of intersection vertices and mesh vertices.



WSCG 2012 Communication Proceedings                                   184                                               http://www.wscg.eu
Intermediate models can now be easily obtained by in-                animated version can be seen at the following URL:
terpolating positions of vertices. The interpolation is              http://www.youtube.com/watch?v=dhPqLp2X8NQ.
possible since we know, for each one of them, an ini-
tial and a final position as following: for a mesh vertex               General movement and deformation of the kidney are
coming from Ms , the initial position is its position in             respected. The natural rotation of the principal axis of
Ms . The final position is known by the combination                  the organ is present here, as well as its enlargement. On
of its BC and the face of Mt it lies on. Inversely, for              the other hand, local deformations are not totally sa-
a mesh vertex coming from Mt , the initial position is               tisfying, especially for the tumor. The one on the mor-
known using its BC and the face of Ms it lies on. The                phed kidney is absorbed into a part of the kidney and
final position is its natural position in Mt . For an inter-         reappear from a different part, right next to its origi-
section vertex, the initial position is known thanks to its          nal location. The natural deformation would have been
α coefficient and the edge of Ms it lies on. The final po-           a smooth displacement between theses two locations,
sition is computed using its β coefficient and the edge              almost like a translation. This is due to the morphing
of Mt it lies on.                                                    method itself. Although these false deformations are
                                                                     not really noticeable, they become obvious when tumor
                                                                     is displayed: it sticks out of the kidney model (Figures
3.4    Tracking the tumor                                            10). Another drawback of our method is the cutting
The tumor tracking is the second goal of our method.                 of the mesh by a plane. In order to have a correct mor-
It is important to know where it is located to adjust                phing, the tearing path obtained from the intersection of
the wave beam accordingly. From this point of view,                  the mesh and this plane must be composed of one con-
there are two main differences between the tumor and                 nected component. Such a criterion is not always gua-
the kidney. The first one is the tumor is not deformed               ranteed. From our experiment and analysis, computing
by the respiratory cycle, it only moves along with the               intersection between a kidney model and its principal
kidney. The second one is the tumor is similar to an                 plane will result in a one connect component tearing
ellipsoid. In the segmentation step, the tumor is seg-               path (this is due to the shape of the organ itself), but
mented separately from the kidney and in a such way                  that would not be necessarily the case for another organ
that the center of the tumor is known. An other mesh                 or some kind of arbitrary models.
morphing to obtain the tumor movements (i.e. its track-
ing) would be inappropriate since its shape remains the
same from one breathing phase to another. Moreover, it
would cost useless computational time. A more conve-
nient way to do that is to interpolate the position of the
tumor since we have the coordinates of its center for
the inhale, exhale and mid-cycle phases. We can use                  Figure 10: Highlighting local deformation problem. In-
a quadratic Bézier curve interpolation, which gives the              termediate model with tumor (blue ellipsoid) presents
tumor position for intermediate phases. In real condi-               local inaccuracy, especially for the tumor region (en-
tions, the patient will be anesthetized and on respirator,           circled).
allowing a full control on his breathing, i.e. the phase of
his respiratory cycle is known at any time. Therefore,                  As the three models have more or less the same num-
it is really easy to synchronize the metamesh and the                ber of vertices, edges and faces, computation times for
tumor interpolation along with the patient’s breathing.              one morphing are equivalent for the other.s The models
Thus, the 3D coordinates of the tumor are known at any               we have are composed by up to 2,300 vertices, 6,900
time and correspond to its real position, resulting in the           edges and 4,600 faces. A morphing is computed in 40s,
tumor tracking.                                                      each step being repeated twice, one for each tag (data
                                                                     was processed on a laptop with an Intel Core i7 proces-
                                                                     sor and 4 Go of RAM). Although that does not allow
4     RESULTS                                                        to compute mesh morphing in real time, this execution
Our method has been tested on a set of three kidney                  time is acceptable for our medical environment, where
models M1 , M2 and M3 obtained as described in sec-                  interventions used for our non-invasive tumor destruc-
tion 3.2. Theses models correspond respectively to the               tion (High Focused Ultrasound) are very long (up to 3
inhale phase, mid-cycle phase and exhale phase. Two                  hours). Moreover, the whole computation time is al-
mesh morphing were performed: the first between M1                   most needed only for the metamesh creation, which is
and M2 and the second between M2 and M3 . Figures                    done only once. Its animation can be done in real time
11, 12 present several intermediate models obtained                  as it is simply an interpolation between an initial and
while performing the morphing from M1 to M2 to M3 .                  final position of its vertices as seen in section 3.3.
As results are not very explicit with frozen models, an



WSCG 2012 Communication Proceedings                            185                                         http://www.wscg.eu
Figure 11: Final results showing natural movements of the right kidney due to respiration. Source and target
models obtained from reconstruction are displayed in red. Intermediate models are displayed in grey. Morphing
from M1 to M2 is showed here (from left to right).




Figure 12: Morphing between M2 to M3 from a different point of view (rotation of 180 degrees around vertical
axis). Models are displayed in wireframe and the tumor is visible (blue ellipsoid).

5   CONCLUSION                                                     support.
We have presented an original and geometric approach
to obtain the natural motion simulation of the kidney
                                                                   REFERENCES
under the respiratory cycle. Starting from three medi-
cal imaging acquisitions of the organ, each one for a              [ACOL00] Marc Alexa, Daniel Cohen-Or, and David Levin.
                                                                            As-rigid-as-possible shape interpolation. Pro-
different phase of the cycle, kidney is first segmented
                                                                            ceedings of Computer Graphics and Interactive
then reconstructed in order to create one model for each
                                                                            Techniques, 2000.
phase. Kidney is finally animated in 3D and respira-
                                                                   [BCA96]     Eric Bardinet, Laurent Cohen, and Nicholas Ay-
tory movements are simulated through mesh morphing
                                                                               ache. Tracking and motion analysis of the left
among the three models we previously had (from first
                                                                               ventricle with deformable superquadrics. Medi-
to second model and from second to third). To do that,                         cal Image Analysis, 1(2):129 – 149, 1996.
it is first necessary to cut the mesh, which is done auto-
                                                                   [EDD+ 95] Matthias Eck, Tony DeRose, Tom Duchamp,
matically here. Then the two different parts of a mesh
                                                                             Hugues Hoppe, Micheal Lounsbery, and Werner
are mapped onto the unit disk. This mapping is used                          Stuetzle. Multiresolution analysis of arbitrary
to compute a metamesh which comprises the topology                           meshes. Proceedings of SIGGRAPH, pages 173
of two successive models. Soft transition between two                        – 182, 1995.
models, and thus the kidney animation, is finally ob-              [GSL+ 98]   A. Gregory, A. State, M.C Lin, D. Manocha, and
tained by interpolating each vertex of the metamesh be-                        M.A. Livingston. Feature-based surface decom-
tween an initial and a final position. Although general                        position for correspondence and morphing be-
deformation and movement of the kidney is well simu-                           tween polyhedra. Proceedings of Computer An-
lated, local deformations are not precise enough, espe-                        imation, pages 64 – 71, 1998.
cially for tumors near the surface. A way to overcome              [HNS+ 08] Alexandre Hostettler, Stéphane Nicolau, Luc
this problem would be to force regions with similar cur-                     Soler, Yves Rémond, and Jacques Marescaux.
vature to morph into each other.                                             A real-time predictive simulation of abdominal
                                                                             organ positions induced by free breathing. Inter-
                                                                             national Symposium on Biomedical Simulation,
ACKNOWLEDGMENT                                                               pages 89 – 97, 2008.
This work is granted by the Foundation "Santé, Sport               [KBH06]     Michael Kazhdan, Matthew Bolitho, and
                                                                               Hugues Hoppe. Poisson surface reconstruction.
et Développement Durable", presided by Pr. Yvon
                                                                               Eurographics Symposium on Geometry Process-
Berland. The authors would like to thank everyone
                                                                               ing, 2006.
involved in the KiTT project: Christian Coulange for
                                                                   [KCP92]     James Kent, Wayne Carlson, and Richard Par-
his precious help, Marc André, Frédéric Cohen and
                                                                               ent. Shape transformation for polyhedral ob-
Philippe Souteyrand for their wise advices and for pro-
                                                                               jects. Computer Graphics, 26(2), July 1992.
viding CT scan data, and Pierre-Henri Rolland for his



WSCG 2012 Communication Proceedings                          186                                          http://www.wscg.eu
[KK99]      George Karypis and Vipin Kumar. Multilevel                         1281:248 – 253, 2005.
            k-way hypergraph partitioning. Proceedings of          [Pol00]     K. Polthier. Conjugate harmonic maps and min-
            the 36th annual ACM/IEEE Design Automation                         imal surfaces. Technical report, Technische Uni-
            Conference, 1999.                                                  versity of Berlin, 2000.
[KSK97]     T. Kanai, H. Suzuki, and F. Kimura. 3d geo-            [RMK+ 05] Mauricio Reyes,        Grégoire Malandain,
            metric metamorphosis based on harmonic map.                      Pierre Malick Koulibaly, Miguel Gonzalez
            Proceedings of The Fifth Pacific Conference on                   Ballester, and Jacques Darcourt. Respiratory
            Computer Graphics and Applications, 1997.                        motion correction in emission tomography im-
[KSK00]     Takashi Kanai, Hiromasa Suzuki, and Fumihiko                     age reconstruction. Medical Image Computing
            Kimura. Metamorphosis of arbitrary triangular                    and Computer-Assisted Intervention, pages 396
            meshes. Proceedings of Computer Graphics and                     – 376, 2005.
            Application, 20(2), March 2000.                        [RMOZ01] Torsten Rohlfing, Calvin Maurer, Walter O’Dell,
[LDSS99]    Aaron Lee, David Dobkin, Win Sweldens, and                      and Jianhui Zhong. Modeling liver motion and
            Peter Schroder. Multiresolution mesh morphing.                  deformation during the respiratory cycle using
            Proceedings of Computer Graphics and Interac-                   intensity-based free-form registration of gated
            tive Techniques, 1999.                                          MR images. Medical Imaging 2001: Visual-
[LMVD11] Valentin Leonardi, Jean-Luc Mari, Vincent Vi-                      ization, Image-Guided Procedures, and Display,
         dal, and Marc Daniel. Reconstruction 3d du vol-                    pages 337 – 348, February 2001.
         ume rénal à partir d’acquisitions scanner volu-           [RSH+ 99]   Daniel Rueckert, L. I. Sonoda, C. Hayes,
         miques. Journée du Groupe de Travail en Mod-                          D. L. G. Hill, M. O. Leach, and D. J. Hawkes.
         élisation Géométrique, GTMG, pages 83 – 92,                           Nonrigid registration using free-form deforma-
         March 2011.                                                           tions: Application to breast MR images. IEEE
[LSS+ 98]   Aaron Lee, Win Sweldens, Peter Schroder,                           Transactions on Medical Imaging, 18(8), Au-
            Lawrence Cowsar, and David Dobkin. Maps:                           gust 1999.
            Multiresolution adaptive parameterization of           [SBMG06] David Sarrut, Vlad Boldea, Serge Miguet,
            surfaces. Proceedings of SIGGRAPH, pages 95                     and Chantal Ginestet. Simulation of four-
            – 104, July 1998.                                               dimensional CT images from deformable reg-
[MCG+ 03] Martin Murphy, Steven Chang, Iris Gibbs,                          istration between inhale and exhale breath-hold
          Quynh-Thu Le, Jenny Hai, Daniel Kim, David                        CT scans. Medical Physics, 33(3), March 2006.
          Martin, and John Adler. Patterns of patient              [SGB+ 00]   Achim Schweikard, Greg Glosser, Mohan Bod-
          movement during frameless image-guided ra-                           duluri, Martin Murphy, and John Adler. Robotic
          diosurgery. International Journal of Radiation                       motion compensation for respiratory movement
          Oncology Biology Physics, 55(5):1400 – 1408,                         during radiosurgery. Journal of Computer-Aided
          2003.                                                                Surgery, 2000.
[NdSE+ 08] Karsten Ostergaard Noe, Baudouin Denis                  [SSK+ 00]   H. Shirato, S. Shimizu, K. Kitamura, T. Nish-
           de Senneville, Ulrik Vindelev Elstrom, Kari                         ioka, K. Kagei, S. Hashimoto, H. Aoyama,
           Tanderup, and Thomas Sangild Sorensen. Ac-                          T. Kunieda, N. Shinohara, H. Dosaka-Akita, and
           celeration and validation of optical flow based                     K. Miyasaka. Four dimensional treatment plan-
           deformable registration for image-guided radio-                     ning anf fluoroscopic real-time tumor tracking
           therapy. Acta Oncology, 47(7):1286 – 1293,                          radiotherapy for moving tumor. International
           2008.                                                               Journal of Radiation Oncology Biology Physics,
[NPSA07]    Stéphane Nicolau, Xavier Pennec, Luc Soler,                        48:435 – 442, September 2000.
            and Nicholas Ayache. Clinical evaluation of            [YHM07]     Han-Bing Yan, Shi-Min Hu, and Ralph Mar-
            a respiratory gated guidance system for liver                      tin. 3d morphing using strain field interpolation.
            punctures.    Medical Image Computing and                          Computer Science and Technology, 1, 2007.
            Computer-Assisted Intervention, pages 77 – 85,
            2007.
[NUG+ 08] Masahiko Nakamoto, Osamu Ukimura, Inderbir
          Gill, Arul Mahadevan, Tsuneharu Miki, Makoto
          Hashizume, and Yoshinobu Sato. Realtime or-
          gan tracking for endoscopic augmented reality
          visualization using miniature wireless magnetic
          tracker. Medical Imaging and Augmented Real-
          ity, pages 359 – 366, 2008.
[OTW+ 05] B. Olbricha, J. Trau, S. Wiesner, A. Wicherta,
          H. Feussner, and N. Navab. Respiratory motion
          analysis: Towards gated augmentation of the
          liver. Computer Assisted Radiology and Surgery,




WSCG 2012 Communication Proceedings                          187                                            http://www.wscg.eu
WSCG 2012 Communication Proceedings   188   http://www.wscg.eu
  Rendering of Translucent Objects, Verification and
              Validation of Algorithms
                   Victor A. Debelov                                                  Dmitry S. Kozlov
  Institute of Comp. Math. & Math. Geophysics                                  Novosibirsk State University
                     SB RAS                                                         Pirogova str., 2
              Prospect Lavrentieva, 6                                          630090, Novosibirsk, Russia
           630090, Novosibirsk, Russia                                           kozlov@oapmg.sscc.ru
             debelov@oapmg.sscc.ru


                                                     ABSTRACT
An approach to verification and validation of algorithms that render transparent objects (media) is described.
Rendering of transparent optically isotropic objects has been studied extensively. However, the papers devoted
to optically anisotropic objects are few in number. The main goal of the present paper is to suggest a
collaboration in creating and supporting an open database of tests. To prepare a real scene with a crystal
specimen, to photograph it, and to describe the corresponding virtual scene is a complex problem. Obviously this
is an almost impossible task for many devolopers of rendering algorithms. Well-known examples of translucent
media are crystals. They are convenient for testing purposes as they have permanent solid shapes. Although the
crystals are rendered by a recursive ray tracing algorithm, the tests considered in the paper can be applied to
other algorithms.
Keywords
Anisotropic media, crystal, birefringence, pleochroism, optical dispersion, ray tracing, polarized light, rendering
algorithm, test repository, verification, validation.
1. INTRODUCTION                                                     rendering by polarized light and the optical
The difference between isotropic and anisotropic                    phenomena (the major properties are optical
media is explained in [Hay06]: "There are two types                 dispersion, birefringence, and pleochroism). A
of optical crystals, isotropic and anisotropic crystal.             comprehensive review of early works can be found in
The isotropic crystals have the same refractive index               [Guy04] and [Wei08]. The latter paper is devoted to
for all directions. The anisotropic crystal has a                   rendering of uniaxial monocrystals. The paper
different refractive index in a different direction, and            [Deb12] describes an algorithm to render isotropic
has two different values for the same direction.                    and uniaxial crystalline aggregates. The paper
However, there are one or two particular directions                 [Lat12] describes also computations of refracted rays
where these two refractive indices have the same                    in biaxial crystals.
value. These particular directions are called optic                 The above papers provide with different 3D scenes. It
axes and the crystal having one optic axis is called                would be useful to combine the test sets. This would
uniaxial or monoaxial crystal, while the one having                 allow third parties to provide improvements of the
two optic axes is called biaxial crystal. Since the                 algorithms.
anisotropic crystal has two refractive indices, there               A good example is the site [Mat], which represent a
are two refracted rays in crystal for one incident ray              repository of test data for use in comparative studies
and so-called double refraction or birefringence                    of the algorithms of numerical linear algebra. Any
occurs".                                                            new numerical algorithm has an opportunity to be
In the literature on computer graphics there are not                examined for accuracy and steadiness and compared
many papers on crystal rendering devoted to                         with the other algorithms.
                                                                    A similar role in global illumination is played by the
Permission to make digital or hard copies of all or part of
                                                                    Cornell Box [Cor]. The Cornell Box has become a de
this work for personal or classroom use is granted without
fee provided that copies are not made or distributed for
                                                                    facto standard. It contains an exhaustive test scene
profit or commercial advantage and that copies bear this            definition. Later this test was extended to
notice and the full citation on the first page. To copy             semitransparent surfaces; see [Far05]. In [Smi00] are
otherwise, or republish, to post on servers or to                   proposed a number of tests devoted to global
redistribute to lists, requires prior specific permission           illumination algorithms which are available on the
and/or a fee.                                                       Internet. The description of the tests is a paper text. It
                                                                    would be more useful to separate the papers devoted




WSCG 2012 Communication Proceedings                           189                                         http://www.wscg.eu
to a description of simulation and those with test              9.   Selection of the computer model of the virtual
specifications.                                                      camera. In our case it is a well-known pin-hole
In paper [Deb10] an approach is proposed to create a                 camera.
database for verification and validation of algorithms          10. Single view calibration of the pin-hole camera
that render transparent optically anisotropic objects.              from the photograph of the real scene.
As an appendix to the paper an Internet resource is             11. Spectral Rendering. It is necessary to have the
made with exact specifications of test scenes'                      spectral characteristics of all objects of the real
geometry and illumination. At the present time the                  test scene or their approximate values. While
resource is under reconstruction to adjust the ideas                rendering a spectral image representation is
proposed in the above paper.                                        obtained. We call it a SRGB image.
We recall that the purpose of the present paper is to           12. Tone reproduction: transformation of the SRGB
create a common test database for the algorithms that               image to a RGB image.
render optically anisotropic translucent objects. No
analysis and criticism of the algorithms are given.             13. Visual comparison of the photograph and the
We describe our process of verification and                         synthesized image.
validation in section 2. Section 3 is devoted to a              14. Pixel by pixel comparison of the photograph and
survey of tests used by developers. Section 4                       the synthesized image. The image resolution
describes a real scene, techniques, and tricks used to              must be equal to the photograph resolution.
recover the geometry and other parameters. A typical            The purpose of this comparison is to test the
virtual scene is considered in Section 5. Section 6             assumptions and operation of the algorithm and
describes some additional parameters of the                     determine the field of its application.
rendering process to determine uniquely the
calculated image. In section 7 additional useful tests          Below we will consider the steps in detail.
are described. In section 8 conclusive remarks on our
                                                                3. RELATED WORK
test repository are given.
                                                                In [Guy04] several crystals are described without
2. PROCESS OF VALIDATION                                        detailed specifications of the specimens geometry,
After a local model of light interaction with                   camera, optical axis, etc. Although the authors
transparent optically anisotropic crystalline media             claimed that models of standard gemstone cuts are
[Deb12] was developed we decided to create a                    readily available on the Internet they performed an
rendering algorithm to verify and validate the model.           additional investigation to justify geometry of an
Our algorithm is based on recursive ray tracing. One            available specimen of tourmaline. Authors worked
criterion of validation is to reach the maximal                 also on the acquisition of light illuminating their test
coincidence between a photograph of a real scene                scene. No data on the geometry obtained, virtual
and a simulated image of a virtual scene similarly to           camera, and lighting (cube-map) are available in the
an approach used in the Cornell Box project. In other           paper.
words, the purpose is to compute a visually plausible           Another paper [Wei08]: a calcite specimen was used
image corresponding to a photograph of a real scene.            for a real test scene. Again, the paper contains no
The algorithm is implemented in the following steps:            detailed specifications of the specimen geometry,
                                                                landscape grid pattern, virtual camera, optical axis,
1.   Selection of a test mineral specimen. It must be a         and illumination.
     transparent      monocrystal      with     strong
     birefringence.                                             The paper [Lat12] describes algorithms based on a
                                                                recursive numerical method in contrast to both papers
2.   Cutting the specimen to obtain a simple                    mentioned above, which contain closed form
     geometric shape. In our case it is a hexahedron.           formulas for exact calculations. Actually, this paper
3.   Recovery of the geometry.                                  allows calculating the refracted rays for a ray
4.   Determination of scene illumination.                       incident onto a boundary of a crystal of different
                                                                types: isotropic, uniaxial, and biaxial. The authors
5.   Creating a real test scene of crystal and light            produce test images using a convex calcite plate in a
     sources.                                                   virtual scene. The paper includes computed images
6.   Selection of a photo camera.                               that illustrate birefringence. Note, that exact
7.   Obtaining a photograph of the real scene.                  geometry of virtual scenes and camera parameters,
                                                                illumination, and image resolution are not specified
8.   Creating a virtual scene, i.e., a computer model           in the text. Although the algorithms [Lat12] do not
     of the test scene.                                         compute the correct colors of the resulting images,
                                                                those images allow comparing with images obtained




WSCG 2012 Communication Proceedings                       190                                        http://www.wscg.eu
                                       Figure 1. Scheme of a real scene.
by other algorithms in order to examine the                     No detailed specifications of the virtual test scenes to
correctness of birefringence.                                   be rendered are available also in the papers
The paper [Deb12] is devoted to derivation of closed            mentioned above.
form formulas to calculate a local illumination model           We suggest a different approach: to create a special
for interaction of a light ray with the boundary of two         appendix that stores exact specifications of the virtual
transparent media of isotropic and uniaxial optical             test scenes described in a paper. It may be a personal
types. It includes also: a photograph of a calcite              Internet site. Note that the data presented at the site
uniaxial crystal over a color checkered pattern, a              recommended in our paper [Deb10] are also
corresponding virtual scene with the recovered                  incomplete and poorly documented. At the present
geometry of the specimen, a synthesized image, and              time this site is being reconstructed.
several virtual test scenes with corresponding                  Our main objections concern fact that the information
rendered images. In spite of the fact that the paper            on test scenes is often incomplete and does not allow
contains detailed description of validation                     one to reproduce the rendering of scenes and
experiments, nevertheless, the data presented are               comparing of the images obtained. However, all the
insufficient to reproduce them thoroughly.                      above-mentioned papers present numerous pictures
The paper [Deb10] has a similar purpose to create a             that illustrate the approaches and/or algorithms being
testbed for algorithms rendering transparent isotropic          described. Also a limited length of papers does not
and anisotropic media with polarized light. There the           allow authors to present all details which could
following groups of tests are suggested in the paper:           overload a text. We expand an idea proposed in
   Low level test based on some clearly formulated             [Deb10] and suggest creating a testbed as a common
    physical laws like Snell's law, Brewster's angle,           Internet resource devoted to detailed information on
    etc. These tests allow us to assess, e.g., the              the test data. We expect that new efficient algorithms
    accuracy of algorithms.                                     to render anisotropic crystals will appear in the near
                                                                future. This test repository, being an expandable
   Special virtual scenes demonstrating the physical           database, can be helpful in debugging them.
    phenomena: ortoscopic and conoscopic images,
    internal conical refraction, etc. [Bor80]. The              4. REAL SCENE
    authors thoroughly describe a 3D scene to render            A real scene is selected in steps 1–7 mentioned in
    the effect of internal conical refraction. Instead          section 2. We spent more than a year to find a crystal,
    of giving the exact scene specifications they               cut it, take a photograph, and calibrate the virtual
    describe some guidelines how to construct such              camera. The above repository can be helpful to save
    scenes. Since the camera parameters and                     time and efforts of the developers of renderers.
    computed image are not described, it is difficult           Otherwise the community will be limited to those
    to compare the algorithms.                                  who have stones, able to cut a specimen and calibrate
                                                                the virtual camera, etc. The majority of researchers
   Comparison of a real scene photograph with a
                                                                are programmers rather than geologists or jewelers.
    computed image of the corresponding virtual
                                                                Therefore all photographs of proper specimens or
    scene. Note that the photograph of a real scene in
                                                                gemstones are important. First we consider
    the paper is much different from the image of
                                                                specimens that have simple geometric shapes: cube,
    the virtual one. Moreover, the real calcite
                                                                sphere, or convex polyhedron. Jewelry shapes are
    specimen used is not a monocrystal but a
                                                                more complex and require additional efforts to
    crystalline aggregate. Besides, the camera
                                                                recover the exact geometry, see [Guy04].
    parameters are not known.




WSCG 2012 Communication Proceedings                       191                                        http://www.wscg.eu
It seems that recovering the geometry of transparent
crystals is a difficult task. For example, calcite is too
fragile, so the faceted shape of a specimen may get
chipped and peeling. The KDP (a Potassium
Dihydrogen Phosphate) crystals are not waterproof
(expired air also), and require additional care, e.g.,
mask, gloves, etc. The crystals are not expensive, and
have visible strong birefringence. Measuring the
linear sizes of edges by standard tools may produce
errors due to the possible chipped vertices of the
specimen.
Often knowledge of the construction of a real scene
may be useful. A scheme of a real scene and its
approximate sizes are shown in Fig. 1. A specimen
lies on a sheet of white paper with a printed color
texture. The sheet is put on a transparent glass plate
and covered by a sheet of black paper with a
rectangular window. Two cylindrical luminescent                   Figure 3. Scan of the face with vertices 1234. The
lamps are placed under the plate in such a way as to                 fat black dot is the exit of the optical axis.
provide an approximately uniform illumination of the
sheet. Other light sources are absent. Specifically, a            Fig.5. In order to decrease the difference between the
window with the printed texture and lamps determine               textures colors in the photo and the calculated image
the geometrical shape and spectrum of the light                   ones, the colors of the virtual texture can be taken
source.                                                           from a blurred (unfocused) photograph of the real
                                                                  texture made with the same camera parameters and
                                                                  exposition.
                                                                  The only optical axis was determined from scans of
                                                                  two opposite faces, 1234 and 5678, a look through
                                                                  which results in the absence of doubling of the
                                                                  texture. On each face we selected a dot that belonged
                                                                  to the axis. An important point: the dots were
                                                                  selected manually, which can lead to small errors in
                                                                  the direction of the axis.
                                                                  Some specifications of the photo camera being used
                                                                  may be useful too, for example, a camera Canon
                                                                  450D with a lens EFS18-55 mm F:3.5-5.6 IS. A
                                                                  minimal matrix sensitivity of ISO100 was selected to
                                                                  decrease noise. To minimize the lens’s aperture, the


    Figure 2. Photograph of a calcite specimen.
Consider a photo with enumerated vertices (Fig.2). In
our experiment we took a calcite specimen
(hexahedron). Using a scanner we obtained some
images of the faces (like one in Fig.3) and
determined the lengths of edges of all faces, see
Fig.4.
A source of illumination in the scene is a rectangle
with a color texture, Fig.2. There were problems of
coincidence of the spectra of real and virtual textures:
a) the spectra of the lamps being used are usually
unknown; b) the spectra of transmittance of the paper
and inks are unknown; c) the lens adds some
unknown distortions to the spectrum of transmitted
light; d) the sensitivity of the camera matrix to
various parts of the spectrum is unknown too, etc. In                 Figure 4. Scan of the face with vertices
our calculations we used a light source as shown in                 1234. Lines approximating edges are shown.



WSCG 2012 Communication Proceedings                         192                                      http://www.wscg.eu
                                                              to [Lat12, Fig.13] the test scene was illuminated by a
                                                              point light source at the camera tip.
                                                              A typical virtual scene is shown in Fig.7: a specimen,
                                                              a camera, and an axis aligned box around the
                                                              specimen. The latter can be positioned and resized by
                                                              the user. It plays the role of a cube-map which
                                                              determines illumination. Arranging the box the user
                                                              can put one of specimen's faces just onto the box's
                                                              face. In such a way virtual scenes similar to those
                                                              used in [Wei08] and [Deb12] were constructed. The
                                                              six textures used allow one flexibility with assigning
                                                              of illumination of the scene.




    Figure 5. The texture used as a light source
                 in the virtual scene.
diaphragm was set to a maximal value of F:36. In this
case diffraction does not affect significantly the
image. The exposition must be taken considerably
longer than the blinking lamp period (if luminous
lamps are used).                                                     Figure 7. Virtual scene environment.
The vertices of the hexahedron are obtained as the            Note that all the textures are one-sided, and are
intersection points of lines approximating the edges          invisible from outside. They illuminate unpolarized
(Fig.6), e.g., vertex 7 in Fig.2.                             light inside the box. It is best to use a spectral
                                                              representation but, in practice, only a RGB
                                                              representation is available. Other explicitly defined
                                                              formats can be applied, e.g., those in [Guy04].
                                                              Various formats can be considered as additional
                                                              information, and a source of data. We believe that the
                                                              spectra being used must always be provided.
                                                              Generally, the scene is filled with air or vacuum,
                                                              sometimes, it may be any of transparent media:
                                                              isotropic, uniaxial, and biaxial. The substance is
                                                              called filler.
                                                              The following optical characteristics are specified for
                                                              the scene filler and the specimen:
                                                                                         с is specified for a
                                                              1) Axes. The only optical axis
                                                              uniaxial medium and two axes с1 and с 2 for a
 Figure 6. Gray scan of the photograph with lines             biaxial one. For biaxial media the directions of
              approximating edges.                            optical axes depend on the wavelength therefore they
Thus, we have obtained some specifications of the             must be specified for each wavelength used in the
geometry and illumination of the virtual scene with           image calculation, even if the directions are
some errors at each step.                                     calculated. Obviously, isotropic media require no
Finally, the camera parameters (camera calibration)           axis specification.
were determined manually with the help of an                  2) Main indices of refraction: one for an isotropic
interactive application. A photograph (Fig.2) and a           medium   ni , two   for a uniaxial ( no ,   ne ), and three
gray scan (Fig.6) were used for calibration.
                                                              for a biaxial one ( n1 ,   n2 , n3 ).   The corresponding
5. VIRTUAL SCENE
Our test environment is quite similar to that used in         values are specified for each wavelength used in the
[Guy04], [Wei08], [Deb10], [Deb12]. It is not clear           image calculation, even if the calculation of samples
what illumination was used in [Lat12] as according            is done with the Sellmeier's equation, Laurent's
                                                              equation [Bor80], or in another way.



WSCG 2012 Communication Proceedings                     193                                            http://www.wscg.eu
3) Absorption. In the case of a transparent colorless           typical examples are shown. For the tests the
medium no absorption data are required. In the                  repository includes:
general case the modeling of absorption is
challenging problem because of the following two
facts. First, the refraction and absorption properties
are not independent [Bor80]. Second, the rays
propagating in absorbing anisotropic media are
elliptically polarized but in transparent anisotropic
media they are linearly polarized [Bor80]. These
facts are usually ignored, see e.g. [Guy04]. Therefore
majority of crystalline media require the
determination of one, two, or three attenuation
spectra. Apparently, this format satisfies the
absorption data from [Guy04].
Similarly to the above remark about the format of
illumination data, we believe that some parameters of
a virtual scene as the spectra must be determined.
Note that a source of possible errors in the final per
pixel comparison of a photo and a calculated image
are the unknown physical conditions of the specimen,                Figure 9. Computed image of a uniaxial cube.
namely, the temperature, external forces and fields,
etc. These parameters are usually taken from                        An image of a test scene rendered with OpenGL.
references where they are evaluated accordingly to                   It is used to comment the scene geometry and
certain conditions.                                                  simplify the understanding.
All the virtual scenes must be placed in the                        Coordinates of cube vertices.
repository, even if there are no corresponding                      Coordinates of a square axes-aligned textured
photographs of the real scenes. This may help in                     plate.
comparing the algorithms being used.
                                                                    Texture image.
6. VIRTUAL TEST SCENES                                              Light environment.
A most important group consists of tests of
comparison of the photograph with the computed                      Ray tracing depth in bounces.
image is as considered above.                                       Pin-hole camera parameters: focus length,
A second group of tests consists of virtual scenes that              aperture point, view direction.
do not require the existence of the corresponding real              Image plane sizes.
scenes.
                                                                    Image resolution.




   Figure 8. Computed image of a biaxial cube.
                                                                Figure 10. Computed image of an isotropic cube.
Various virtual scenes are created during the
debugging process. In Fig.8, Fig.9, and Fig.10 three



WSCG 2012 Communication Proceedings                       194                                        http://www.wscg.eu
   Optical type of the scene filler: isotropic,               Each image is specified by its computer platform and
    uniaxial, biaxial. In our case it is isotropic.            the computational time. This will facilitate in
                                                               comparing the performance of the algorithms.
   Refractive index of the filler.
                                                               A result of rendering our virtual scene is shown in
   Optical type of the specimen: biaxial (Fig.8),
                                                               Fig.11.
    uniaxial (Fig.9), or isotropic (Fig.10).
   Representation of the spectra: number of                   8. OTHER TESTS
    samples from 380nm to 780nm.
                                                               Virtual Scenes Illustrating Some Well-
   Main refractive indices for each sample.                   Known Facts From Optics
   Calculated image.                                          Internal conical refraction is a phenomenon observed
                                                               in biaxial crystals. The conoscopic images of
   Several copies of the calculated image with
                                                               anisotropic crystals are a practical means in
    appropriate comments (optional).
                                                               petrography; see [Bor80] for a theoretical foundation.
It is now possible to calculate the image of the               In [Deb10] corresponding virtual scenes are
specified scene and provide its per pixel comparison           described. These tests can help in debugging and
with the image from the repository.                            comparing the rendering algorithms.
7. RENDERING                                                   Additional Tests
The repository may include several calculated images           This group of tests is targeted to help in the
for a single virtual scene. They can be different              debugging of separate program blocks. The
because of the following rendering parameters:                 description may be reduced to a minimum. We
   Image resolution.                                          suggest that the authors contributing to the repository
                                                               may only name a fact and refer to a proper source
   Number of wavelengths used (e.g., one for                  from the well known literature. For example,
    monochromatic light). All the optical parameters           "Brewster Angle Test”, see [xx, page yyy]. A better
    (see previous section) should be represented by            way is a detailed description of a test including the
    spectra of equal length to represent the smooth            specifications of: a) the optical characteristics of two
    part of the spectrum and the set of separate peaks         media; b) the ray incident onto the boundary between
    to represent the another part.                             the media; c) polarization of the ray; d) the resulting
   Depth of ray tracing. A recursive ray tracing is           rays (reflected and refracted) with their polarization
    very time consuming, since at each bounce the              states.
    ray is split into up to four generated rays. For
    example, if the data are: depth=20, spectra of 21
    samples the rendering took several hours of
    calculation on a 8-processor cluster.




                                                                   Figure 12. Birefringence test, arrows show
                                                                               polarization state.
                                                               In [Deb10] several examples are given: Snell's law,
                                                               Brewster's angle, and a birefringence test. Consider
                                                               the last one. In Fig.12 a typical scene is presented.
                                                               Not only the ray directions be verified, but the
                                                               polarization state of the generated rays as well. This
                                                               can be useful if the repository includes several
                                                               examples of optical media with incident rays and
                                                               derived rays of different polarization states and
Figure 11. Calculated image corresponding to the               intensities.
              photograph in Fig.2.
                                                               The problem of numerical stability may arise in
   Camera parameters given in the scene coordinate            debugging, e.g., when the directions of the ray, the
    system. For example, we used pin-hole camera               optical axis, and the normal are almost the same. We
    in the algorithm.                                          believe that such situations must be put into the



WSCG 2012 Communication Proceedings                      195                                        http://www.wscg.eu
repository too, especially, the problem has been               10. ACKNOWLEDGMENTS
solved.                                                        This work was supported in part by the Russian
                                                               Foundation for Basic Research, grants No. 12-07-
9. CONCLUSIONS                                                 00386 and No. 12-07-00391.
We did not intend to cover every possible situation in
the rendering of crystals. In this paper we have               11. REFERENCES
presented our approach. The repository must not                [Bor80] Born, M. and Wolf, E. Principles of optics:
have a certain predefined format. Tests of any                    electromagnetic theory of propagation,
complexity delivered by any developers are                        interference and diffraction of light. Cambridge:
welcome. The contributing developer may deliver                   Cambridge University Press, 1980.
his/her information in any convenient format.
                                                               [Cor] Cornell Box:
Nevertheless, we have remarks: a) lossless image
                                                                  http://www.graphics.cornell.edu/online/box
formats like BMP, PNG should be used; b) the
spectra should be used wherever it is possible. In             [Crt] Crystal tests:
case other formats are used they should be                        http://oapmg.sscc.ru/temp_crystal_tests/
transformed to the BMP or spectra formats explicitly.          [Deb10] Debelov, V.A., Kozlov, D.S. Verification of
This will help in avoiding possible uncertainties.                algorithms of photorealistic rendering of crystals.
Additional information of any kind will be very                   Proc. Graphicon'2010, Russia, St.Petersburg,
useful, for example: an OpenGL image of the                       September 20-24, 2010, pp. 238–245. (In
recovered geometry (see Fig.7), references to the                 Russian).
relevant papers and reports, images with comments, a              http://www.graphicon.ru/proceedings/2010/Proce
photograph or a scheme of a real scene (Fig.1), and               edings.pdf
appropriate unpublished comments.                              [Deb12] Debelov, V.A., Kozlov, D.S. A local model
We suggest separating all tests in the repository into            of light interaction with isotropic and uniaxial
the following groups:                                             transparent media. Vestnik of Novosibirsk State
                                                                  University, Series: Information Technologies,
   A photograph of a real scene,
                                                                  vol. 10, No. 1, pp. 5–23, 2012, (in Russian).
   Virtual scenes,
   Virtual scenes of optic phenomena,                         [Far05] Farnsworth, M., Erbacher, R. F. Global
   Tests of particular features,                                 illumination: efficient renderer design and
   Other tests.                                                  architecture. Proc. Intern. Conf. on Geometric
                                                                  Modeling, Visualization & Graphics, pp. 1691-
Additionally, a set of keywords should be supported;              1695, 2005.
each keyword refers to the relevant tests. The set of
keywords contains: real scene photographs; recovery            [Hay06] Hayamitzu, Y. Analysis of internal conical
of specimen's geometries; definitions of illumination,            refraction using ray tracing formulas for the
transparent media, absorbing media, isotropic media,              biaxial crystal. Optical review 13, No. 4, pp.169–
uniaxial media, biaxial media, camera calibration,                183, 2006.
etc.                                                           [Guy04] Guy, S. and Soler, C. Graphics gems
We do not believe that we have found a complete                   revisited. ACM Trans. on Graphics (Proceedings
solution. This paper was caused by the present time               of the SIGGRAPH conference) 23, No. 3,
situation with accessible tests.                                  pp.231–238, 2004.

An Internet site, [Crt], was developed initially as a          [Lat12] Latorre, P., Seron, F. J., and Gutierrez, D.
support for the paper [Deb10]. At the present time it             Birefringence: calculation of refracted ray paths
is under reconstruction. Nevertheless, the reader can             in biaxial crystals. The Visual Computer 28, No.
find a detailed description of the tests corresponding            4, pp. 341-356, 2012.
to Fig.8-11.                                                   [Mat] Matrix Market:
The problem of creating test scenes based on real                http://math.nist.gov/MatrixMarket/
scene photographs is not an easy task. Experts on a            [Smi00] B. Smits, and H. W. Jensen. Global
wide variety of research have to be involved:                    illumination test scenes. Tech. Rep. UUCS-00-
crystallographers, specimen cutters, etc. Also a wide            013, Computer Science Department, University
range of specific devices have to be used:                       of Utah, June 2000.
spectrometers, etc.                                            [Wei08] Weidlich, A. and Wilkie, A. Realistic
We hope that this paper will help in creating a                  rendering of birefringency in uniaxial crystals.
repository with the specifications described above.              ACM Transactions on Graphics 27, (1):6:1–6:12
The inclusion of any test into our website is
welcome.



WSCG 2012 Communication Proceedings                      196                                       http://www.wscg.eu
                            WSCG 2012
                              Index


Abdulla,W.            259         Jimenez,J.R.   105
Ahmad,M.A.            367         Kakimoto,M.     95
Acharya,S.            357         Kalra,P.       347
Arora,N.              347         Karki,B.       357
Aryal,J.              327         Kenwright,B.     1
Bahnsen,C.            231         Khurana,S.     357
Benger,W.             357         Klein,A.        53
Beran,V.              205         Klein,A.,      197
Bian,X.               341         Klicnar,L.     205
Bittorf,B.            269         Köppen,V.       35
Blanz,V.               59         Kozlov,D.      189
Brener,N.             357         Krim,H.        341
Bruni,V.              283         Krivokuca,M.   259
Bugaj,M.              291         Krömker,D.      87
Crumley,Z.            113         Kršek,P.       223
Cyganek,B.            291         Kumar,A.       347
Daniel,M.             179         Kurowski,M.     79
de Rezende,P.J.        27         Lavoué,G.      259
Debelov,V.            189         Lazunin,V.     131
Delmas,P.             249         Lee,G.R.        45
Dewilde,A.            231         Lee,H.C.        45
Drap,P.               275         Lee,T.M.        45
Falcao,A.X.            27         Leonardi,V.    179
François,A.           327         Lutteroth,C.   249
Gain,J.               113         Maddock,S.     317
Gargalik,R.           163         Madsen,C.B.    231
Gillies,D.F.           69         Mahiddine,A.   275
Gomes,J.F.             27         Malik,M.       367
Graca,S.              377         Marais,P.      113
Guthe,M.               59         Marchetti,A.    11
Hast,A.                11         Mari,J.L.      179
Hoppenheit,J.         155         Marks,S.       169
Hrmo,I.               163         Masik,S.,       35
Hulík,R.              223         Merad,D.       275
Ihrke,I.              239         Metzgar,J.     147
Iyengar,S.            357         Minich,C.      309
Jawad,M.              335         Minoi,J.-L.     69
Jean-Marc Boi,J.-M.   275         Morik,M.        35
Müller,R.        35         Seidel,H.-P.    239
Müller,S.       155         Seinturier,J.   275
Nakata,N.        95         Semwal,S.K.     147
Nguyen,M.H.     249         Schiffner,D.     87
Nishita,T.       95         Schmidt,M.       59
Nischwitz,A.     53   197   Schumann,M.     155
Noguera,J.M.    105         Spanlang,B.      19
Obermeier,P.     53   197   Suzuki,C.T.N.    27
Oliveira,J.F.   377         Tang,Y.         123
Oshita,M.       213         Tappert,B.      197
Pedersen,C.     231         Tomori,Z.       163
Pimenta,W.      139         Tranchet,G.     231
Qureshi,H.      367         Vassilev,T.I.    19
Raffin,R.       327         Vidal,V.        179
Realinho,V.     377         Vitulano,D.     283
Reuter,A.       239         Warburton,M.    317
Ritter,M.       357         Windsor,J.      169
Robert,A.J.      69         Wu,Z.           123
Rossi,E.        283         Wuensche,B.     169   249   259
Roy,S.          357         Wüthrich,C.     269
Safdar,K.       299         Yasin,M.        335
Saito,P.T.M.     27         Yoon,G.H.        45
Santos,L.P.     139         Zhou,M.         123
Sarfraz,M.S.    335
Savchenko,V.    131
